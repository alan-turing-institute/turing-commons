---
slideOptions:
  transition: slide
  spotlight:
    enabled: true
---

# Fairness (Section 3): Statistical Fairness

> **Note**
> The following sections are part of this module:
>
> - Section 1: [What is Fairness?](rri-203-1.md)
> - Section 2: [Sociocultural Fairness](rri-203-2.md)
> - Section 3: [Statistical Fairness](rri-203-3.md)
> - Section 4: [Practical Fairness](rri-203-4.md)

---

## The Allure of Statistical Fairness

Fairness in ML and AI has seen enormous interest over the last decade, as many researchers and developers try to bring order to a vast and complex landscape of statistical concepts and measures.
This is understandable.
Statistical measures are, typically, well-defined and bring a quantitative dimension to the discussion of fairness.

Attempting to translate fairness into a statistical problem, therefore, is appealing because it suggests that if we can get the right measure or formula we can "solve" the problem of fairness in an objective manner.
However, the allure of this statistical or computational approach needs to be properly seen for what it is—an important tool, but one that has many limitations and needs to be used responsibly.

----

As a simple example of one limitation, consider the following question:

> Is it fair for a company to build an AI system that monitors the performance of employees in a factory and notifies them when they are not meeting their targets? Or, should the company instead invest in better training and support for employees and work with their staff to understand obstacles or barriers?

----

There is no easy answer to this question.
In fact, you will probably have realised that a lot depends on factors that are not appropriately specified in the question (e.g. what happens if the staff continue to miss targets, who is this company, and who are their staff?).
However, putting aside these unspecified factors, it should be evident that such a question cannot be reduced to some statistical problem.

Therefore, in this section we will take a critical look at some proposed statistical concepts and techniques for understanding fairness in ML and AI, paying close attention to how they can contribute to the discussion of fairness, and where they fall short.

---

## Fairness in Classification Tasks

The area of ML and AI that has seen the most attention, with respect to fairness, is the task of classification.
In this section, we are going to build up our knowledge of statistical fairness by using a simple binary classification task as a running example.
This simplification has a cost and a benefit:

- The cost is that we will not be able to capture the full complexity of statistical fairness in ML and AI, such as exploring all relevant metrics, or different types of problems (e.g. multi-class classification, non-classification tasks, etc.)
- The benefit is that we will be able to focus on the core concepts and ideas and link them to ethical and social issues of fairness.

Let's get started. 

----

First of all, let's introduce our classification task.
We're going to pretend that we are developing a classification model that diagnoses whether a patient has a disease or not.
We're not going to worry about what this disease may be, or what the prevalence of the disease may be throughout the population—our first oversimplification.
Instead, let's just assume we have 100 patients and we know that 2 of them have the disease.

<!-- show graphic -->

----

Show why accuracy is not a good metric when there is class imbalance, because a model could just learn to predict that no one has the disease and still get 98% accuracy.
----


In a classification task, the goal is to predict which class a given input belongs to.
This high-level description can be applied to a wide range of tasks, including:

- Which object is depicted in an image?
- Is this applicant likely to default on a loan?
- Is a post on social media an instance of hate speech?
- Will this patient be readmitted to hospital within 30 days?

----

Some of these tasks are only instances of classification tasks because they have been framed in a way that admits a binary classification.
For example, the final question may depend, first, on obtaining a probabilistic estimate of readmission and then deciding on a threshold, above which readmission is considered likely.
If, say, a patient receives a probability of `< 0.7` then they will be classified as `unlikely` to be readmitted.
However, if the patient receives a probability of `≥ 0.7` then they will be classified as `likely` to be readmitted.

The ability to frame a task as a instance of classification, then, sometimes depends on previous choices (or assumptions) established during the design of a project or system.

----

To keep things simple, this section will only look at classification tasks.
However, the above point about the formulation of a problem is important to recognise before we proceed, as it is one instance of design choices that affect the overall fairness of a system, but can be *screened out* by a focus on statistical fairness.

As such, let's start by listing several key issues that can become screened off and overlooked if we don't pay close enough attention:

1. Classification may depend on a prior choice about where to *set a threshold*
2. The limits of *protected characteristics*
3. The data are considered the *ground truth*
4. Training data can include *proxies for sensitive characteristics*
5. Maintaining the *status quo* is not always fair

:::success
**Further Reading**

Julia Powles and Helen Nissenbaum '[The Seductive Diversion of ‘Solving’ Bias in Artificial Intelligence](https://onezero.medium.com/the-seductive-diversion-of-solving-bias-in-artificial-intelligence-890df5e5ef53)'
:::

---

## Choosing fairness metrics

Group-level fairness only provides assurance that the average member of a (protected, marginalsied or vulnerable) group will be treated fairly. No guarantees are provided to individuals. 

<!-- activity: taboo trade-off, which possession would people save from a burning building. designer to evoke variation in assessment of the sacred (https://www.scientificamerican.com/article/psychology-of-taboo-tradeoff/) -->

Humans have tried to quantify fairness for centuries. The Law of Hammurabi, for exmaple, states that if a man "destroy's the eye of a man's slave or beaks a bone of a man's slave, he shall pay one-half his price". Fortunately, such simplistic laws are no longer adhered to in modern society. 

<!-- incentivising unethical practice, allows people to do cost/benefit analysis on harming others intentionally. Also creates unitended consequences, such as the charge for late pickups at a nursery. -->

- A metric is a measure, typically representing the distance between two objects as represented on some standard scale (e.g. length in centimetres).

- Although the use of specific scales leads many to falsely attribute objectivity to statistical measures of fairness, this fallacy overlooks a core value-laden assumption: which measure to choose.

<!-- illustrative example from partial case study showing lack of flexibility in algorithmic features and protected characteristics -->

----

### Sufficient Statistics for Merit

A statistic is defined as sufficient if it cannot be replaced with another statistic derived from the same sample, such that the replacement provides any additional information regarding the parameter in question.

---

## Evaluating Fairness

<!-- this is where we should bring in all the technical stuff from @ruf2021 and @carey2021 

- Differential false positive rates
- Problems of defining fairness across individual and group level categories
    - Fair when considered as an inividual level, but due to historical factors baseline rates of reoffence are different across black and white defendants (i.e. black people are more likely to be arrested in the US due to systemic racism).
    - It is not possible to be fair to individuals and groups at the same time.

-->

### It's about more than just accuracy

Statistical notions of fairness when applied to ML/AI prioritise distributional notion of fairness, such as those that look to ensure equal error rates across sub-groups. But as [Barocas et al](https://fairmlbook.org/legitimacy.html) note:

> "One danger of the machine learning approach is that it leads to a narrow focus on the accuracy of the prediction. In other words, “good” decisions are those that accurately predict the target. But decision making might be “good” for other reasons: focusing on the right qualities or outcomes (in other words, the target is a good proxy for the goal), considering only relevant factors (in a sense we’ll discuss below), considering the full set of relevant factors, incorporating other normative principles (e.g., need, desert, etc.), or allowing people to understand and potentially contest the policy. Even a decision making process that is not terribly accurate might be seen as good if it has some of these other properties."

---

## Notes

- Have a slide with a confusion matrix that shows how each segment connects to the corresponding graphic representation of data points in a circle
- Use the analogy of trawling to help explain precision and recall
- Precision is sometimes referred to as positive predictive value (PPV), specifically in binary classification problems, because it represents the proportion of true positive predictions among all positive predictions made by a model or test (i.e. TP / P). To put it simply, PPV is the probability that a positive prediction is correct. In the context of a classifier used in medical diagnosis, for instance, PPV is the probability that a patient has a disease (TP) given that the classification result is positive. Because some test results will come back positive even when a patient does not have the disease, the set of all positive test results can be considered P in this example. If the classifier has high PPV, it means that the likelihood of a positive test result correctly indicating the presence of a disease is high.
  - PPV is an important metric in contexts that have high costs or consequences for missed positves (i.e. false negatives). This is because it measures the accuracy of positive predictions, which is particularly relevant in scenarios where positive predictions are rare (i.e. the minority class).

---

## Unused Resources

- Obermeyer et al. (2019) example of algorithm that contained racial bias due to using predicted health care cost as proxy for how sick an individual was. However, due to systemic barriers to accessing healthcare, or implicit biases of healthcare professionals, Black individuals were treated unfairyly. Only 17.7% of patients that the algorityn assisgned to receive extra care were Black.
    - Other examples: 
        - Women are less like to be shown ads for high paid positions like CEO or CFO (Datta et al. 2015)
        - Black women are less likely to be inaccurately classified by facial recognition systems (Buolamwini & Gebru, 2018)