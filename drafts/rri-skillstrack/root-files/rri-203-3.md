---
slideOptions:
  transition: slide
  spotlight:
    enabled: true
---

# Fairness (Section 3): Statistical Fairness

> **Note**
> The following sections are part of this module:
>
> - Section 1: [What is Fairness?](rri-203-1.md)
> - Section 2: [Sociocultural Fairness](rri-203-2.md)
> - Section 3: [Statistical Fairness](rri-203-3.md)
> - Section 4: [Practical Fairness](rri-203-4.md)

---


## The Allure of Statistical Fairness

FairML has been dominated by discussions of statistical concepts and measures. This is understandable. Statistical measures are, typically, well-defined and often do not depend on assumptions about substantive aspects of the underlying data distribution (e.g. which groups are included/missing from the distribution).

Reducing fairness to a statistical measure, therefore, renders the task into a problem to be solved. However, statistical notions of fairness typically remain silent on the more fundamental questions such as "should we build this system at all?"

<!-- Statistical fairness is often unable to address the broader issues of procedural fairness or social justice, which arise at earlier stages of the project lifecycle.  -->

:::success
**Further Reading**

Julia Powles and Helen Nissenbaum '[The Seductive Diversion of ‘Solving’ Bias in Artificial Intelligence](https://onezero.medium.com/the-seductive-diversion-of-solving-bias-in-artificial-intelligence-890df5e5ef53)'
:::

---

## Choosing fairness metrics

Group-level fairness only provides assurance that the average member of a (protected, marginalsied or vulnerable) group will be treated fairly. No guarantees are provided to individuals. 

<!-- activity: taboo trade-off, which possession would people save from a burning building. designer to evoke variation in assessment of the sacred (https://www.scientificamerican.com/article/psychology-of-taboo-tradeoff/) -->

Humans have tried to quantify fairness for centuries. The Law of Hammurabi, for exmaple, states that if a man "destroy's the eye of a man's slave or beaks a bone of a man's slave, he shall pay one-half his price". Fortunately, such simplistic laws are no longer adhered to in modern society. 

<!-- incentivising unethical practice, allows people to do cost/benefit analysis on harming others intentionally. Also creates unitended consequences, such as the charge for late pickups at a nursery. -->

- A metric is a measure, typically representing the distance between two objects as represented on some standard scale (e.g. length in centimetres).

- Although the use of specific scales leads many to falsely attribute objectivity to statistical measures of fairness, this fallacy overlooks a core value-laden assumption: which measure to choose.

<!-- illustrative example from partial case study showing lack of flexibility in algorithmic features and protected characteristics -->

----

### Sufficient Statistics for Merit

A statistic is defined as sufficient if it cannot be replaced with another statistic derived from the same sample, such that the replacement provides any additional information regarding the parameter in question.

---

## Evaluating Fairness

<!-- this is where we should bring in all the technical stuff from @ruf2021 and @carey2021 

- Differential false positive rates
- Problems of defining fairness across individual and group level categories
    - Fair when considered as an inividual level, but due to historical factors baseline rates of reoffence are different across black and white defendants (i.e. black people are more likely to be arrested in the US due to systemic racism).
    - It is not possible to be fair to individuals and groups at the same time.

-->

### It's about more than just accuracy

Statistical notions of fairness when applied to ML/AI prioritise distributional notion of fairness, such as those that look to ensure equal error rates across sub-groups. But as [Barocas et al](https://fairmlbook.org/legitimacy.html) note:

> "One danger of the machine learning approach is that it leads to a narrow focus on the accuracy of the prediction. In other words, “good” decisions are those that accurately predict the target. But decision making might be “good” for other reasons: focusing on the right qualities or outcomes (in other words, the target is a good proxy for the goal), considering only relevant factors (in a sense we’ll discuss below), considering the full set of relevant factors, incorporating other normative principles (e.g., need, desert, etc.), or allowing people to understand and potentially contest the policy. Even a decision making process that is not terribly accurate might be seen as good if it has some of these other properties."

---

## Unused Resources

- Obermeyer et al. (2019) example of algorithm that contained racial bias due to using predicted health care cost as proxy for how sick an individual was. However, due to systemic barriers to accessing healthcare, or implicit biases of healthcare professionals, Black individuals were treated unfairyly. Only 17.7% of patients that the algorityn assisgned to receive extra care were Black.
    - Other examples: 
        - Women are less like to be shown ads for high paid positions like CEO or CFO (Datta et al. 2015)
        - Black women are less likely to be inaccurately classified by facial recognition systems (Buolamwini & Gebru, 2018)