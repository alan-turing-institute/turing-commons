---
slideOptions:
  transition: slide
  spotlight:
    enabled: true
---

# Fairness (Section 3): Statistical Fairness

> **Note**
> The following sections are part of this module:
>
> - Section 1: [What is Fairness?](rri-203-1.md)
> - Section 2: [Sociocultural Fairness](rri-203-2.md)
> - Section 3: [Statistical Fairness](rri-203-3.md)
> - Section 4: [Practical Fairness](rri-203-4.md)

---

## The Computational Allure of Statistical Fairness

Humans have tried to quantify fairness for centuries.
The Law of Hammurabi, for exmaple, states that if a man "destroy's the eye of a man's slave or beaks a bone of a man's slave, he shall pay one-half his price".[^practicalfairness]
Fortunately, such simplistic laws are no longer adhered to in modern society.
However, the idea that we can quantify fairness is still very much alive, and still not without its problems.

Fairness in ML and AI has seen enormous interest over the last decade, as many researchers and developers try to bring order to a vast and complex landscape of statistical concepts and measures.
This is understandable.
Statistical measures are, typically, well-defined and bring a quantitative dimension to the discussion of fairness.

Attempting to translate fairness into a statistical problem, therefore, has a certain allure because it suggests that if we can get the right measure or formula we can "solve" the problem of fairness in an objective manner.
However, the allure of this statistical or computational approach needs to be properly seen for what it is—an important tool, but one that has many limitations and needs to be used responsibly.
This section is about understanding both the strengths and limitations of statistical fairness.

----

As a simple example of one limitation, consider the following question:

> Is it fair for a company to build an AI system that monitors the performance of employees in a factory and automatically notifies them when they are not meeting their targets? Or, should the company instead invest in better training and support for employees and work with their staff to understand obstacles or barriers?

----

As you can probably guess, there is no easy answer to this question.
In fact, you may have realised that a lot depends on factors that are not appropriately specified in the question (e.g. what happens if the staff continue to miss targets, who is this company, and who are their staff?).
However, putting aside these unspecified factors, it should be evident that such a question cannot be reduced solely to some statistical problem.

Here, judging the fairness of the company's actions would require a more holistic approach, one that takes into account the social and cultural context of the company and its staff.
Within this more holistic approach, statistical fairness could be one tool that is used to help inform the discussion, but it will not be the only tool.

Therefore, in this section we will take a critical look at some proposed statistical concepts and techniques for understanding fairness in ML and AI, paying close attention to how they can contribute to the discussion of fairness, and where they fall short.

---

## Fairness in Classification Tasks

The area of ML and AI that has seen the most attention, with respect to fairness, is the task of classification.

In a classification task, the goal is to predict which class a given input belongs to.
This high-level description can be applied to a wide range of tasks, including:

- Which object is depicted in an image?
- Is this applicant likely to default on a loan?
- Is a post on social media an instance of hate speech?
- Will this patient be readmitted to hospital within 30 days?

----

In this section, we are going to follow this approach and build up our knowledge of statistical fairness by using a simple binary classification task as a running example.

This simplification has a cost and a benefit:

- The cost is that we will not be able to capture the full complexity of statistical fairness in ML and AI, such as exploring all relevant metrics, or different types of problems (e.g. multi-class classification, non-classification tasks, etc.)
- The benefit is that we will be able to focus on the core concepts and ideas and link them to ethical and social issues of fairness.

With these caveats in mind, let's dive in.

---

### Classifying Patients: A Running Example

First of all, let's introduce our classification task.

We're going to pretend that we are developing a classification model, using supervised learning, which will try to predict whether a patient has a disease or not based on a set of features.
We're not going to worry about what this disease may be or what the features are that we are using to train the model—our first oversimplification.
Instead, let's just assume we have a sample of 100 patients and 2 of them have the disease.

<!-- show and label graphic of 100 people with two coloured differently -->

----

#### Statistical Fairness Issue #1: Class Imbalance and Accuracy

When designing this model we may decide that evaluating the performance of this model is best achieved by measuring its accuracy.

However, this is where we come to our first problem that statistical fairness has to address: class imbalance.

In our sample of 100 patients, only 2 of them have the disease.
This means that if we were to build (an admittedly terrible) model that always predicted that a patient does not have the disease, then the model would have an overall accuracy of 98%.
From one (very limited) perspective, the model would be performing very well, but we obviously know that this is not the case.

When we have imbalanced classes (i.e. a small number of positive examples), which is quite common, such a simplistic notion of accuracy is not going to be sufficient.

----

<!-- begin admonition -->
**Positive Classes**

In ML classification, the term 'positive class' is used to refer to the class that we want to identify or predict.
For example, in a medical diagnosis task, the positive class might represent the presence of a particular disease, while the negative class represents its absence.

This can sometimes cause confusion, as the term 'positive' is often used to refer to something that is good or desirable, but in some tasks the identification of the positive class may be related to an undesirable or negative outcome.

<!-- end admonition -->

----

Fortunately, there is a well-understood approach to dealing with this initial limited notion of accuracy: a confusion matrix.

A confusion matrix is a table that helps us to visualise the performance of a classification model by showing where it is doing well and what errors it is making.
The table below shows a typical confusion matrix, where the rows represent the predicted label (i.e. the model's prediction) and the columns represent the ground truth (i.e. the actual state of the world).

| | Predicted Positive | Predictive Negative |
| --- | --- | --- |
| **Actual Positive** | True Positive (TP) | False Negative (FN) |
| **Actual Negative** | False Positive (FP) | True Negative (TN) |

----

Let's translate this general table into our running example.

Here, the positive class is the presence of the disease, while the negative class is the absence of the disease.
Therefore, the four cells would correspond to the following:

- True Positive (TP): The patient has the disease and the model correctly predicts that they have the disease.
- False Positive (FP): The patient does not have the disease but the model incorrectly predicts that they do have the disease.
- False Negative (FN): The patient has the disease but the model incorrectly predicts that they do not have the disease.
- True Negative (TN): The patient does not have the disease and the model correctly predicts that they do not have the disease.

<!-- combine Jonny's illustration with above list-->

----

<!-- begin admonition -->
**Different Terminology**

Depending on your background, you may know some of the terms in this section by different names.
For example, 'False Positive' is sometimes referred to as a 'Type I Error', and 'False Negative' is sometimes referred to as a 'Type II Error'.

This will also hold true for terms we define later in the section.
This is unavoidable in such a multi-disciplinary context, although it is acknowledged that this can be confusing.
Therefore, you may need to refer to this section more frequently than other sections.

<!-- end admonition -->

----

We can also represent this visually based on how our model performs with respect to the original dataset.
Let's return to our original 100 patients, but this time let's make the disease a bit more prevalent (30/100 patients have the disease now), and let's see how well our model does at predicting the disease this time around.

<!-- show the circular graphic where 30 patients having the disease and the following distribution of positives and negatives

- TP: 25
- FP: 5
- FN: 5
- TN: 65

 -->

This graphic allows us to see where our model has classified positive and negative instances correctly and incorrectly.
The data points (i.e. patients) within the circle represent the positive class (i.e. having the disease), whereas those outside of the circle represent the negative class (i.e. not having the disease).
And this approach also allows us to use the four values to define new metrics that can help us to understand the performance of our model.

----

Let's define some key terms, all of which are based on the original confusion matrix:

| Name | Label | Definition | Formal Definition |
| --- | --- | --- | --- |
| Actual Positives | `P` | The number of data points in the positive class, regardless of whether they were predicted correctly or incorrectly. | `P` = `TP` + `FN` |
| Actual Negatives | `N` | The number of data points in the negative class, regardless of whether they were predicted correctly or incorrectly. | `P` = `TP` + `FN` |
| Accuracy | `ACC` | The proportion of data points that were correctly predicted from the entire dataset. | `ACC` = (`TP` + `TN`) / (`P` + `N`) |
| Precision (or Positive Predictive Value) | `Precision` or `PPV` | The proportion of data points that were predicted to be in the positive class that were actually in the positive class. | `PPV` = `TP` / (`TP` + `FP`) |
| Recall (or True Positive Rate, or Sensitivity) | `Recall` or `TPR` | The proportion of data points in the positive class that were correctly predicted. | `TPR` = `TP` / `P` |
| False Positive Rate | `FPR` | The proportion of data points in the negative class that were incorrectly predicted. | `FPR` = `FP` / `N` |

----

<!-- begin admonition collapsible -->
**List of Metrics**

There are many more performance metrics that can be extracted from a confusion matrix.
We focus on the above ones simply because they are the ones needed for understanding some of the key concepts we discuss later on in this section.
 
The following table from (Ruf and Detyniecki, 2019)[^ruf] is a helpful reference for the most common metrics, and their paper is also an excellent resource in general.

<!-- add image of table of metrics -->

<!-- end admonition -->

----

We have already looked at the first three terms, let's now look at `Precision` and `Recall`.

These two terms are often introduced in tandem because of how they relate to each other.
To get a general understanding of the two terms, consider the following image.

<!-- insert fishing image -->

Here, the objective is to catch as many fish as possible.
We can think of catching a fish as our *positive class*.

The "model" used by the boat on the left (i.e. the fishing rod) is very precise—it catches some of the fish (i.e. true positives), but not all of them, and it does not produce match bycatch (e.g. false positives).

The "model" used by the boat on the right, however, has good recall—it catches all of the fish (i.e. true positives), but it also produces a lot of bycatch in the process (e.g. false positives).

This analogy is useful because it allows us to understand the trade-off between precision and recall.
As we raise the classifier's boundary (i.e. increase the size of the net), we will increase the number of true positives for our model (or minimise the number of false negatives).
However, in doing so, we will also increase the number of false positives.

But, if we lower the boundary (i.e. decrease the size of the net), we return to a situation where we have a higher number of false negatives but a lower number of false positives.

This is known as the **precision-recall trade-off**, and it brings us to our next statistical fairness issue.

#### Statistical Fairness Issue #2: The Precision-Recall Trade-off

This issue of where to "draw" the boundary for our classifier is a common one in machine learning, and the choice is in part a value-laden one.
Both precision and recall are trying to maximise the number of true positives, but they also try to minimise the number of false positives and false negatives, respectively.

| Metric    | Measures           | What it Optimises |
|-----------|--------------------|-------------------|
| Precision | TP / (TP + FP)     | Lower Rate of False Positives |
| Recall    | TP / (TP + FN)     | Lower Rate of False Negatives |

Another way to frame the intuition here is that `precision` is not as concerned with a higher number of false negatives, whereas `recall` is not as concerned with a higher number of false positives.

But, this translates into very different scenarios in the real world.

In our running example, a model with high `precision` will result in a higher rate of false negatives, which means that some patients may not receive treatment when they need it.
In contrast, a model with high `recall` will capture more of these patients but some patients who do not need to be sent for further tests may have their time wasted and receive unsettling news.

In general, therefore, a high recall value is typically preferred because we generally deem the harm caused by a false positive to be less than the harm caused by a false negative.
But this is not always the case—"recall" our example of the fishing boat, where minimising the amount of bycatch may be more important to ensure sustainable fishing practices.

----

The following quotation is helpful in understanding this trade-off:

> "A general rule is that when the consequences of a positive prediction have a negative, punitive impact on the individual, the emphasis with respect to fairness often is on precision. When the result is rather beneficial in the sense that the individuals are provided help they would otherwise forgo, fairness is often more sensitive to recall."
> -- Ruf and Detyniecki (2021)

---

So far, we have been treating our sample of patients as a homogeneous group.
That is, we haven't considered whether there are different subgroups of patients that can be identified based on relevant characteristics (e.g. age, sex, race, etc.).
This is a big gap from the perspective of fairness, but it requires us to add some further complexity to our running example.

Let's assume that our 100 patients are now split according to their sex, and we have the following distribution.

<!-- insert graphic of people with 60 males and 40 females -->

With this update to our example, we need to return to our confusion matrix once more and consider how this new information alters our understanding of the performance of our model.

<!-- male table -->
| | Predicted Positive | Predicted Negative | Class Total |
| --- | --- | --- | --- |
| **Actual Positive** | 10 | 10 | 20 |
| **Actual Negative** | 2 | 38 | 40 |

<!-- female table -->
| | Predicted Positive | Predicted Negative | Class Total
| --- | --- | --- | --- |
| **Actual Positive** | 7 | 6 | 13 |
| **Actual Negative** | 22 | 5 | 27 |

Let's now calculate some of the performance metrics for these two confusion matrices and show them side by side:

| Metric | Male | Female |
| --- | --- | --- |
| True Positive Rate (`TP` / `P`) | 0.5 | 0.5 |
| False Positive Rate (`FP` / `N`) | 0.05 | 0.8 |
| True Negative Rate (`TN` / `N`) | 0.95 | 0.2 |
| False Negative Rate (`FN` / `P`) | 0.5 | 0.5 |
| Accuracy ((`TP` + `TN`) / (`P` + `N`))| 0.8 | 0.3 |
| Precision (`TP` / (`TP` + `FP`)) | 0.8 | 0.24 |
| Recall (`TP` / (`TP` + `FN`)) | 0.5 | 0.5 |

As you can see, there is variation in the performance of our model across the two subgroups in a number of categories.
Although recall is the same for the two groups, the accuracy and precision of the models differ significantly because of the higher number of false positives for females.

Translated into practical terms, this means our current model will be failing to identify a large number of female patients who need to be sent for further tests or treatment.

What can we do about this?

A few options include:

- Collecting more data and improving representativeness of the dataset
- Altering the decision threshold for the model to see if the precision-recall trade-off can be improved
- Oversampling from the minority group to balance the classes

But before we consider these options, we need to touch upon another important issue: the base-rate.

In the previous example, the base rate was the same for both subgroups.
As a reminder, the base rate is just the number of positive cases divided by the total number of cases ($BR = P/P+N$).
But this is often not the case in real-world datasets, especially in healthcare where the prevalence of a disease can vary drastically between different subgroups (e.g. affecting elderly patients more than young patients).

So, let's now change the base rate and review how this affects our model.

<!-- male table -->
| | Predicted Positive | Predicted Negative | Class Total |
| --- | --- | --- | --- |
| **Actual Positive** | 10 | 10 | 20 |
| **Actual Negative** | 2 | 38 | 40 |

<!-- female table -->
| | Predicted Positive | Predicted Negative | Class Total
| --- | --- | --- | --- |
| **Actual Positive** | 18 | 9 | 27 |
| **Actual Negative** | 11 | 2 | 13 |

Let's now calculate some of the performance metrics for these two confusion matrices and show them side by side:

| Metric | Male | Female |
| --- | --- | --- |
| Base Rate (`P` / `P` + `N`) | 0.33 | 0.68 |
| True Positive Rate (`TP` / `P`) | 0.5 | 0.67 |
| False Positive Rate (`FP` / `N`) | 0.05 | 0.85 |
| True Negative Rate (`TN` / `N`) | 0.95 | 0.15 |
| False Negative Rate (`FN` / `P`) | 0.5 | 0.33 |
| Accuracy ((`TP` + `TN`) / (`P` + `N`))| 0.8 | 0.5 |
| Precision (`TP` / (`TP` + `FP`)) | 0.8 | 0.62 |
| Recall (`TP` / (`TP` + `FN`)) | 0.5 | 0.67 |

So, now we have a situation where the likelihood of having the disease is higher for females (i.e. a base rate of 0.68 for females and a base rate of 0.33 for males), and they are under-represented in our dataset.
And, although there is an improvement from the previous set of metrics, this hypothetical model still has higher levels of performance for males who are over-represented in the dataset and less likely to have the disease.

Intuitively, this is not a fair model.
But can we be more precise in saying how?

A first pass would appeal to moral and legalistic notions of non-discrimination and equality.
We have not spoken about which features our model is using in this running example, but let's pretend that the project team did not collect any data on the sex of the patients.

Perhaps, they were motivated by a conception of fairness-as-blindness, where the model should not be able to discriminate against any protected characteristic.
However, despite not collecting the data, the model still ended up discriminating against female patients.

In legal terms, this would be an instance of 'indirect discrimination', which means that some rule, policy, practice, or in this case an 'algorithm', appears to be neutral with respect to how it treats different groups, but has a disproportionate impact on a protected group.

<!-- begin admonition -->

As defined in the UK's Equality Act 2010[^ukgov], it is against the law to discriminate against anyone because of the following protected characteristics:

- age
- gender reassignment
- being married or in a civil partnership
- being pregnant or on maternity leave
- disability
- race including colour, nationality, ethnic or national origin
- religion or belief
- sex
- sexual orientation

When we refer to a 'protected group', we are referring to a group whose members share one of these characteristic.

<!-- end admonition -->

The notion of a protected characteristic is a helpful one in FairML, because it allows us to bring additional precision to our discussion and helps us identify different types of fairness (and unfairness) as we will see in the next section.

However, it is not without it's problems.

For instance, poverty is not a protected characteristic in the UK, but it is a well-known predictor of poor health outcomes.
As such, if a model is trained on data that includes a proxy for poverty (e.g. the area in which a patient lives), it may end up discriminating against patients from poorer areas, but there would be little legal recourse for the patients to seek redress.

Therefore, although we will refer to 'protected characteristics' in the remainder of this discussion, it is worth keeping in mind whether the moral scope of this term is adequately captured by the legal definitions.

### Statistical Fairness Definitions

If we are to choose responsible methods for addressing inequalities in model performance, such as the ones we have just discussed, we need to be able to precisely identify how and where the model is failing to be fair.

Let's define a few key variables to help us with this task:

- $Y$ is the *outcome* variable, which is the variable we are trying to predict.
- $\hat{Y}$ is the *prediction* of the model.
- $A$ is the *protected characteristic*.

There are three widely used formal criteria of *non-discrimination* that all bear on the relationship between the model's performance across sub-groups.
They help us formally define a variety of 'group-level' fairness criteria, which we can then use to determine targets for our model to achieve or constraints to impose during the development of the model.

The three criteria can be described formally and intuitively as follows:

- Independence:
  - Formal: $A$ is *unconditionally* independent of the model's predictions $\hat{Y}$
  - Intuition: the outcome of a model's predictions should be the same for all individuals, regardless of their membership in a protected group
- Separation:
  - Formal: $A$ is *conditionally* independent of the model's predictions $\hat{Y}$, given the true outcome $Y$
  - Intuition: the proportion of correct predictions are equal across groups, (e.g a male and female who have the disease are equally likely to be correctly predicted as having the disease)
- Sufficiency:
  - Formal: $A$ is *conditionally* independent of the true outcome $Y$, given the model's predictions $\hat{Y}$
  - Intuition: 

Starting with independence, we can see the immediate appeal of this criterion: all groups have an equal opportunity to be predicted correctly.
But, as we have already seen this is unrealistic when, for instance, the base rate of a disease is different across groups.

And, in cases where 


### The Fairness Compass: A Tool for Choosing Fairness Criteria

Group-level fairness only provides assurance that the average member of a (protected, marginalsied or vulnerable) group will be treated fairly. No guarantees are provided to individuals.

---

## Unused Resources

- A statistic is defined as sufficient if it cannot be replaced with another statistic derived from the same sample, such that the replacement provides any additional information regarding the parameter in question.
- Problems of defining fairness across individual and group level categories
    - Fair when considered as an inividual level, but due to historical factors baseline rates of reoffence are different across black and white defendants (i.e. black people are more likely to be arrested in the US due to systemic racism).
    - It is not possible to be fair to individuals and groups at the same time.
- Obermeyer et al. (2019) example of algorithm that contained racial bias due to using predicted health care cost as proxy for how sick an individual was. However, due to systemic barriers to accessing healthcare, or implicit biases of healthcare professionals, Black individuals were treated unfairyly. Only 17.7% of patients that the algorityn assisgned to receive extra care were Black.
    - Other examples: 
        - Women are less like to be shown ads for high paid positions like CEO or CFO (Datta et al. 2015)
        - Black women are less likely to be inaccurately classified by facial recognition systems (Buolamwini & Gebru, 2018)
- > "One danger of the machine learning approach is that it leads to a narrow focus on the accuracy of the prediction. In other words, “good” decisions are those that accurately predict the target. But decision making might be “good” for other reasons: [...]] considering only relevant factors (in a sense we’ll discuss below), considering the full set of relevant factors, incorporating other normative principles (e.g., need, desert, etc.), or allowing people to understand and potentially contest the policy. Even a decision making process that is not terribly accurate might be seen as good if it has some of these other properties." (Barocas et al. 2019)

---

[^practicalfairness]: O'reilly book on practical fairness (Aileen Nielsen)
[^ruf]: Ruf, B., & Detyniecki, M. (2021). Towards the Right Kind of Fairness in AI. https://axa-rev-research.github.io/static/AXA_FairnessCompass-English.pdf
[^ukgov]: https://www.gov.uk/discrimination-your-rights