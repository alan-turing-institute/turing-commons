---
slideOptions:
  theme: solarized
  transition: slide
  spotlight:
    enabled: true
  mouseWheel: false
---

# The Project Lifecycle (Section 3) Model Development

> **Note**
> The following sections are part of this module:
>
> - Section 1: [What is the Project Lifecycle?](rri-101-1.md)
> - Section 2: [Project Design](rri-101-2.md)
> - Section 3: [Model Development](rri-101-3.md)
> - Section 4: [System Deployment](rri-101-4.md)

---

## Model Development

In this section we will begin by looking at the second set of tasks of the project lifecycle model, which are concerned with the model development stage.
For each task, a description is given as well as information about the importance of the typical activities associated with the task, including issues that have an ethical significance.

![](https://i.imgur.com/9bqnkQL.png)

<!-- copy this image to repo and update url-->

---

### Preprocessing and Feature Engineering: Task Description

Data analysis can give rise to valuable insights (e.g. business intelligence), but not all the data types that have been collected will be appropriate to train ML algorithms.
Therefore, `preprocessing and feature engineering` involves transforming the data into a form that is suitable for the next stage of the project lifecycle.
This typically involves the cleaning, normalising, or otherwise refactoring of data into the features that will be used in model training and testing, as well as the features that may be used in the final system.

Features, therefore, may not be the same as the raw data that are collected in the prior stages. 
Rather, they may represent a combination of multiple data types, and as such may not always be interpretable to the end user.

---

### Preprocessing and Feature Engineering: Importance of Task

Features are dependant upon, but separate from, the raw data that are collected in the prior stages. 
They can be engineered by hand or through the use of algorithmic techniques to improve the performance of subsequent ML processes.

However, the features that are used in the process of `model training`, for instance, do not only affect the model's accuracy or predictive power, they also impact the ethical consequences of the project (e.g. reducing explanatory potential of system, creating discriminatory outcomes). 
Therefore, selecting the _best_ features is a vital, albeit often time-consuming and complicated task can involve trade-offs about which parameter to optimise for (e.g. predictive power versus interpretability).

---

### Preprocessing and Feature Engineering: Illustrative Example

<!-- insert case study partial -->

---

### Model Selection and Training: Task Description

This task involves the selection of a particular process (or algorithm) for training a model, and the training of the model itself.

There are many factors that feed into the decision of which model to select, including (but not limited to):

- Access to computational resources (some learning algorithms require vast levels of computational power)
- Predictive performance of model (as compared to other models)
- Properties of underlying data (e.g. is the size of the dataset sufficient)

`Model training` is the process of fitting a statistical model to some training data.
The process of training is typically iterative and proceeds by optimising the model's parameters (e.g. weights) to increasingly minimise the error between the model's predictions and the true values of the training data.
The `problem formulation` task is important here because the target variable that was previously determined will guide the choice of model and the training process.

In addition, this task requires the project team to split their data into a _training_ and _testing_ set to prepare for the next task.

---

### Model Selection and Training: Importance of Task

There are, of course, many technical and logistical reasons for the responsible selecting and training of a model (e.g. ensuring parsimony, optimising performance).

However, a key concept in the responsible development of a model is the inherent interpretability and post hoc explainability of the model and the behaviour of the system into which it is implemented.
Although there are nuances and exceptions, it is generally the case that more complex models are harder to interpret and explain (e.g. linear regression versus convolutional neural networks). 
Selecting the right technique, therefore, depends on the ultimate use case of the model and system.

In addition, the training process can be computationally intensive and may require the use of specialised hardware (e.g. GPUs).
This gives rise to issues of sustainability and fairness, as the training process may be inaccessible to some individuals or groups.

---

### Model Selection and Training: Illustrative Example

<!-- insert case study partial -->

---

### Model Testing and Validation: Task Description

`Model testing and validation` involves using the testing set from the previous task to evaluate the performance of the trained model.
The evaluation of the model can be carried out against a variety of metrics, but typically includes the evaluation of the model's accuracy as applied to novel data (held out from the original training data).

This form of testing is sometimes known as _internal validation_, as it is carried out using a subset of the dataset that was used to train the model.
In addition, the project team may also wish to evaluate the model's performance against entirely new data (external validation), which may be collected from a separate trial or even carried out by a separate team.

---

### Model Testing and Validation: Importance of Task

Where a dataset is split into testing and training data, or where a model's performance is evaluated against wholly new data (e.g. external validation from a separate trial or project team), there are options to assess more than just the model's performance.

For instance, testing the generalisability of a model to a new domain or context can also help ensure the model is both sustainable and fair (e.g. has similar levels of accuracy or performance when validated externally).
In addition, the model can be evaluated for its interpretability and explainability (e.g. how well the model's predictions can be explained by the features that were used to train it).
If the model has low interpretability or explainability, then the project team may wish to consider retraining the model with a different set of features or employ some post hoc explanation techniques (e.g. LIME or SHAP).

---

### Model Testing and Validation: Illustrative Example

<!-- insert case study partial -->

---

### Model Documentation: Task Description

This task involves the documentation of both formal and non-formal properties of the model and the processes by which it was developed. 
This includes (but is not limited to):

- Data sources and summary statistics
- Model used (e.g. proprietary model purchased from vendor)
- Model parameters (e.g. weights)
- Evaluation metrics (e.g. model performance)
- Model performance (e.g. accuracy)
- Model limitations (e.g. bias)
- Model assumptions (e.g. normality of data)

The categories of information that are documented will depend on the project's requirements.
For example, if the project is part of an academic research project, then the team may wish to follow the standard format of a scientific paper or a pre-specified protocol. 
In contrast, if the project is part of a commercial development project, then the team may have additional requirements based on a procurement process or other contractual obligations.

:::success

**Model Cards for Model Reporting**

See Mitchell et al. (2018) [Model Cards for Model Reporting](https://arxiv.org/abs/1810.03993) for one proposed template that project teams can use as a starting point for model documentation.
Teams may wish to adapt this template to suit their own needs.

:::

---

### Model Documentation: Importance of Task

Clear and accessible documentation is an important form of responsible project governance for the following reasons:

- In research projects it ensures reproducibility and replicability of results, as well as other values associated with [open research](https://the-turing-way.netlify.app/reproducible-research/open.html?highlight=open%20science), such as public accessibility.
- In commercial projects it ensures accountability and transparency of decision-making processes, which may be required by law or by contractual obligations.
- In can help affected individuals seek redress for any harms that may arise from the design, development, or deployment of data-driven technologies.
- It allows project team members, who may be responsible for downstream tasks, to review and reflect on the project's progress and outcomes so far.

<!-- admonition -->

What other reasons are there for clear and accessible documentation?

<!-- end admonition -->

---

### Model Documentation: Illustrative Example

<!-- insert case study partial -->

---
