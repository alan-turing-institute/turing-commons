{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"welcome/","title":"Welcome","text":""},{"location":"welcome/#welcome","title":"Welcome \ud83d\udc4b","text":"<p>Welcome to the Turing Commons\u2014a home for resources and tools to help you reflect, discuss, and take responsibility for the design, development, and use of data-driven technologies. On this site you will guidebooks, activities, case studies, blog posts, and more. All of these resources have been designed to help you develop the skills and knowledge you need to be a responsible data scientist, data engineer, or data science leader.</p> <p>This site is a living resource, however, which means the content is constantly being updated, revised, and refined. At present, the resources centre upon three skills tracks that focus on the following broad topics:</p> <ul> <li>Responsible Research and Innovation (RRI)</li> <li>Public Engagement of Data Science and AI (PED)</li> <li>AI Ethics &amp; Governance (AEG)</li> </ul> <p>\u27a1\ufe0f Head on over to our Skills Tracks page to learn more about each of these tracks and the resources available.</p>"},{"location":"welcome/#target-audience","title":"Target Audience \ud83c\udfaf","text":"<p>At present, our resources are primarily aimed at academic researchers and data science professionals who are interested in developing their skills in responsible data science and AI. As such, they assume a certain level of technical expertise and familiarity with data science and AI.</p> <p>However, we are always looking to expand the resources available on this site, so if you have any suggestions for new content that you think we should prioritise, please let us know! For instance, we have also worked on resources for members of the public who are interested in understanding the ethical, social, and legal issues surrounding data-driven technologies:</p> <ul> <li>Citizen's Guide to Data: Ethical, Social and Legal Issues</li> </ul>"},{"location":"welcome/#contributing","title":"Contributing \ud83e\udde9","text":"<p>As the name suggests, the Turing Commons is inspired by the research of nobel prize-winning economist Elinor Ostrom, and specifically her work on governing the commons. Although we do not emulate the open editing and governance model of other digital commons platforms, such as Wikipedia, we do believe that the best way to create a resource that is useful to a wide range of people is to make it open and accessible to all. This is why all of the material you will find on this site is available for free through a Creative Commons Attribution 4.0 International (CC BY 4.0) license.</p> <p>Therefore, if you'd like to learn more about how you can get involved and help contribute to these resources, please head on over to our contributing guide on GitHub.</p>"},{"location":"assets/images/illustrations/","title":"Reuse of Illustrations","text":"<p>All of the illustrations in this folder were commissioned for the purpose of the Turing Commons project and were illustrated by Johnny Lighthands.</p> <p>As with the rest of the content in this repository, you are free to copy and redistribute the material in any medium or format, as well as remix, transform, and build upon the material for any purpose, even commercially. However, when doing so, you must give appropriate credit both to the illustrator and this project, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</p> <p>Creative Commons Attribution 4.0 International License.</p> <p></p>"},{"location":"assets/images/illustrations/high-res/","title":"Reuse of Illustrations","text":"<p>All of the illustrations in this folder were commissioned for the purpose of the Turing Commons project and were illustrated by Johnny Lighthands.</p> <p>As with the rest of the content in this repository, you are free to copy and redistribute the material in any medium or format, as well as remix, transform, and build upon the material for any purpose, even commercially. However, when doing so, you must give appropriate credit both to the illustrator and this project, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</p> <p>Creative Commons Attribution 4.0 International License.</p> <p></p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/course-announcement-ai-ethics-and-governance/","title":"Course Announcement: AI Ethics and Governance","text":"<p>What do we mean when we talk about the ethics of artificial intelligence? If guided by popular culture and science fiction, we would probably turn to images of seemingly sentient robots and questions on the ethics of our (mis)treatment of them.<sup>1</sup></p> <p></p> <p>However fascinating they may be, we are not yet at a stage where we need to grapple with these considerations. But this does not mean that we should not care about AI ethics. Quite the contrary. As our society becomes increasingly reliant on algorithms and other data-driven technologies, their functionings and outcomes have immense consequences on individuals, communities, and society at large.</p> <p> *Figure 1. Illustration representing three types of bias\u2014social, statistical, and cognitive\u2014that can affect the design, development, and deployment of AI systems.</p> <p>There is no shortage of concerns, and these keep expanding as our uses of technology multiply. What do privacy and autonomy mean in a datafied society? How do we define and implement fair, non-biased outcomes when using AI systems? And what should the requirments of explainability and transparency be as AI and machine learning become increasingly complex and opaque? These are just some of the ethical questions that the ubiquity of AI gives rise to.</p> <p>We will explore these and other topics in the AI Ethics and Governance course delivered by the Turing Commons team.</p>","tags":["ai ethics"]},{"location":"blog/course-announcement-ai-ethics-and-governance/#about-the-course","title":"About the Course","text":"<p>The course will take place between the 21<sup>st</sup> and the 25<sup>th</sup> of November, and will be delivered by Professor David Leslie and the Turing Commons team. The five day course is scheduled between 10am and 4pm (GMT), and each day will comprise a series of lectures, hands-on sessions through structured activities, and group discussions. The course will be delivered online using Zoom, and to ensure effective group discussion will be limited to 30 participants.</p> <p>After completing the course, participants should have an understanding of the following: - what practical ethics is and how it serves as a foundation for AI ethics; - how AI ethics can be understood in terms of AI harms which violate certain core values; - what the stakeholder engagement process is and what it entails; - what a stakeholder impact assessment is and how to carry one out; - the different elements of AI fairness; - the problem of bias in AI and how to address it; - the key elements of transparent and explainable AI; - what the CARE &amp; ACT principles are and how to apply them in their own research.</p> <p>Following the live workshops, recordings of the taught components will be made available to all (along with relevant materials) for asynchronous self-study. </p> <p>Applications are now open for registration. Application deadline is November 7<sup>th</sup>. If you have any queries, please reach out to the Turing Commons project organiser, Claudia Fischer.</p> <p>We look forward to seeing you in AI Ethics and Governance soon!</p> <ol> <li> <p>Some recent(ish) examples are the novels Machines like Me by Ian McEwan, Klara and the Sun by Kazuo Ishiguro, and the HBO TV show Westworld.\u00a0\u21a9</p> </li> </ol>","tags":["ai ethics"]},{"location":"blog/participatory-design-workshops/","title":"Participatory Design Workshops","text":"","tags":["rri"]},{"location":"blog/participatory-design-workshops/#a-collaboration-between-the-ai4er-cdt-at-cambridge-university-and-the-turing-commons-project","title":"A collaboration between the AI4ER CDT at Cambridge University, and the Turing Commons project","text":"<p>It\u2019s not every day that you get to sit down on a wintery morning, cup of tea in hand, to help direct the future of an innovative new course to be delivered by the Turing Commons.</p> <p>My name\u2019s Orlando, and I\u2019m currently a Master of Research (MRes) student with the Artificial Intelligence for Environmental Risk (AI4ER) CDT at the University of Cambridge. Our course aims to harness the increasing availability of large datasets and powerful compute to better understand physical systems in order to inform national and international policy, and make a tangible, positive impact on the environments in which we live.</p> <p>The beverage-backed online meeting followed a general call by the Turing Commons team \u2013 part of the Alan Turing Institute \u2013 for all MRes, PhD, and early career researchers in AI4ER to have their voice heard.</p> <p>The call is responding to a broad consensus which demands that the development of AI methods be widely accessible, transparent for stakeholders, and, above all, guided by an understanding of any potential ethical ramifications from inception to deployment, and beyond.</p> <p>Given the multi-faceted and wide-reaching research of the AI4ER CDT\u2019s MRes and PhD students such knowledge is especially vital for us, and we\u2019re also well-placed to help guide the establishment of such a course. Within this collaboration, a variety of voices \u2013 including the very earliest of early career researchers such as myself and fellow MRes student Andrew McDonald \u2013 are vital to understand each aspect of the field including project lifecycles, open science tools, and public communication of AI technologies.</p> <p>Our efforts in that first meeting focused on designing the structure and delivery of a Responsible Research and Innovation (RRI) skills track. The COVID-inspired discussions of in-person vs asynchronous online learning resurfaced and concluded with a resolution to harness the best of both. This will likely involve making a variety of participation options available to everyone in accordance with their academic (or non-academic) background and availability. This theme of inclusivity was also reflected in the importance of including activities and technology spotlights that would be of universal interdisciplinary appeal, as well as different domain-specific case studies that would allow various disciplines to use the resources most effectively. The ability to provide continued feedback regarding the course\u2019s effectiveness \u2013 invaluable to the evaluation of its success \u2013 will also be embedded in its delivery.</p> <p>Putting something together on this scale necessarily takes a great deal of time. Production of the skills track is under way, supported by ongoing meetings and the hard work of the Turing Commons's team. Our initial meeting was followed up by a case study design workshop, where we collectively developed case studies on issues around AI and environmental sciences. The final participatory design workshop will take place Monday, March 6<sup>th</sup> 2023, where we will have an in-person opportunity to trial some of the course materials on explainability, our newly developed case studies, and various other new activities.</p> <p>The Turing Institute is always on the lookout for stakeholders willing to offer their experience, advice, and a little of their time to help direct the responsible future development of artificial intelligence and machine learning methods. In particular, there is a need for case studies on diverse research areas, as the team is building a repository of such studies to enable people from all backgrounds to receive tailored course content.</p> <p>If you\u2019d like to get involved, please get in touch with Clau Fischer, part of the Turing Commons team.</p> <p>In the meantime, check out the Turing Commons existing skills tracks and the original version of their courses, and watch this space for 2023\u2019s RRI course!</p>","tags":["rri"]},{"location":"blog/accessible-and-representative-imageries-and-imaginaries-of-ai---aiuk24-fringe-event/","title":"Accessible and Representative Imageries and Imaginaries of AI - AIUK24 Fringe Event","text":"<p>This page provides further detail on the Accessible and Representative Imageries and Imaginaries of AI event, to be held as a hybrid event on March 28<sup>th</sup> at The Alan Turing Institute's office in London.</p> <p> Illustration by Jonny Lighthands. </p>","tags":["events"]},{"location":"blog/accessible-and-representative-imageries-and-imaginaries-of-ai---aiuk24-fringe-event/#key-information","title":"Key information","text":"<p> March 28<sup>th</sup>, 2024</p> <p> 1pm-5pm </p> <p> The Alan Turing Institute office, The British Library</p> <p>To join the event, please visit the registration page here to fill out the form.</p> <p>This event is part of the AI UK Fringe 2024, a series of events exploring key topics around data science and AI. To see the full lineup of events, click here. </p> <p>If you have any questions about registration, or require further details, please contact Youmna Hashem at yhashem@turing.ac.uk</p>","tags":["events"]},{"location":"blog/accessible-and-representative-imageries-and-imaginaries-of-ai---aiuk24-fringe-event/#agenda","title":"Agenda","text":"<p>12:30pm: Introductions, arrival and networking.</p> <p>1pm - 2:15pm: Panel discussion + Q&amp;A.</p> <p>2:15 - 2:30pm: Break.</p> <p>2:30pm - 4:15pm: Workshop. </p> <p>4:15pm - 5:00pm: Wrapping up - with refreshments being served at the location. </p>","tags":["events"]},{"location":"blog/accessible-and-representative-imageries-and-imaginaries-of-ai---aiuk24-fringe-event/#introduction-to-the-event","title":"Introduction to the event","text":"<p>The event focuses on two intersecting normative axes of accessibility and representativeness, which are in urgent need of increased critical engagement in global AI images and visual art practices. The event will involve:</p> <ol> <li>A panel discussion with invited speakers profiling relevant research within the Turing and by external partners on AI ethics, data justice, and open science; </li> <li>An interactive workshop designed to engage participants in a critical examination of AI-generated imagery.</li> </ol>","tags":["events"]},{"location":"blog/accessible-and-representative-imageries-and-imaginaries-of-ai---aiuk24-fringe-event/#aims-and-objectives","title":"Aims and objectives","text":"<p>The overarching aim is to address the hidden assumptions, abilities, knowledges, and interpretations that shape how AI is represented in images and is used in image generation. The goal of this event is both to explore ways to make AI more accessible to different human capabilities (e.g. social, physical, and mental abilities), and to investigate how to make AI outputs more representative of diverse populations, reflecting a broader range of lived experiences.</p> <p>By the end of the event, we hope you will feel better equipped to critically examine representations of AI, the visual outputs these systems generate, and how they can be better reflective of different abilities, forms of knowledge, diverse lived experiences, and socio-cultural global practices. </p>","tags":["events"]},{"location":"blog/accessible-and-representative-imageries-and-imaginaries-of-ai---aiuk24-fringe-event/#panel","title":"Panel","text":"<p>The introductory panel discussion will be on normative framings of data justice and accessibility principles based on research undertaken within the Turing Commons, Turing Way, and Advancing Data Justice project teams. The panel discussion will provide critical and normative framings for the ensuing interactive workshop activity, which invites a curated mix of participants to delve into the often-unseen human labour and subjective interpretations that contribute to the creation of AI imagery. The panel discussion will interrogate how these can be made more inclusive and representative of a wider public. </p> <p>Join a stellar list of speakers:</p> <p>Dr SJ Bennett Postdoctoral Researcher - Algorithmic Societies, Durham University</p> <p>Corrine Chan London-based Taiwanese Artist</p> <p>Sarah Selby Visual artist and Senior Lecturer, Creative ComputingSenior Lecturer, Creative Computing, University of the Arts London</p> <p>Dr Olive Gingrich Artist and Programme Lead, University of Greenwich</p> <p>Dr Mike Katell Ethics Fellow, The Alan Turing Institute &amp; Visiting Senior Lecturer,Digital Environment Research Institute, QMUL.</p>","tags":["events"]},{"location":"blog/accessible-and-representative-imageries-and-imaginaries-of-ai---aiuk24-fringe-event/#workshop","title":"Workshop","text":"<p>The workshop will involve, as a first activity, a socio-technical deconstructing of curated AI Imageries where participants will engage in the critical analysis of a selection of images of the AI technologies and AI-generated images. Building on insights from this first activity, participants will then collaboratively reimagine the foundations of AI imagery.</p> <p>The workshop is aimed at a broad and multidisciplinary group of stakeholders, including artists, researchers, students, and creative and technology professionals, for a diverse participation.</p> <p>Both the panel and the workshop will provide an opportunity for an open and accessible discussion on the roles of underlying algorithms, design choices, and contextual factors that can influence AI imagery. Activities will examine how AI imagery can represent diverse subjects and environments, replicates biases and harms, and how a normative framing can allow for a practical and ethical appraisal of AI image creation.</p>","tags":["events"]},{"location":"blog/how-ordinary-residents-are-creating-camden-councils-data-charter/","title":"How ordinary residents are creating Camden Council\u2019s Data Charter","text":"","tags":["public engagement"]},{"location":"blog/how-ordinary-residents-are-creating-camden-councils-data-charter/#reposted-from-the-londons-office-of-technology-and-innovations-blog-by-sam-nutt-link-to-the-original-post-here","title":"Reposted from The London's Office of Technology and Innovation's blog by Sam Nutt. Link to the original post here.","text":"<p>Over three weekends in January, Camden Council are running a Resident\u2019s Panel with a group of local residents to reflect on, sense-check and make improvements to their Data Charter, which was created a year earlier.  At LOTI, we\u2019ve identified resident engagement as one of the key capabilities for councils to become more responsible innovators and ethical users of data, and given my other area of work on innovative participation methods, I was excited to attend the second weekend meeting of this Panel, and blog about my impressions. </p> <p></p>","tags":["public engagement"]},{"location":"blog/how-ordinary-residents-are-creating-camden-councils-data-charter/#background-to-the-residents-panel","title":"Background to the Resident\u2019s Panel","text":"<p>This Charter was originally created in 2021 and implemented in January 2022, using a combination of participatory methods, including distributed dialogues in local community spaces and a Resident Panel.  It is the first data policy of its kind, fully proposed and developed by residents and adopted by the Council.  The Charter established seven principles that Camden Council was committed to working by in their data projects, as well as made recommendations for data governance practices for Camden.</p> <p>One of those commitments was to reconvene a new Panel a year after the Data Charter\u2019s implementation \u2013 which I was attending this January.  This Second Panel was all about transparency and accountability, two of the principles of the Charter.  It invited a representative group of residents from the borough to reflect on how well Camden had fulfilled their commitments to the Charter, and where the Charter could be improved.  The day I attended the panel, the residents were working through four case studies of data projects to see how the Data Charter had been applied, assessing where they thought the charter worked or didn\u2019t, and where it could be improved. </p> <p>Brendan Kelly, the Data Custodian for Camden Council, explained why Camden was doing the process:</p> <p>Quote</p> <p>We knew that the Data Charter wasn\u2019t going to be perfect the first time around, and we said at the time that it was going to be a living document.  This was always an important part of the process, being able to go back to residents, a second set of residents with fresh eyes, and also on the work in between.  It keeps the democratic process going.</p> <p></p>","tags":["public engagement"]},{"location":"blog/how-ordinary-residents-are-creating-camden-councils-data-charter/#my-reflections-on-the-day","title":"My reflections on the day","text":"<p>Above all, the day showed just how well ordinary people are able to give valuable and thoughtful direction around data governance and innovation, when trusted to do so. One common misconception about participatory processes is that they don\u2019t work as well for more \u2018technical\u2019 knowledge barriers. However, one hour in the room or breakout groups of Camden\u2019s Resident Panel showed just how false this is. Residents were partly aided by the Alan Turing Institute, which developed a \u2018Residents Guide to Data\u2019 \u2013 a great resource for any local authority wanting to help educate residents on how councils use data. </p> <p>Claudia Fischer, Researcher at the Alan Turing Institute and a facilitator in the Resident Panel, said:</p> <p>Quote</p> <p>Instances like the Camden Resident\u2019s Panel highlight the importance of communication and effective dialogue between the developers and deployers of AI systems, and those using them.  Residents were not only eager to learn more about how these systems work and how data is stored and used, but they also raised relevant issues about how those issues can personally impact them and other users of Camden\u2019s services.</p> <p>Residents really made great points and suggestions, that the council might otherwise have missed.  It resonated with a recent article from Democracy Next, on \u2018How non-expertise becomes a strength\u201d.  It describes how a lack of existing preconceptions or ideology on expert topics sets the foundation for better deliberation, where insights emerge through co-learning, respectful listening and sharing of ideas, which leverage a range of lived experiences. </p> <p>The key issue that emerged in discussions was the importance (and lack of) communication about data.  Participants thought the Charter and the work of the Council were good \u2013 when they knew about it.  However, they felt the Council could do more actively communicating how and why they were making decisions using the Data Charter to residents.  It also spoke to broader questions about how the benefits of increased trust from participating in a small, deliberative body (the Panel was 20 residents) can be scaled up to the whole community.</p> <p>Because the process clearly built trust amongst participants in the council.  At the start of the process, none of the residents fully realised what Camden was doing with data, including how they responsibly and ethically managed ethical risks.  And, trust in data may well translate into broader trust in the council in other areas, so this is an immensely worthwhile process.  This quote from Simon, a Camden resident, put it best:</p> <p>Quote</p> <p>The most important thing is the fact that we\u2019re in the room, and that the council is setting stuff up so that we\u2019re asked about it.  People fear the unknown, like AI, or profiling. But, if you share [how you are working on] it with them, share why, then you are going to get so many better outcomes.  If you engage, you will get better outcomes for everyone else.</p> <p></p> <p>Some of the other insights on Camden\u2019s practices that the residents reflected on included:</p> <ul> <li>A possible flaw that residents identified in the data collection process of one of the case studies, and thus the data quality, of a research project. </li> <li>The manner in which digital exclusion might produce inequalities when data processes only benefit those who can use the internet.</li> <li>The broad importance of accessibility, when it comes to understanding information about data, as well as accessing council services and communication. </li> <li>More explainable, jargon-free language \u2013 and in general, clearer communication \u2013 for different user groups.</li> <li>Sharing the evaluation of the efficacy of automated processes versus human processes.</li> </ul> <p>Another takeaway that I found interesting was how ordinary people do not see or experience the council in terms of discrete services or policies.  People are naturally systems thinkers.  For example, when discussing one case study on monitoring air quality actually inside private properties, residents asked whether the council was factoring in new policies like the introduction of Low Emission Zones.  People also brought in phenomena like digital exclusion, the cost of living crisis, the historical closure of public services, and other things, constantly.  When it comes to communicating with residents and articulating a vision of life in the borough, it highlighted how councils need to tell better, inter-connected stories about their work.  I would point interested readers to the Centre for Public Impact\u2019s work on Storytelling for Systems Change as a good starter for this.</p> <p>Overall, my day observing the Charter demonstrated exactly why councils need to be engaging with residents, and what a good version of that looks like.  Residents can provide real insight and input, elevating policy to a level that a council wouldn\u2019t otherwise be able to match \u2013 even on the most technical of topics.  And residents wanted to do this, they were invested in the process, believed that what they said would directly impact the council\u2019s work, and ultimately improve the lives of the communities they are part of.  My next step is to take these learnings and see how we can foster similar practices across London, to truly foster a more participatory data ecosystem in whatever borough you live in.</p>","tags":["public engagement"]},{"location":"blog/peeling-back-the-layers-of-science-and-technology/","title":"Peeling Back the Layers of Science and Technology","text":"<p>In April 2022, we ran our first week-long course on Public Engagement of Data Science and AI with 25 fantastic researchers from across the globe.</p> <p>This post explores why we developed the course, outlines the activities we ran with the course participants, and concludes with next steps.</p>              Your browser does not support the video tag.  <p>\"Any sufficiently advanced technology is indistinguishable from magic.\"</p> <p>Arthur C. Clarke's phrase was made famous in the context of his science fiction stories, which were full of futuristic and magical-seeming technologies and alien worlds that captured the imagination of many of his readers. But the advances of science and technological innovation that we see all around us in our everyday lives can also seem magical at times, as well as feeling bewildering and obscure. For many researchers and developers, a key goal of public engagement is about supporting people to dispel the feeling of bewilderment through increased knowledge and understanding, while still maintaining a feeling of enthusiasm and excitement about the many possibilities and opportunities to use science and technology to improve society. This is no easy task!</p>","tags":["public engagement"]},{"location":"blog/peeling-back-the-layers-of-science-and-technology/#course-summary","title":"Course Summary","text":"<p>Therefore, over the course of a week in April 2022, 30 participants from different research disciplines joined a course being run as part of the Turing Commons project on 'Public Engagement of Data Science and AI'. It was organised around a series of structured presentations, seminars, and group activities, as well as a capstone project which involved a hypothetical public engagement project.</p> <p>The course was designed with several learning objectives in mind, roughly split into two main sections. First, participants critically examined the practical and ethical values of public engagement with data science and AI. They explored what public engagement is and the different forms that it can take, as a way of creating a robust theoretical foundation. Next, the course turned to practical methods and tools for engaging with the public responsibly, such as how to carry out stakeholder analysis activities, structure a clear and accessible message, or communicate uncertainty in research.</p> <p>To support these learning objectives, Sir David Spiegelhalter\u2014Chair of the Winton Centre for Risk and Evidence Communication at the University of Cambridge\u2014gave an excellent talk on trustworthy communication about data and algorithms. His presentation focused on the difficulties of interpreting (and therefore communicating) data and statistics, drawing on many examples from his public engagement work during the Covid-19 pandemic.</p>","tags":["public engagement"]},{"location":"blog/peeling-back-the-layers-of-science-and-technology/#designing-a-hypothetical-case-study","title":"Designing a Hypothetical Case Study","text":"<p>A primary goal for the course participants was to develop a hypothetical public engagement project by the end of the course. Working in three smaller groups, the participants built upon the material covered each day through a series of activities that tested the skills they were acquiring in the course while also providing an opportunity to share their different perspectives with one another.</p> <p>One of these activities was an in-depth stakeholder analysis focused on identifying, understanding, and analysing stakeholders, led by Cami Rinc\u00f3n (part of the Ethics and Responsible Innovation team). The exercise involved conducting a mock analysis intended to get participants thinking about the questions and issues they would confront were they to engage with stakeholders in a real project. For this, they had to identify affected stakeholders from a set of personas, build out their specific characteristics, and then scope the potential impacts of the project on them.</p> <p>From there, and taking into account the different roadblocks and enablers that could impact potential stakeholders, the groups decided on a public engagement objective as well as which of the various public engagement method(s) they would use to achieve their goals.</p> <p> Example of hypothetical personas identified as impacted stakeholders for a group working on a skin condition triaging app.</p> <p>On the final day of the course, the three groups presented their hypothetical project, supported by illustrations they had also co-designed with Eleonore Guerra.</p> <p> Illustration created by Eleonore Guerra for group working on a app which allows the local community to voice their views about local transport infrastructure.</p> <p>During their capstone presentations, the groups introduced their hypothetical project, and discussed its main ethical implications. They also presented their stakeholder personas and the project's potential impacts on them. Taking all of this into consideration, they then defined their public engagement objective and method, and explained how they would (hypothetically) carry it out.</p> <p> Public engagement workshop plan taking into account the potential concerns of impacted stakeholders.</p> <p>After the course ended, we asked participants for anonymous feedback on whether and how the course was useful for them. Overall, the course was incredibly well received:</p> <p>Quote</p> <p>Very clear presentations, guest speakers also added a great depth to the experience.</p> <p>The activities and group work helped understand the course content really well so I would say that the activities are the highlights.</p> <p>The teaching was very well thought out and enjoyed the interactivity aspect of the training.</p>","tags":["public engagement"]},{"location":"blog/peeling-back-the-layers-of-science-and-technology/#concluding-remarks","title":"Concluding Remarks","text":"<p>Public engagement is sometimes treated as an after-thought by researchers and scientists\u2014something that is important for funding or building a career, but mainly an ancillary activity that exists at the borders of research and development. As such, opportunities to learn about its social and ethical value, and also put principles into practice, are limited.</p> <p>This was the first time that this course was run, but as we continue to build out the content of the Turing Commons and grow its community, we hope that we will be able to both support and learn from more participants. In the meantime, a huge thanks to the first group who made the course such a pleasure to run.</p>","tags":["public engagement"]},{"location":"blog/turing-commons-roadmap/","title":"Turing Commons Roadmap","text":"<p>This blog post sets out a general roadmap and explores a series of objectives for the Turing Commons as we head into 2023.</p> <p></p> <p>The Turing Commons started out, as all good projects do, with an idea and a name, presented to the Turing's Data Ethics Group in the Winter of 2019 ( to David Leslie and Christina Hitrova).</p> <p>With the important decision out of the way, our plan was to build a community platform (the \u201ccommons\u201d) to host and support a set of resources that were freely open and accessible to all people with an interest in the ways that data-driven technologies are changing society.</p> <p>Those who are familiar with Garret Hardin\u2019s influential analysis of the \u2018Tragedy of the Commons\u2019<sup>2</sup>, will appreciate why the \u201ccommons\u201d cannot be left unmanaged if it is to serve a sustainable and collective benefit. However, such governance or curation should not occur in the dark, in case it ends up serving the vested interests of a small minority.</p> <p>So, in the spirit of transparency and openness, this post sets out our current roadmap for how and where we plan to develop the Turing Commons, and how we will work with others to ensure that the resources are co-designed to serve genuine needs and challenges.</p>","tags":["objectives"]},{"location":"blog/turing-commons-roadmap/#goals-and-objectives","title":"Goals and Objectives","text":"<p>From the start of this project, a key goal has been to develop high-quality resources for academic researchers whose work involves the design, development, or evaluation or data-driven technologies, including machine learning or artificial intelligence. To that end, we have produced and delivered three courses on the following topics:</p> <ol> <li>Responsible Research and Innovation</li> <li>Public Engagement for Data Science and AI</li> <li>AI Ethics and Governance</li> </ol> <p>For each course we started by hosting a series of workshops with 10-15 researchers to identify the topics, questions, and issues which were most important to them. The feedback we received from these workshops was used to develop our original courses.</p> <p>We have learned a lot from planning and delivering these courses. Following the delivery of our first course on Responsible Research and Innovation, we sought feedback from the participants, including a specific request for recommendations for improvements.</p> <p>Several participants suggested more time to explore the new ideas being presented. In response, we redesigned our second course to have a better balance between time spent delivering new material and time spent exploring the material during activities or group discussion. Others recommended closer integration of our activities with the core material. For this, we ensured that our next course has a clear thread which ran through each of the days, progressively building up to a capstone activity.<sup>1</sup></p> <p>The benefit of these small but important changes was clear in the feedback from our second course:</p> <p>Participant Feedback</p> <p>The activities and group work helped understand the course content really well so I would say that the activities are the highlights.</p> <p>Being responsive to the changing needs of our course participants is a key objective for us, and is central to the above goal of creating high quality (and needs driven) resources. However, designing content and resources that are valuable for all of our participants has been challenging given the multi-disciplinary setting of our courses. For example, if we provided an illustrative example to help explain a key concept, those participants who happened to have a background in the respective area (e.g. healthcare) were able to grasp the idea more readily. Addressing this challenge leads us to the first of our new objectives.</p> <p></p>","tags":["objectives"]},{"location":"blog/turing-commons-roadmap/#modular-and-tailored-resources","title":"Modular and Tailored Resources","text":"<p>At present, we are revisiting the content of our three courses and focusing on the following set of objectives:</p> <p>First Objectives</p> <ol> <li>Revise the content of each course based on what worked well, what did not, and what needs updating.</li> <li>Modularise the courses to allow more flexibility for participants who are unable or do not wish to take a 5-day long course.</li> <li>Design a more flexible set of materials and resources that can be tailored to different disciplines.</li> </ol> <p>Let\u2019s look at the (2) and (3).</p> <p>Asking researchers to block five days from their schedules to attend a course is demanding, and can also be exclusionary for some (e.g. those with parenting responsibilities, or part-time jobs). Designing our courses in this way allowed us to test our material as a whole, but our next step is to modularise the courses to enable a more flexible approach that supports different modes of learning.</p> <p>To this end, we have started splitting our original courses into three main components:</p> <ol> <li>Core modules: the primary topic areas covered in our courses</li> <li>Optional modules: additional content that can be brought across from other courses, or take specific concepts in different directions (e.g. data privacy and protection)</li> <li>Activities: the set of activities (both self-directed and group-based) that help users understand the module\u2019s topics</li> <li>Case Studies: illustrative case studies that anchor the topics in concrete cases and practical examples</li> </ol> <p>Separating these components allows us to design and develop a modular approach where users can tailor our content and resources to their own skills and training needs. For example, a researcher in robotics could select 3 of our recommended (core) modules on public engagement, combine them with introductory modules on AI ethics (from a separate course) and supplement them with the recommended activities and domain-specific case studies to support their learning. This results in a modular approach to building \u2018skills tracks\u2019 depicted in Figure 1.</p> <p> Figure 1: a schematic depicting the modular approach to our skills tracks, comprising core (and optional) modules, activities, and case studies</p> <p>In addition to being more flexible and tailored, this also enables us to focus on novel approaches to self-directed learning, which has so far been absent on our platform (e.g. creating an online learning environment for individuals who cannot attend hosted courses).</p> <p>While this sets out our general approach, there is an important component missing... who is involved in the redesign and redevelopment?</p>","tags":["objectives"]},{"location":"blog/turing-commons-roadmap/#participatory-design-and-user-engagement","title":"Participatory Design and User Engagement","text":"<p>Developing domain-specific case studies that can be used to tailor core modules to the needs of researchers in different disciplines requires domain-specific expertise. Therefore, the redesign and redevelopment of our content and resources has to be conducted in a participatory manner.</p> <p>Recently, we have started collaborating with the UKRI CDT in Biomedical AI (University of Edinburgh) and the UKRI CDT in AI for the study of Environmental Risks (University of Cambridge) to develop and evaluate two of our newly designed core modules for Responsible Research and Innovation. This participatory design work will include the co-creation of tailored case studies for their respective domains (e.g. predictive diagnostics for healthcare, earth monitoring and surveillance technologies). It will also allow us to work with domain experts to identify the most pressing needs and challenges related to skills and training gaps in responsible research and innovation.</p> <p></p> <p>A key output from our planned workshops will be to evaluate two core modules on \u2018AI Fairness\u2019 and \u2018Explainable AI\u2019, which will be adapted to the context of the two Centres for Doctoral Training (CDT). In addition, we will evaluate activities for self-directed learning and group activities. These activities will also make use of the case studies (to be co-developed) that will help ground the content in practical and domain-specific examples.</p> <p>Although limited to two domains during our pilot phase, we intend to explore further areas following our initial evaluations. However, scaling in this way can be time-consuming and slow, so we are also researching and developing a new technical infrastructure to support the process.</p>","tags":["objectives"]},{"location":"blog/turing-commons-roadmap/#building-an-open-infrastructure","title":"Building an Open Infrastructure","text":"<p>Our current website is hosted by GitHub pages, and uses the fantastic Material for MkDocs as a static-site generator. This set-up has allowed us to focus on creating written content using Markdown, rather than worrying about web development. However, there are limitations to static sites, and some of these limitations also serve as barriers to our next objective:</p> <p>Second Objective</p> <p>Create an open platform that supports interactive and self-directed learning approaches that are customisable to different users and groups.</p> <p>As mentioned in the previous section a key milestone on our project\u2019s roadmap is the development of domain-specific case studies that enable users to tailor our core modules to their domain (e.g. robotics or journalism). Another milestone for our future roadmap is to develop a case study repository and API to build additional functionality into our website. Doing so will allow users to better tailor skills tracks to their needs by using the available case studies and modules that ground the content of the modules.</p> <p>And, where there are gaps in our resources, an open API will allow partners to more easily integrate their own contributions into the platform, in turn supporting and caring for the commons.</p> <p>This goal requires us to do two things:</p> <ol> <li>Develop a database (or data store) to serve case study data to our front-end</li> <li>Enable more interactivity in our website, which uses the database to present specific content to users based upon pre-specific information and preferences, such as<ul> <li>Disciplinary background</li> <li>Research priorities</li> <li>Prior knowledge of topics</li> <li>Time available for learning</li> </ul> </li> </ol> <p>Figure 2 shows how this could work if using something like a JSON file to dynamically adjust elements of a template for our case studies.</p> <p> Figure 2: an example of how a JSON file could populate a HTML document for a case study on decision support systems</p> <p>However, the modular design of these case studies will also allow for partial updating of additional parts of our website (e.g. information boxes in our online guidebooks). Meeting these two objectives can be achieved using our current infrastructure as a proof-of-concept. However, our longer term plans will require a more thorough redesign to add features such as the following:</p> <ul> <li>Public API access to allow users to submit new case studies to our repository (i.e. feeding into the \u201ccommons\u201d)</li> <li>User authentication to allow individuals to keep track of their progress, store notes, and build new skills tracks that are customisable to their needs</li> <li>Improved interactivity for activities and learning that rely on modern web frameworks with state management functionality (e.g. React)</li> <li>Enabling communication and collaboration between users through our platform</li> </ul> <p>Implementing these features will take time and resources, and we hope to ensure our roadmap remains open to external collaborators who can steer the project in new an exciting directions. This brings us to our final objective.</p>","tags":["objectives"]},{"location":"blog/turing-commons-roadmap/#expanding-the-commons","title":"Expanding the Commons","text":"<p>Third Objective</p> <p>To further build on our current resources to meet the needs of additional groups and communities.</p> <p>Many of the topics and issues we have explored in our courses so far have importance beyond the academic community. We know this from some of our own public engagement work, as well as from discussions we have had with partners from the public, private and third sectors. Although we have started with an academic audience, this is not our end point.</p> <p>The following list contains examples of groups who we also wish to build resources with and for:</p> <ol> <li>Regulators and Policy-Makers</li> <li>Journalists and Science Communicators</li> <li>Members of the Public</li> <li>Industry Professionals (e.g. developers and data analysts)</li> </ol> <p>While there are topics in our current resources that will be of interest to some of these groups (e.g. public engagement for journalists; AI fairness for regulators), we cannot assume that the needs of the academic community will be met by our current offerings. Therefore, in the near future we have plans to work with these groups to co-design materials and resources that make the ethical, social, and legal issues of data-driven technologies clear and accessible to all.</p> <p>We have also recently taken part in a workshop with the UK Government\u2019s Science and Innovation Network, organised by the Turing\u2019s International team. The purpose of this workshop was to identify ways that we can work with international partners to support the global community and in turn co-create bidirectional forms of value.</p> <p>By expanding the scope of the Turing Commons in the above ways, we hope that our first objective also feeds into a wider-reaching goal.</p> <p>Overarching Goal</p> <p>To equip diverse groups and communities with the knowledge and understanding of data-driven technologies so they are able to participate fully in democratic forms of deliberation about how these technologies should be used to benefit individuals and society.</p> <p>We\u2019re excited about these plans and developments, and especially to working with new partners who are passionate about creating a responsible and trustworthy ecosystem for data-driven technologies.</p>","tags":["objectives"]},{"location":"blog/turing-commons-roadmap/#how-can-you-get-involved","title":"How can you get involved?","text":"<p>At the moment, we are still working on creating structured forms of support to allow interested parties to get involved. For instance, we will be releasing updates on this blog and on social media over the coming weeks with details of how to contribute to this blog directly.</p> <p>For the time being, the best way to get involved is to just to reach out to us and start a conversation. Do you have an idea for a case study? Do you want to take part in one of our evaluations? Do you have time to help us maintain the current GitHub repository?</p> <ol> <li> <p>See this blog post for more information.\u00a0\u21a9</p> </li> <li> <p>Hardin, G. (1968). The Tragedy of the Commons. Science, 162(3859), 1243\u20131248. http://www.jstor.org/stable/1724745 \u21a9</p> </li> </ol>","tags":["objectives"]},{"location":"resources/","title":"Resources","text":"<p>The following list is a collection of resources that either support our skills tracks (e.g. activities) or serve as stand-alone tools.</p> <ul> <li> <p> Gallery</p> <p>An interactive and searchable gallery of the illustrations used throughout our platform.</p> <p> Go to resource</p> </li> <li> <p> Activities</p> <p>A repository of activities that can be used to support self-directed and group learning.</p> <p> Go to resource</p> </li> <li> <p> Case Studies</p> <p>A repository of case studies that support our skills tracks and modules.</p> <p> Go to resource</p> </li> <li> <p> Bibliographies     ---</p> <p>A set of curated bibliographies (or, reading lists) on various topics to allow the interested reader to explore specific topics in more depth. Coming soon.</p> <p> Go to resource</p> </li> </ul>"},{"location":"resources/activities/","title":"Activities","text":"<p>Info</p> <p>On this page you will find a collection of our activities and other materials, which are designed to support either structured workshops or other forms of reflection, deliberation, assessment or general engagement. Some of these activities are designed to accompany our courses and modules. Others are more general purpose.</p> <p></p>"},{"location":"resources/activities/#general","title":"General","text":""},{"location":"resources/activities/#project-lifecycle-model","title":"Project Lifecycle Model","text":"<p>The Project Lifecycle is a heuristic model that serves as a \"cognitive scaffold\" to support the collective reflection, deliberation, and decision-making of teams and organisations throughout the various stages of a project's lifecycle. Why do we call it a \"heuristic model\"?</p> <p>We have a full module that explores the project lifecycle, as well as an interactive handout that provides a summary of the model and its constitutive stages. Please note you will need Adobe Acrobat Reader installed for the interactivity to work.</p> <p> Download the Project Lifecycle Interactive PDF</p>"},{"location":"resources/activities/#bias-cards","title":"Bias Cards","text":"<p>This resource is a deck of cards that can be printed (double-sided) and used to identify, evaluate, and mitigate social, statistical, and cognitive biases across the lifecycle of a project (e.g. to build an AI system).</p> <p>Our module on fairness, includes a section on the different types of bias.</p> <p> Download the Bias Cards</p>"},{"location":"resources/activities/#responsible-research-and-innovation-skills-track","title":"Responsible Research and Innovation Skills Track","text":"<ul> <li> <p> Understanding Responsibility</p> <p>Module: What is Responsible Research and Innovation? This activity helps participants reflect on a series of scenarios that constitute moral dilemmas. Participants will learn to identify moral agents and moral subjects, consider what the moral responsibilities of the agents are, as well as think about conflicts or interests or values that may arise.</p> <p> Download</p> </li> <li> <p> Collective and Distributed Responsibility</p> <p>Module: The Project Lifecycle This activity uses the project lifecycle model to get participants thinking about the different remits of responsibility that fall within different stages of the lifecycle. The activity prompts reflection about the upstream and/or downstream dependencies the successful completion of these tasks have, as well as the importance of understanding responsibility as collective and distributed.</p> <p> Download</p> </li> <li> <p> Bias Checklist *Reflect*list</p> <p>Module: Fairness (SAFE-D) This activity is designed to gte participants to identify and understand how different biases can impact the design, development, and deployment of data-driven technologies. It uses the project lifecycle model as a scaffold, and asks participants to consider how a variety of biases (social, statistical, and cognitive) could enter the project lifecycle and impact downstream decisions and outcomes. It also asks participants to think about tailored strategies for minimising the impact of biases.</p> <p> Download</p> </li> <li> <p> Selecting and Evaluating Claims</p> <p>Module: Explainability (SAFE-D)</p> <p>This activity is designed to identify, understand, and evaluate claims made about a project, data, model, or system in service to providing an explanation to a stakeholder or affected user. You will need to identify the objective of the claims as well as their quality, using a case study as an anchor.</p> <p> Download</p> </li> </ul>"},{"location":"resources/bibliographies/","title":"Bibliographies","text":"<p>Coming soon!</p> <p></p>"},{"location":"resources/case-studies/","title":"Case Studies","text":"<p>Info</p> <p>A repository of case studies that can be used to support directed and group learning. More case studies, as well as a new case study repository, coming soon!</p> Case Study Name Application Balancing Risk and Resilience in AI-driven Disaster Response AI in Humanitarian Action Using AI to help address psychosocial needs  AI in Humanitarian Action Shaping Climate Resilience Policy Environment and Public Health Air quality forecasting with an AI-driven system Environmental Automated app limits Mental health Decision support system Mental health Genetic disorder detection Biomedicine Peer to peer support Mental health Remote sensing in context Environmental"},{"location":"resources/gallery/","title":"Gallery","text":"<p>The following illustrations were commissioned for the Turing Commons and created by the wonderful illustrators Jonny Lighthands and El\u00e9onore Guerra.</p> <p>All files are licensed under a Creative Commons Attribution-ShareAlike 4.0 International.</p> <p>Information</p> <ol> <li>Please click on the images to see a full-screen version.</li> <li>You can filter images by tags using the filter below.</li> <li>You can access additional animations through our GitHub repository.</li> </ol>"},{"location":"skills-tracks/aeg/","title":"About this Course","text":"<p>This course is designed to help you understand the fundamentals of AI Ethics and Governance. The course begins with an introduction to metaethics and normative theories. It then follows with the practical ways AI systems can produce diverse harms to individuals, society, and even the biosphere, as well as the values that should be upheld when thinking about AI ethics.</p> <p>The course then goes into a deeper dive on the following topics: AI Sustainability through Stakeholder engagement and impact assessment, AI fairness and bias mitigation, accountability and governance, explainability and transparency, and the CARE &amp; Act principles.</p> <p>Throughout the course we will have time for Q&amp;A, group discussions, case studies, and structured activities to further the discussion and understanding of these concepts.</p>"},{"location":"skills-tracks/aeg/#who-is-this-guidebook-for","title":"Who is this Guidebook For?","text":"<p>Primarily, this guidebook is for researchers with an active interest in the ethics and governance of data science and AI.  This doesn't mean you have to be a data scientist or develop machine learning algorithms. You could also be an ethicist, sociologist, or someone with an interest in law and public policy.</p> <p>This course has practical, and sometimes hands-on activities that are designed to a) encourage critical reflection and b) help you build practical understanding of the processes associated with effective and responsible engagement with AI systems. While they can be carried out as part of individual and self-directed learning, they are most suited to group discussion.</p>"},{"location":"skills-tracks/aeg/#learning-objectives","title":"Learning Objectives","text":"<p>This guidebook has the following learning objectives:</p> <ul> <li>Get familiar with some key concepts in practical ethics. In particular, understand the metaethical motivation behind our course, as well as the main families of normative ethical theories.</li> <li>Understand the different kinds of harms that AI systems can create.</li> <li>Explore the different values that underpin the thinking and reflection behind issues in AI ethics.</li> <li>Understand the importance of AI sustainability and anticipatory reflection through a comprehension of the stakeholder engagement process (SEP) and stakeholder impact assessment (SIA).</li> <li>Explore some of the ethical issues around AI systems, such as: fairness &amp; bias mitigation, explainability &amp; transparency, and accountability &amp; governance.</li> <li>Learn the importance and use of the CARE &amp; ACT principles in developing AI systems where ethics is considered throughout the process and in an iterative manner.</li> </ul>"},{"location":"skills-tracks/aeg/#table-of-contents","title":"Table of Contents","text":"<ul> <li> <p> Introduction to Practical Ethics</p> <p>This chapter looks at foundational concepts of practical ethics, through two broad-brush introductions to: (i) metaethics and (ii) normative ethical theories.</p> <p> Go to chapter</p> </li> <li> <p> AI Harms and Values</p> <p>This chapter looks at the different kinds of harms AI systems may cause, as well as the values that should be used as goals and objectives when thinking about the ethics of AI systems.</p> <p> Go to chapter</p> </li> <li> <p> AI Sustainability and Stakeholder Engagement</p> <p>This chapter introduces the concepts of AI Sustainability and the importance of anticipatory reflection. We will develop sustainability by looking at the Stakeholder Engagement Process (SEP) and the Stakeholder Impact Assessment (SIA).</p> <p> Go to chapter</p> </li> <li> <p> Fairness, Bias Mitigation, Accountability, and Governance</p> <p>This chapter addresses various issues that arise from the use of AI systems: AI fairness and bias mitigation, as well as accountability and governance.</p> <p> Go to chapter</p> </li> <li> <p> Transparency &amp; Explainability and CARE &amp; ACT Principles</p> <p>The concluding chapter starts with the importance of transparency and explainability in AI systems. We then go through what we call CARE &amp; ACT Principles: (i) consider context, (ii) anticipate impacts, (iii) reflect on purpose, positionality, and power, (iv) engage inclusively, and (v) act responsibly and transparently.  Go to chapter</p> </li> </ul>"},{"location":"skills-tracks/aeg/chapter1/","title":"AI Ethics: Introduction to Practical Ethics","text":"Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk))"},{"location":"skills-tracks/aeg/chapter1/#chapter-outline","title":"Chapter Outline","text":"<ul> <li>Introduction to Metaethics</li> <li>Introduction to Normative Ethical Theories</li> </ul> <p>Chapter Summary</p> <p>This chapter introduces the basic theoretical building blocks for learning AI ethics. We start with a brief metaethical motivation, looking at different positions in metaethics and reflecting on how these affect the way we do practical ethics. Additionally, we will delve into the some of the main normative theories in practical ethics: consequentialist theories, deontological theories, virtue ethics, and biocentric ethics. These concepts will be useful to paint the ethical landscape against which the sub-field of AI (practical) ethics has developed.</p> <p>Learning Objectives</p> <p>In this chapter, you will:</p> <ul> <li>Learn what practical ethics is.</li> <li>Familiarise yourself with the main positions in metaethics, as well as learn about why we favour a procedural approach to ethics.</li> <li>Learn about some of the main normative ethical theories: consequentialist theories, deontological theories, virtue ethics, and biocentric ethics.</li> <li>Reflect on how these concepts in practical ethics create the wider ethical landscape on which AI ethics develops.</li> </ul>"},{"location":"skills-tracks/aeg/chapter1/metaethics/","title":"Introduction to Metaethics","text":""},{"location":"skills-tracks/aeg/chapter1/metaethics/#what-is-practical-ethics","title":"What is practical ethics?","text":"<p>Practical ethics is a branch of philosophy, in particular a branch of ethics concerned with examining the aims and principles of moral behaviour and applying them to various problems of our everyday lives or real-world scenarios in general.[@uehiro]</p> <p>In this course, we will look at issues in practical ethics which arise from our use of AI systems and the prevalence of data in our lives. It is important to note that these harms are not exclusive to the use of AI systems, but can be characteristic of them. To do so, we will introduce some key concepts in metaethics and some of the most relevant ethical theories in use today.</p>"},{"location":"skills-tracks/aeg/chapter1/metaethics/#what-is-metaethics","title":"What is Metaethics?","text":"<p>Metaethics, as its name suggest, attempts to understand the various aspects (ontological, epistemological, semantic, and psychological) that make up the presuppositions and commitments of moral thought, talk and practice.[@sep-metaethics2014]</p> <p>What does this mean? Metaethics deals with a variety of quite complex, yet relevant questions such as the following:</p> <ul> <li>Is there anything such an objective moral truth? Or is morality more a matter of taste?</li> <li>Are all cultural moral standards equally valid or true?</li> <li>Are there moral facts? And if they exist, what is their origin?</li> <li>How are moral facts (if they exist) related to other kinds of facts about our world?[@sep-metaethics2014]</li> </ul> <p>Clearly, none of these questions will have straightforward or easy answers, and we will not be dealing with them extensively in this course. However, in this brief introduction to practical ethics, we will look at different ways that some of these questions have been tried to answer.</p> <p>Quote</p> <p>What is morality?  Are there moral facts about the world?   If there are, can we know them?   If there are not, how do we decide on right and wrong in our actions?</p>"},{"location":"skills-tracks/aeg/chapter1/metaethics/#1-universalist-perspectives","title":"1. Universalist Perspectives","text":""},{"location":"skills-tracks/aeg/chapter1/metaethics/#11-moral-realism","title":"1.1 Moral realism","text":"<p>Moral realists, roughly speaking, accept the following two statements:</p> <ol> <li>There are such things as moral facts or properties, and;</li> <li>These are independent of human attitudes towards them (this is called attitude independence).</li> </ol> <p>In practice, this means that moral realists believe that objective moral facts do exist, and there is only one \"set\" of these facts. Objective moral truths also imply that things like subjectivity, or culture are not relevant when evaluating whether an act is good or bad. Moral realism is, in and of itself, neutral on what it is exactly that makes something good or bad; it just is prepared to state that moral facts exist. </p> <p>Of course, this does not mean that subjectivity or culture play no role in what people think is good or bad. However, the moral realist is prepared to say that those differences, and any human attitude over what makes something good or bad, is irrelevant when evaluating moral facts. That is, the reason why some things are good or bad do not hinge on whether people, culture, or society evaluate them as good or bad.</p> <p>The existence of objective moral truths clearly begs the question: where do these facts come from? This is what is known as the 'grounding problem'. </p> <p>One way of summing up moral realism is to say that epistemically they believe that \"there are moral facts, in the same way that there are planets and spoons\".[@rachels2019]</p>"},{"location":"skills-tracks/aeg/chapter1/metaethics/#12-procedular-ethics","title":"1.2 Procedular ethics","text":"<p>In procedural ethics, moral validity is bound by practices of giving and asking for reasons. In this way, a moral claim that is justified is one that can convince someone else by the \u201cunforced force\u201d of the better argument. </p> <p>Legitimate moral decision-making thus requires persuasion through rational justification and compelling argumentation. This means that procedular ethics begins by trying to reconstruct the requirements for rationality. These requirements include assuming an impartial perspective, which considers the interests of all affected by an action equally, and making universalisable moral judgments that are applicable in all like situations.</p> <p>Does this mean that procedural ethicists believe that moral facts exist? Yes and no. While processes of rational justification are supposed to yield a basis for valid moral claims, many procedural ethicists abstain from making substantive claims about moral reality. In their view, the proper role of moral theory is not to posit the fixed, and mind- or language-independent moral values and properties of the universe, but rather to facilitate and to normatively justify the continuation of ongoing rational dialogue.</p> <p></p>"},{"location":"skills-tracks/aeg/chapter1/metaethics/#2-non-universalist-perspectives","title":"2. Non-universalist Perspectives","text":""},{"location":"skills-tracks/aeg/chapter1/metaethics/#21-cultural-relativism","title":"2.1 Cultural Relativism","text":"<p>Cultural Relativism says, in effect, that there is no such thing as universal truth in ethics; there are only the various cultural codes that derive from social approval, and nothing more. Cultural relativism challenges our belief in the objectivity and universality of moral truth.[@rachels2019]</p> <p>Values and moral beliefs form an interconnected and coherent whole in a given cultural field or system of meanings, but this symbolic whole is relative to the historical background of the group it is derived from.</p> <p>There are no objective moral standards that go beyond the locality of cultures, and hence, no universal standards that could help adjudicate between them.</p> <p>Some examples of claims made by cultural relativists are:</p> <ul> <li>Different societies have different moral codes, and the moral code of a society determines what is right within that society.</li> <li>There is no objective standard that can be used to judge one society\u2019s code as better than another\u2019s. There are no moral truths that hold for all people at all times.</li> <li>The moral code of our own society has no special status.</li> <li>It is arrogant for us to judge other cultures. We should always be tolerant of them.</li> </ul>"},{"location":"skills-tracks/aeg/chapter1/metaethics/#22-moral-subjectivism","title":"2.2 Moral Subjectivism","text":"<p>Moral subjectivism takes relativism to a next level; instead of basing it on codes shared by whole cultures, they believe morality is an inherently individual matter. That is, what is morally right to do is up to the individual\u2019s desires, wants, tastes, or preferences.</p> <p>Therefore, in this view, moral language is not fact-stating language and thus does not contain mind-independent ethical truths. Instead, moral judgments are subjective expressions of feelings or attitudes, and so more like judgments of taste than statements of fact or rational justifications.</p>"},{"location":"skills-tracks/aeg/chapter1/normative/","title":"Introduction to Normative Ethical Theories","text":"<p>In contrast with metaethics, which is concerned with the nature of ethics, normative ethics is concerned with the content of ethical theories. Normative ethical theories attempt to give an answer to the question of what makes someone ethical, or what makes certain actions morally permissible. Ethical theories are varied and they come in different flavours. Here we will group them into four families of theories.</p>"},{"location":"skills-tracks/aeg/chapter1/normative/#consequentialist-theories","title":"Consequentialist Theories","text":"<p>Consequence-based theories are normative theories which, as the name suggests, define whether an action is morally permissible in terms of the consequences it brings about. In simple terms, they define morality as a matter of maximising the best possible consequences in any given situation. This then begs the question, what are the best consequences to be maximised? Consequentialists have given different answers to this question.</p> <p>One very influential flavour of consequentialism is utilitarianism, which specifies that the way we should rank consequences is by how much total (aggregate) utlity they bring about (where more utlity is better).<sup>1</sup> This means that when confronted with a decision, utilitarianism will tell you to choose whichever action maximises overall utility. This theory's practical impact is not to be discounted. It has served as the basis for classical economics and is widely taught to and applied by economists today. Other examples of consequentialist theories are hedonism, act conseuquentialism, and rule consequentialism.<sup>2</sup></p> <p>In essence, what makes a normative ethical theory consequentialist is that, given a metric for what good consequences are (as well as a way of ranking better and worse  consequences), it then states that in any situation, the morally approppriate way to act is to choose the action (among the existing possibilities) that brings about the best consequence.</p>"},{"location":"skills-tracks/aeg/chapter1/normative/#deontological-theories-or-principles-based-theories","title":"Deontological Theories (or principles-based theories)","text":"<p>Deontological theories in contrast, take a different approach. Morality is not to be measured by the consequences actions bring about, but instead morality dictates that one should follow a set of rules or principles regardless of what the consequences are. That is, the rightness of an action is determined by an agent\u2019s application of some universal standard, rule, or maxim of rightness irrespective of its consequences and independent of the interests or ends of the agent whose conduct the standard, rule, or maxim is guiding.</p> <p>The most famous proponent of deontological theories is Immanuel Kant, who was a firm believer in rule or duty-based morality, and who went as far as to define the maxim or principle to be followed in order to act morally which he called the Categorical Imperative. This rule roughly states that one must always act as if one\u2019s actions could be willed into a universal rule that everyone in society follows. </p> <p>In principle-based theories, morality is about following a set of rules or principles, and different theories will give different answers as to what principles one should act upon.</p>"},{"location":"skills-tracks/aeg/chapter1/normative/#virtue-ethics","title":"Virtue Ethics","text":"<p>Virtue ethics is not concerned with how people should act in a particular situation, but instead on what kind of people they are. Its roots can be found in Confucius and Mencius in the East and in Plato and Aristotle in the West. </p> <p>Instead of beginning with the question: \u201cWhat should I do?\u201d, it asks the question: \u201cWhat sort of person should I strive to become in order to live an ethical life?\u201d. Virtue ethicists emphasise that morality has to do with having a certain moral character which is achieved by cultivating virtues. A virtue is an excellent trait of character[@sep-ethics-virtue], and it comes in degrees.</p> <p>These traits may derive from natural tendencies, but they must also be nurtured. Under these theories, a person who acts in a kind or benevolent way (these would be the virtues) does not do so because doing so would maximise the outcome of her actions (as a consequentialist would), or because it is her duty to do so (as deontology would advise), but instead because acting in that way cultivates said virtues.</p> <p>The development of moral virtues through practice, discipline, and repetition is the purpose of the human form of life. For this ethical view to work, there must be some reserve of objective moral values available for agents to draw on to form virtuous habits and dispositions.</p> <p>Some of the virtues stressed in antiquity but also justified in modern secular ethics: wisdom (ability to exercise practical reason to determine right action); temperance (ability to remain cool-headed and guided by reason instead of emotion); courage (ability to confront danger boldly and with self-assurance); justice (ability to act impartially and with fairness).</p>"},{"location":"skills-tracks/aeg/chapter1/normative/#bio-centric-ethics","title":"Bio-centric ethics","text":"<p>Biocentric ethics attempts to justify moral responsibility and the rightness of action not in a human-centered way but in terms of the intrinsic value of all life forms and ecologies. It treats \u201cnature\u201d (the living biosphere) not as an instrument or resource available to be used to achieve the purposes of human industry, but rather as an entity that makes a moral claim on us.</p> <p>From a biocentric perspective, moral actions are those that preserve the flourishing and the diversity of all living beings and aim to secure the sustenance of the biosphere as a whole.</p> <p>For the purposes of the rest of the course we will focus mostly on the first two families of theories.</p> <p></p> <p>Example</p> <p>Let's take a classic example to help draw the distinction between consequentialism and deontology. Suppose an assassin comes to your door and asks if you have seen your friend Peter. And suppose you are indeed hiding Peter in your house as he is fleeing this very assassin. Should you tell the assassin the truth, or should you lie and tell him you don't know where Peter is?</p> <p>From a consequentialist perspective, it is clear what one should do. Telling the truth will result in Peter dying and lying will result in him living. The first consequence is a lot worse than the second, so you should lie to the assassin.</p> <p>However, from a deontological perspective it is not so obvious. If morality is about duty, and one of our duties is to be honest, then perhaps we also have a moral duty to tell the truth, even in this case.<sup>3</sup></p> <p>Most people would find the second conclusion morally abhorrent. This shows us that in some cases we seem to take a more consequentialist approach to ethics.</p> <p>However, the converse can also be the case.</p> <p>Example</p> <p>Imagine you are a surgeon and Peter, your patient, is peacefully recuperating after an appendicitis operation. He should shortly be back home. And imagine that five people in need of different organ transplants come into the hospital. If these people do not get a transplant soon they will die. Suppose Peter is a match for all of these people. A strict consequentialist doctor might decide that we should kill Peter in order to save the other five via organ transplants. After all the consequences of one life lost and five gained are a lot better than one life gained and five lost. </p> <p>Most people would find this conclusion morally abhorrent. A deontologist would reply that it is never permissible to use people as means to an end, since people are always ends in themselves. Therefore our duty to respect Peter's dignity and not use him as a means to saving the other five overrides any consequentialist considerations we might have.</p> <ol> <li> <p>The question as to what exactly utility is and how one goes about measuring it poses all sorts of complications which are beyond the scope of this introduction.\u00a0\u21a9</p> </li> <li> <p>For a list of different kinds of consequentialist theories see the Stanford Encyclopedia of Philosophy's entry on Consequentialism[@sep-consequentialism].\u00a0\u21a9</p> </li> <li> <p>There is a lot more nuance as to what a deontological theory would say in the example just described. However, this simplification helps illustrate a core difference between the two families of theories.\u00a0\u21a9</p> </li> </ol>"},{"location":"skills-tracks/aeg/chapter2/","title":"AI Harms and Values","text":"Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk))"},{"location":"skills-tracks/aeg/chapter2/#chapter-outline","title":"Chapter Outline","text":"<ul> <li>AI Harms</li> <li>AI Values</li> </ul> <p>Chapter Summary</p> <p>Following an brief introduction into the world of practical ethics, we now turn to AI Ethics in particular. We start this chapter by identifying the different kinds of harms AI systems can create, be it to particular individuals, communities, society as a whole, or even to the biosphere.  Additionally, we then turn to the values that drive issues behind AI ethics. In particular, we focus on what we call the SUM values, as they support, underwrite, and motivate a responsible innovation ecosystem. They are: protect, respect, connect, and care.</p> <p>Learning Objectives</p> <p>In this chapter, you will:</p> <ul> <li>Familiarise yourself with the different kinds of harms AI systems can create, as well as understand the level of harm they pose (individual, social, planetary).</li> <li>Look at the SUM Values and how they relate to AI ethics.</li> </ul>"},{"location":"skills-tracks/aeg/chapter2/harms/","title":"AI Harms","text":"<p>After that short introduction into metaethics and normative ethical theories, we will now start thinking about the specific context of artificial intelligence. What considerations are relevant when thinking about the ethics of using AI and data science in our everyday lives?</p> <p>As we saw in chapter 1, there are many different ways to think about what morality is and where it comes from, and the debate is far from settled (if one believes it ever can be settled in the first place). The question then becomes: how can we talk about AI ethics if we do not have a precise definition of what morality is or what it requires from us?</p> <p>This is no small barrier. To overcome it, we will draw on two traditions of moral thinking: a) bioethics and b) human rights discourse. Bioethics is the study of the ethical impacts of biomedicine and the applied life sciences. Human rights discourse draws its original inspiration from the UN Declaration of Human Rights. It is anchored in a set of universal principles that build upon the idea that all humans have an equal moral status as bearers of intrinsic human dignity.</p> <p>Most generally, human rights are the basic rights and freedoms that are possessed by every person in the world from cradle to grave and that preserve and protect the inviolable dignity of each individual regardless of their race, ethnicity, gender, age, sexual orientation, class, religion, disability status, language, nationality, or any other ascribed characteristic. These fundamental rights and freedoms create obligations that bind civil servants and governments to respecting, protecting, and fulfilling human rights. In the absence of the fulfilment of these duties, individuals are entitled to legal remedies that allow for the redress of any human rights violations. Human rights and freedoms, as have been codified in the Human Rights Act (1998) and in the European Convention on Human Rights (1953) can be then applied to the different aspects of an AI or data-driven system.</p> <p>Whereas bioethics largely stresses the normative values that underlie the safeguarding of individuals in instances where technological practices affect their interests and wellbeing, human rights discourse mainly focuses on the set of social, political, and legal entitlements that are due to all human beings under a universal framework of judicial protection and the rule of law. The main principles of bioethics include respecting the autonomy of the individual, protecting people from harm, looking after the well-being of others, and treating all individuals equitably and justly.</p> <p>The main tenets of human rights include the entitlement to equal freedom and dignity under the law, the protection of civil, political, and social rights, the universal recognition of personhood, and the right to free and unencumbered participation in the life of the community.</p> <p>Key Concept: Bioethics and Human Rights in Context</p> <p>The principles that have emerged from both traditions found their origins in moral claims that have responded directly to tangible, technologically inflicted harms and atrocities. That is, both traditions emerged out of concerted public acts of resistance against violence done to disempowered or vulnerable people. Whereas human rights has its origins in efforts to redress the well-known barbarisms and genocides of the mid-twentieth century, in the case of bioethics, its emergence tracked the public exposure in the 1960s and 1970s of several atrocities of human experimentation (such as the infamous Tuskegee syphilis experiment). In the latter instances, it was discovered that members of vulnerable or marginalised social groups had been subjected to the injurious effects of institutionally run biomedical experiments without having knowledge of or giving consent to their participation.</p> <p>It is drawing on these points of departure that we now turn to AI and data science ethics. Anchoring the foundation of AI ethics in real-world social injuries has been a useful strategy. It has enabled the scope of the values and ethical concerns that underwrite responsible practices in the design, development and deployment of AI and data- driven systems to be informed by the actual risks posed by their use. In this course we will focus on six main kinds of AI harms.</p> <p>Abstract</p> <ol> <li>Loss of autonomy, interpersonal connection, and empathy</li> <li>Poor qualities and dangerous outcomes</li> <li>Bias, injustice, and discrimination</li> <li>Widening global and digital divides</li> <li>Breaches in data integrity, privacy and security</li> <li>Biospheric harm</li> </ol>"},{"location":"skills-tracks/aeg/chapter2/harms/#1-loss-of-autonomy-interpersonal-connection-and-empathy","title":"1. Loss of autonomy, interpersonal connection, and empathy","text":"<p>Automated AI systems with the power to make decisions about people can have potentially dehumanising consequences for those subject to them.  Obviously, not all automated decisions are created equal. It is one thing if an automated system decides whether an email should be classified as spam or not, and quite another if an AI is in charge of allocating scarce social services, or deciding who gets hired for a job.</p> <p>Individuals may feel disempowered in the face of unstoppable automation, especially when these decisions are relevant to their sense of personal autonomy. This feeling can be compounded as well if there are few or no avenues in place to dispute or contest the automated decision.</p> <p>People may also feel as if they are being \"reduced to a statistic\" by these systems, or that the use of their personal data violates their privacy. Finally, but no less importantly, automation may also result in a loss of empathy and crucial human connection. </p> <p>For example, if AI is used to make or assist in making decision which impact people's lives, such as AI-assisted hiring tools there can be a loss of autonomy and interpersonal connection. People might want to know why they were not called to an interview, and knowing that there's no one to ask might be frustrating. Similarly, there is a loss of autonomy if the person does not then have a clear avenue where they can ask about the reasons about why they did not get called, and how they might improve in the future.</p>"},{"location":"skills-tracks/aeg/chapter2/harms/#2-poor-qualities-and-dangerous-outcomes","title":"2. Poor qualities and dangerous outcomes","text":"<p>Algorithmic models are only as good as the data on which they are trained, tested, and validated (commonly called \u2018Garbage in, garbage out\u2019). Inaccuracies, measurement errors, and sampling biases across data collection and recording can taint datasets. Using poor quality data could have grave consequence for individual well-being and the public welfare.</p> <p>This problem exist wherever datasets are used. The question of the quality of a dataset obviously comes in degrees, and it can be more or less dangerous depending on the context in which the data is used. It can also stem from multiple sources. Some can be honest mistakes, like typos or missing data points, while others derive from more profound problems, such as the structural biases that our society has which are then replicated in the datasets. But the main insight remains: as long as the data used for a model is not appropriate and does not accurately represent the underlying phenomenon it is studying, the outcomes of said AI model will not be accurate either.</p>"},{"location":"skills-tracks/aeg/chapter2/harms/#3-bias-injustice-and-discrimination","title":"3. Bias, injustice, and discrimination","text":"<p>Supervised machine learning models draw insights (learn) from the existing data patterns on which they are trained. When they are working reliably, they can make accurate, out-of-sample predictions from what they inferred from the training data. However, the problem is that these patterns may not be equitable or fair.</p> <p>In fact, this is very often not the case, and cases where data is biased, unjust or discriminatory sadly abound. Most (if not all) machine learning models are trained on historical data, and this means that as long as they do, they will be embedded with past and present injustices, forms of discrimination, and multiple biases which the data itself contains. Not only that, the machine learning algorithms will then reproduce, and maybe even amplify said biases.</p> <p>In day 4 we will look at the various different biases which can creep up in the ML/AI lifecycle. One crucial thing to keep in mind though, is that there is no fairness without awareness. When designing, implementing, and using these systems, conscious awareness of biases and explicit mitigation strategies must be discussed and implemented.</p> <p>Examples of this kind of harm come from different sectors of society. A 2019 paper published in Science showed how an algorithm used in US healthcare to predict patients' needs was producing racist results.[@obermeyer2019] The bias was introduced because the algorithm used past health costs as a proxy for health needs, which inadvertently favoured White patients. Less money was spent on black patients with the same level of need as their white counterparts, and the algorithm thus falsely concluded that black patients are healthier than equally sick white patients.</p> <p>Another famous example is Amazon's hiring algorithm which turned out to be biased against women. In an attempt to automate their hiring practices, Amazon developed an experimental hiring tool which used artificial intelligence to give job candidates scores ranging from one to five stars.[@martin2022] The algorithm quickly taught itself to discriminate against women candidates, penalising resumes which included the word women, (in 'women's chess club' for instance), as well as downgrading resumes which came from all-women colleges (ibid). Although the algorithm is not used by the company, (it was actually taken down precisely because of concerns about its sexism[@dastinjeffrey]), it serves as a powerful example of how AI can perpetuate and amplify historical biases (such as learning from the fact that traditionally Amazon has not hired many women, and extrapolating that to mean that women are not good employees).</p> <p></p>"},{"location":"skills-tracks/aeg/chapter2/harms/#4-widening-global-digital-divides","title":"4. Widening global digital divides","text":"<p>The use of AI systems is not distributed uniformly across different countries, or even within regions in the same country. The varying levels of access and use of these technologies can reinforce and amplify already existing digital divides and data inequities. It can also exacerbate exploitative data appropriation from less rich countries and institutions to more well-resourced researchers and companies in richer countries or in better-funded universities within one country. </p> <p>Long-standing dynamics of global inequality, for instance, may undermine reciprocal sharing between research collaborators from high-income countries (HICs) and those from low-/middle-income countries (LMICs).[@leslie2020a] Given asymmetries in resources, infrastructure, and research capabilities, data sharing between LMICs and HICs, and transnational research collaboration, can lead to inequity and exploitation.[@bezuidenhout2017]-[@leonelli2021]-[@shrum2005] That is, data originators from LMICs may put immense amounts of effort and time into developing useful datasets (and openly share them) only to have their countries excluded from the benefits derived by researchers from HICs who have capitalised on such data in virtue of greater access to digital resources and compute infrastructure.[@worldhealthorganization2022] </p> <p>In this way, the benefits of data production and research are not necessarily accrued fairly to originating researchers and research subjects, widening the already wide gaps between the more and less advantaged groups of researchers or communities. An example of this is what happened after the Omicron variant of the Sars-Covid-19 virus was first reported in South Africa. Researchers in the country conducted excellent research and were actually quicker in detecting the variant and sequencing its genome than other countries. However, instead of being lauded by the international research community, the country was rewarded with a travel ban from most of the world, even though it was unlikely to be useful (and actually it was later discovered that Omicron had already been present in Europe before it was detected in South Africa). Even though it wasn't the researchers who were punished in this case, the benefits of the research certainly did not get distributed fairly across the globe.[@bbc2021]</p> <p>These gaps in research resources and capabilities go beyond gaps between HIC's and LMIC's. They can also exist within the same country, between large research universities and technology corporations which are better positioned to advance data research given their access to data and compute infrastructures when compared to less well-resourced universities or institutions.[@ahmed2020]</p> <p>Another example comes from what is called \"parachute research\": researchers from the Global North conduct research in the Global South and then go back to their home countries with the data, without necessary regarding the interests of the researchers or data subjects the data was taken from. In a systematic review that examined African authorship proportions in the biomedical literature published between 1980 \u2013 2016 where research was originally done in Africa, scholars found that African researchers are significantly under-represented in the global health community, even when the data originates from Africa.[@mbaye2019]</p>"},{"location":"skills-tracks/aeg/chapter2/harms/#5-breaches-in-data-integrity-privacy-and-security","title":"5. Breaches in data integrity, privacy, and security","text":"<p>The ways we measure, collect, use, and store data points can lead to a multiplicity of harms to individuals. However, individual harms can also expand and bleed into wider society. Issues can arise by data points being appropriately and fairly measured, as well as with acquiring data sets with informed consent from the data subjects.</p> <p>Once the data is collected, harms may arise in terms of how the data is used. In these cases the notion of contextual privacy[@nissenbaum2009] can be very enlightening. The idea is that a data point (or a piece of information more generally) is not public or private per se, but instead it depends on the context and purpose for which it is being used. For example, one may consent to a fitness tracker collecting one's data for personal purposes, but one has not then automatically consented to the company using the data in other ways (for example, sell it to health insurance companies).  This problem is compounded by the fact that users either have no information that this is being done, and even when they do, as mentioned in (1), they often have few or no avenues to contest these practices.<sup>1</sup></p> <p>Additionally, many potential harms can occur in the way information is stored in the medium and long run where security considerations are not always at the forefront of data managers.</p> <p></p>"},{"location":"skills-tracks/aeg/chapter2/harms/#6-biospheric-harms","title":"6. Biospheric harms","text":"<p>A final kind of AI harm is not done to people directly, but to the environment we all live in. The explosion of computing power (which has partly driven the \u201cbig data revolution\u201d) has had significant environmental cost. Algorithms require data, and as they become more complex, they require increasing amounts of data and computation, which translates into increasing levels of energy consumption. For example, training Google's large language model BERT produces emissions equivalent to one transatlantic flight.[@strubell2019]</p> <p>Many of these models ingest abundant amounts of data for training. As models increase in size and complexity they need more training data, but this does not necessarily lead to an equally large increase in model accuracy. Quite the contrary. In many cases, the gains in accuracy are only modest. For example, between 2013 and 2019 the amount of compute needed to train complex algorithmic models has increased 300,000 times. This results in energy expenditures (from increases in training the model) doubling every six months. As a result, a significant amount of costly resources are used, even when the benefits of improvements in the model are small at best. Additionally, the costs of these resources are burdened upon everyone on the planet (in the form of negative externalities), while the modest gain in model performance is most likely accrued to the proprietary owner(s) of the model. </p> <p>These models contribute to emissions which are partly responsible of biospheric harm and climate change. Additionally, the benefits and risks of the use of data-intensive models is not uniformly distributed among the population or among the world\u2019s regions.  If anything, the allocations of benefit and risk have closely tracked the existing patterns of environmental racism, coloniality, and \u201cslow violence\u201d[@nixon2013] that have typified the disproportionate exposure of marginalised communities (especially those who inhabit what has conventionally been referred to as \u201cthe Global South\u201d) to the pollution and destruction of local ecosystems and to involuntary displacement.</p> <ol> <li> <p>The European Union's General Data Protection Regulation (GDPR) is probably the most advanced online privacy regulation and attempts to give consumers a lot more control over who can access their data and how it can be used.\u00a0\u21a9</p> </li> </ol>"},{"location":"skills-tracks/aeg/chapter2/values/","title":"AI Values","text":""},{"location":"skills-tracks/aeg/chapter2/values/#sum-values-respect-connect-care-and-protect","title":"SUM Values: Respect, Connect, Care, and Protect","text":"<p>In keeping with this hazards-responsive approach, the SUM Values incorporate conceptual elements from both bioethics and human rights discourse, but they do so with an eye to applying the most critical of these elements to the specific social and ethical problems raised by the potential misuse, abuse, poor design, or harmful unintended consequences of AI systems.</p> <p>The SUM Values are values that support, underwrite, and motivate a responsible innovation ecosystem. Their role is to provide an accessible framework of ethical criteria for considering, assessing, and deliberating on the ethical permissibility of a prospective AI project and its ethical impacts. They are meant to be utilised as guiding values throughout the innovation lifecycle: from the preliminary steps of project evaluation, planning, and problem formulation, through processes of design, development, and testing, to the stages of implementation and reassessment. Adopting common values from the outset enables reciprocally respectful, sincere, and open dialogue about ethical challenges by helping to create a shared vocabulary for informed dialogue and impact assessment. Such a common starting point also facilitates discussion and deliberation about how to balance values when they come into conflict.</p> <p>The SUM Values encompass a range of values that are distilled in the following four (corresponding ethical concerns are indicated in the circle):</p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"skills-tracks/aeg/chapter3/","title":"AI Sustainability and Stakeholder Engagement","text":"Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk))"},{"location":"skills-tracks/aeg/chapter3/#chapter-outline","title":"Chapter Outline","text":"<ul> <li>Stakeholder Engagement Process</li> <li>Stakeholder Impact Assessment</li> </ul> <p>Chapter Summary</p> <p>In this chapter we look at the importance of stakeholders in sustainability through what is known as the stakeholder engagement processes (SEP) as well as the stakeholder impact assessment (SIA). The former is a three-staged process with the following core steps: (i) stakeholder analysis and salience, (ii) positionality reflection, and (iii) stakeholder engagement objective and method selection. </p> <p>The latter goes beyond the SEP and is designed to ensure a project can not only remain sustainable, but also support the sustainability of impacted communities throughout time. Beyond development and initial deployment, an AI system requires continuous monitoring to ensure that the long-term transformative effects and impacts of the project are not only well-understood, but accepted and beneficial for impacted individuals and communities.</p> <p>Learning Objectives</p> <p>In this chapter, you will:</p> <ul> <li>Familiarise yourself with the concept of sustainability in the context of AI and the importance of anticipatory reflection.</li> <li>Learn about the AI project lifecycle and how it can be a useful tool for AI sustainability.</li> <li>Understand the importance of an appropriate engagement with stakeholders, as well as how to conduct a stakeholder engagement process (SEP).</li> <li>Familiarise yourself with impact assessments, in particular a stakeholder impact assessment (SIA) as well as its main components.</li> </ul>"},{"location":"skills-tracks/aeg/chapter3/engagement/","title":"Stakeholder Engagement Process","text":"The Sustainability, engagement, and reflection cycle <p>A crucial component of AI Sustainability is conducting an appropriate Stakeholder Engagement Process (SEP). But before we delve into the process itself, we must ask, what is a stakeholder?</p> <p>Key Concept: Stakeholder</p> <p>Stakeholders are individuals or groups that:</p> <p>(1) have interests or rights that may be affected by the past, present, and future decisions and activities of an organisation;</p> <p>(2) may have the power or authority to influence the outcome of such decisions and activities; </p> <p>(3) possess relevant characteristics that put them in positions of advantage or vulnerability with regard to those decisions and activities.</p> <p>The SEP is an iterative process with three core steps: </p> <p></p> <p>Each of these activities should be documented and utilised to create a Project Summary (PS) report. A Project Summary Report is comprised of four components reflecting the SEP process: (1) a preliminary project scoping and stakeholder analysis, (2) a positionality reflection (3) an overview of established stakeholder objectives and methods and (4) a governance workflow map. The fourth component of the PS Report will be discussed later in the course when we look into accountability and governance of AI systems.</p> <p>We will now analyse each of the relevant steps within the process and look at how they relate to thinking about the sustainability of the project as a whole. </p>"},{"location":"skills-tracks/aeg/chapter3/engagement/#preliminary-project-scoping-and-stakeholder-analysis","title":"Preliminary Project Scoping and Stakeholder Analysis","text":"<p>This is the first activity within the SEP process. It involves four sub-steps:</p> <ol> <li> <p>Outlining project use context, domain, and data: In this step, a high-level description of the prospective system is outlined. This includes the domain in which it will operate, the contexts in which it will be used, and the data on which it will be trained. During this initial project scoping activity, information should be drawn from organisational documents (i.e., the project business case, proof of concept, or project charter), project team collaboration, as well as desk research (if necessary) to complete the description.</p> </li> <li> <p>Identifying stakeholders: Once the first step is completed and building on this contextual understanding, this step involves identifying who may be affected by, or may affect the project.</p> </li> <li> <p>Scoping potential stakeholder impacts: This involves carrying out a preliminary evaluation of the potential impacts of the prospective AI system on affected individuals and communities. At this initial stage of reflection, members of the project team should review the SUM values, and the corresponding ethical concerns, rights and freedoms, and then consider which of these could be impacted by the design, development and deployment of the prospective AI system and how.</p> </li> <li> <p>Analysing stakeholder salience: The step requires assessing the relevance of each identified stakeholder group to the AI project and to its use contexts. This includes assessing the relative interests, rights, vulnerabilities, and advantages of identified stakeholders as these interests, rights, vulnerabilities, and advantages may be impacted by, or may impact, the AI system in question. When identifying stakeholders, the project team should also consider organisational stakeholders, whose input will likewise strengthen the openness, inclusivity, and diversity of the project.</p> </li> </ol> <p>Stakeholder analyses may be carried out in a variety of ways that involve more-or-less stakeholder involvement. This spectrum of options ranges from analyses carried out exclusively by a project team without active community engagement to analyses built around the inclusion of community-led participation and co-design from the earliest stages of stakeholder identification. The degree of stakeholder involvement will vary from project to project based upon a preliminary assessment of the potential risks and hazards of the model or tool under consideration.\u00a0</p> <p>Low-stakes AI applications that are not safety-critical, do not directly impact the lives of people, and do not process potentially sensitive social and demographic data may need less proactive stakeholder engagement than high-stakes projects. For example, an application which sorts out email and identifies spam may be correctly classified as low impact and low risk.</p> <p>A responsible and thorough initial evaluation of the scope of the possible risks that could arise from the AI system (to be carried by the team responsible for developing said system) will determine the the potential hazards the project poses to affected stakeholders (be them individuals or communities). A reasonable assessments of the dangers posed to individual wellbeing and public welfare is needed in order to formulate proportionate approaches to stakeholder involvement.\u00a0</p> <p>Regardless of the potential impacts of a project, involving affected individuals and communities in stakeholder analysis (and, later, in stakeholder impact assessment) should, in all cases, be a significant consideration. Stakeholder involvement ensures that a project will possess an appropriate degree of public accountability, transparency, legitimacy, and democratic governance, and it recognizes the important role played in this by the inclusion of the voices of all affected individuals and communities in decision-making processes.\u00a0</p> <p>In addition to providing these important supports for building public trust, stakeholder involvement can help to strengthen the objectivity, reflexivity, reasonableness, and robustness of the choices a project team makes across the project lifecycle. This is because the inclusion of a wider range of perspectives (especially of those who are most marginalised) can enlarge a project team\u2019s purview, expand its domain knowledge as well as its understanding of citizens\u2019 needs. It can likewise unearth potential biases that may arises from limiting the standpoints that inform decision-making to those of team members.\u00a0</p> <p>Public engagement and community involvement, however, are only one part of the measures a team needs to take to ensure the objectivity, reflexivity, reasonableness, and robustness of its stakeholder analysis, impact assessment, and decision-making, more generally. Apart from outward-facing community participation, processes inward-facing reflection should also inform the way the team approaches to these challenges (see the sustainability, engagement, and reflection cycle).</p>"},{"location":"skills-tracks/aeg/chapter3/engagement/#positionality-reflection","title":"Positionality Reflection","text":"<p>All individual human beings come from unique places, experiences, and life contexts that have shaped their thinking and perspectives. Reflecting on these is important insofar as it can help team members understand how their viewpoints might differ from those around them and, more importantly, from those who have diverging cultural and socioeconomic backgrounds and life experiences. </p> <p>Identifying and probing these differences can enable individuals to better understand how their own backgrounds, for better or worse, frame the way they see others, the way they approach and solve problems, and the way they carry out research and engage in innovation. By undertaking such efforts to recognise social position and differential privilege, they may gain a greater awareness of their own personal biases and unconscious assumptions. This then can enable them to better discern the origins of these biases and assumption and to confront and challenge them in turn.\u00a0</p> <p>Social scientists have long referred to this kind of self-locating reflection as \u201cpositionality\u201d. When team members take their own positionalities into account making them explicit, they can better grasp how the influence of their respective social and cultural positions creates strengths and limitations. </p> <p>On the one hand, one\u2019s positionality\u2014with respect to characteristics like ethnicity, race, age, gender, socioeconomic status, education and training levels, values, geographical background, etc.\u2014can have a positive effect on an individual\u2019s contributions to an innovation project; the uniqueness of each person\u2019s lived experience and standpoint can play a constructive role in introducing insights and understandings that other team members do not have. On the other hand, one\u2019s positionality can assume a harmful role when hidden biases and prejudices that derive from a person\u2019s background, and from power imbalances and differential privileges, creep into decision-making processes undetected.\u00a0</p> <p>When taking positionality into account, team members should reflect on their own positionality matrix. They should ask: To what extent do my personal characteristics, group identifications, socioeconomic status, educational, training, &amp; work background, team composition, &amp; institutional frame represent sources of power and advantage or sources of marginalisation and disadvantage? How does this positionality influence my (and my team\u2019s) ability to identify &amp; understand affected stakeholders and the potential impacts of my project? Several other questions must be asked to respond to these two: How do I identify? How have I been educated and trained? What does my institutional context and team composition look like? What is my socioeconomic history?\u00a0</p> <p></p>"},{"location":"skills-tracks/aeg/chapter3/engagement/#stakeholder-engagement-objective-and-method","title":"Stakeholder Engagement Objective and Method","text":"<p>Once the initial project scoping, stakeholder analysis, and positionality reflection have been done, a project team can move on to define the objective for their stakeholder engagement, as well as the method(s) to be used. </p> <p>As we have previously established, stakeholder engagement may be carried out in a variety of ways that involve more-or-less stakeholder involvement. This spectrum of options ranges from analyses carried out exclusively by a project team without active community engagement to analyses built around the inclusion of community-led participation and co-design from the earliest stages of stakeholder identification. </p> <p>The degree of stakeholder involvement will vary from project to project and take into account a preliminary assessment of the potential risks and hazards of the model or tool under consideration. Low-stakes AI applications that are not safety-critical, do not directly impact the lives of people, and do not process potentially sensitive social and demographic data may need less proactive stakeholder engagement than high-stakes projects. Similarly, the project team will need to take into account their assessment of positionality.</p> <p>When weighing these three factors, the team should prioritise the establishment of a clear and explicit stakeholder engagement objective and document this. This is crucial, because all stakeholder engagement processes can run the risk either of being cosmetic tools employed to legitimate projects without substantial and meaningful participation or of being insufficiently participative, i.e. of being one-way information flows or nudging exercises that serve as public relations instruments. The purpose of stakeholder involvement in sustainable AI projects is just the opposite: to amplify the participatory agency of affected individuals and organisations in impact assessment, risk management, and assurance processes.</p> <p>To avoid such hazards of superficiality, the team should shore up its proportionate approach to stakeholder engagement with deliberate and precise goal-setting. There are a range of engagement options that can help a project obtain a level of public participation which meets team-based assessments of impact and positionality as well as practical considerations and stakeholder needs.</p>"},{"location":"skills-tracks/aeg/chapter3/engagement/#defining-a-stakeholder-objective","title":"Defining a Stakeholder Objective","text":""},{"location":"skills-tracks/aeg/chapter3/engagement/#selecting-a-stakeholder-engagement-method","title":"Selecting a Stakeholder Engagement Method","text":"<p>The following table summarises a wide range of salient methods:</p> Mode of Engagement Description Degree of Engagement Practical Strengths Practical Weaknesses Newsletters (email) Regular emails (e.g.: fortnightly or monthly) that contain updates, relevant news, and calls to action in an inviting format. <code>INFORM</code> Can reach many people; can contain large amount of relevant information; can be made accessible and visually engaging. Might not reach certain portions of the population; can be demanding to design and produce with some periodicity; easily forwarded to spam/junk folders without project team knowing (leading to overinflated readership stats). Letters (post) Regular letters (e.g.: monthly) that contain the latest updates, relevant news and calls to action. <code>INFORM</code> Can reach parts of the population with no internet or digital access; can contain large amount of relevant information; can be made accessible and visually engaging. Might not engage certain portions of the population; Slow delivery and interaction times hampers the effective flow of information and the organisation of further engagement. App notifications Projects can rely on the design of apps that are pitched to stakeholders who are notified on their phone with relevant updates. <code>INFORM</code> Easy and cost-effective to distribute information to large numbers of people; Rapid information flows bolster the provision of relevant and timely news and updates. More significant initial investment in developing an app; will not be available to people without smartphones. Community fora Events in which panels of experts share their knowledge on issues and then stakeholders can ask questions. <code>INFORM</code> Can inform people with more relevant information by providing them with the opportunity to ask questions; brings community together in a shared space of public communication. More time-consuming and resource intensive to organise; might attract smaller numbers of people and self-selecting groups rather than representative subsets of the population; effectiveness is constrained by forum capacity. Online Surveys Survey sent via email, embedded in a website, shared via social media, etc. <code>CONSULT</code> Cost-effective; simple mass- distribution. Risk of pre-emptive evaluative framework when designing questions; Does not reach those without internet connection or computer/smartphone access. Phone Interviews Structured or semi-structured interviews held over the phone. <code>CONSULT</code> <code>PARTNER</code> Opportunity for stakeholders to voice concerns more openly. Risk of pre-emptive evaluative framework when designing questions; Might exclude portions of the populations without phone access or with habits of infrequent phone use. Door-to-door interviews Structured or semi-structured interviews held in-person at people\u2019s houses. <code>CONSULT</code> <code>PARTNER</code> Opportunity for stakeholders to voice concerns more openly; can allow participants the opportunity to form connections through empathy and face-to- face communication. Potential for limited interest to engage with interviewers; time-consuming; can be seen by interviewees as intrusive or burdensome. :fontawesome-solid-people-arrows-left-right: In-person interviews Short interviews conducted in- person in public spaces. <code>CONSULT</code> <code>PARTNER</code> Can reach many people and a representative subset of the population if stakeholders are appropriately defined and sortition is used. Less targeted; pertinent stakeholders must be identified by area; little time/interest to engage with interviewer; can be viewed by interviewees as time- consuming and burdensome. Focus Groups A group of stakeholders brought together and asked their opinions on a particular issue. Can be more or less formally structured. <code>CONSULT</code> <code>PARTNER</code> Can gather in-depth information; Can lead to new insights and directions that were not anticipated by the project team. Subject to hazards of group think or peer pressure; complex to facilitate; can be steered by dynamics of differential power among participants. Online Workshops Workshops using digital tools such as collaborative platforms. <code>CONSULT</code> Opportunity to reach stakeholders across regions, increased accessibility depending on digital access. Potential barriers to accessing tools required for participation, potential for disengagement. Crowdsourcing (Online) Well-designed tasks that can be undertaken by a distributed collective, with individuals working on separate components. <code>CONSULT</code> <code>PARTNER</code> Opportunity to engage stakeholders across regions, in an asynchronous manner, with increased accessibility depending on digital access. Supports increased potential for diverse forms of expertise and experience. Can be misused as a method for outsourcing cheap labour; potential barriers to accessing tools required for participation; potential for disengagement; difficult to ensure accuracy and validity of input from participants. Distributed Project Collaboration (Online) Online digital platforms, such as GitHub, enable new forms of citizen science and collaborative development on diverse projects (e.g., open source software, open science). <code>CONSULT</code> <code>PARTNER</code> <code>EMPOWER</code> Opportunity to engage stakeholders across regions, in an asynchronous manner, with increased accessibility depending on digital access; increased potential for diverse forms of expertise and experience; empowers new communities to actively participate in shaping and building tools that have real value for their communities. Potential barriers to accessing digital tools required for participation, including high levels of digital literacy. Citizen panel or assembly Large groups of people (dozens or even thousands) who are representative of a town/region. <code>INFORM</code> <code>CONSULT</code> <code>PARTNER</code> <code>EMPOWER</code> Provides an opportunity for co-production of outputs; can produce insights and directions that were not anticipated by the project team; can provide an information base for conducting further outreach (surveys, interviews, focus groups, etc.); can be broadly representative; can bolster a community\u2019s sense of democratic agency and solidarity. Participant rolls must be continuously updated to ensure panels or assemblies remains representative of the population throughout their lifespan; resource-intensive for establishment and maintenance; subject to hazards of group think or peer pressure; complex to facilitate; can be steered by dynamics of differential power among participants. Citizen jury A small group of people (between 12 and 24), representative of the demographics of a given area, come together to deliberate on an issue (generally one clearly framed set of questions), over the period of 2 to 7 days (Involve.org.uk. <code>INFORM</code> <code>CONSULT</code> <code>PARTNER</code> <code>EMPOWER</code> Can gather in-depth information; can produce insights and directions that were not anticipated by the project team; can bolster participants\u2019 sense of democratic agency and solidarity. Subject to hazards of group think; complex to facilitate; risk of pre-emptive evaluative framework; small sample of citizens involved risks low representativeness of wider range of public opinions and beliefs. <p>As with all forms of engagement, deciding on the best method requires awareness of your audience. Consider the following cases:</p>  Policymakers General Public Researchers <p>A research team has released results from an economics study that could have a positive impact on public policy. They decide to share these results with policymakers. The goal is to directly influence policy. Therefore, the results need to be clearly communicated and also connected to the policy goal. This connection is important to help ensure that policy-makers are able to evaluate the wider implications of the scientific findings. </p> <p>Communication Goal: to demonstrate how scientific findings can support evidence-based policy impact</p> <p>As part of an education outreach campaign to improve digital literacy among adolescents, a mental health charity are running workshops with secondary school students. They wish to communicate recent evidence about the impact of over-using social media on mental health. Rather than communicating complicated statistical information about the methods used in their study, the team develop a more accessible form of their findings and link these findings to practical steps that the students can take to protect themselves online.</p> <p>Communication Goal: to build awareness of possible risks associated with excessive social media usage and support behavioural change strategies</p> <p>A PhD student working in a Physics department has results from a recent study that developed and tested a new method for the large-scale data mining of astronomical data. The PhD student wishes to present this new method and the validation study at an upcoming international conference for data science. The audience will be technically literate, but will not have specialist expertise in astronomy. Therefore, the PhD student describes the method in the context of its original study buy also emphasises the generalisability for other sciences (e.g. genomics).</p> <p>Communication Goal: to advance academic career by gaining experience of presenting conference papers and also generating interest in a novel data science method.</p>"},{"location":"skills-tracks/aeg/chapter3/impact/","title":"Stakeholder Impact Assessment","text":"<p>Designers and users of AI systems should remain aware that these technologies may have transformative and long-term effects on individuals and society. To ensure that the deployment of an AI system remains sustainable and supports the sustainability of the communities it will affect, the project team should proceed with a continuous sensitivity to the real-world impacts that the system will have. The team should come together to evaluate the social impact and sustainability of an AI project through what is known as a Stakeholder Impact Assessment (SIA).</p> <p>The SUM values form the basis of the SIA. They are intended as a launching point for open and inclusive conversations about the individual and societal impacts of data science research and AI innovation projects rather than to provide a comprehensive inventory of moral concerns and solutions. At the very outset of any project, these should provide the normative point of departure for collaborative and anticipatory reflection, while, at the same time, allowing for the respectful and interculturally sensitive inclusion of other points of view.</p> <p>Objectives of a SIA</p> <pre><code>The purpose of carrying out a SIA is multidimensional. SIAs can serve several purposes, some of which include:\n\n- Help to build **public confidence** that the design and deployment of the AI system has been done responsibly.\n- Facilitate and strengthen your **accountability** framework.\n- **Bring to light unseen risks** that threaten to affect individuals and the public good.\n- Underwrite **well-informed decision-making** and **transparent innovation practices**.\n- Demonstrate **forethought** and due **diligence** not only within an organisation but also to the wider public.\n</code></pre> <p>AI projects may require different kinds of impact assessments. For example Data Protection Law requires data protection impact assessments (DPIAs) to be carried out in cases where the processing of personal data is likely to result in a high risk to individuals. DPIAs assess the necessity and proportionality of the processing of personal data, identify risks that may emerge in that processing, and present measures taken to mitigate those risks. Another example are equality impact assessments (EIAs) which aid in fulfiling the requirements of equality legislation.</p> <p>While both DPIAs and EIAs provide relevant insights with respect to the ethical stakes of AI innovation projects, they go only part of the way in identifying and assessing the full range of potential individual and societal impacts of the design, development, and deployment of AI systems. Reaching a comprehensive assessment of these impacts is the purpose of SIAs.</p> <p>Key Concept: Stakeholder Impact Assessment</p> <p>SIAs are tools that create a procedure for, and a means of documenting, the collaborative evaluation and reflective anticipation of the possible harms and benefits of AI innovation projects. SIAs are not intended to replace DPIAs or EIAs, which are obligatory. Rather, SIAs are meant to be integrated into the wider impact assessment regime as a way to demonstrate that sufficient attention has been paid to the ethical permissibility, transparency, accountability, and equity of AI innovation projects.</p> <p>There are three critical points in the AI project lifecycle at which the project team should convene to impact the social impact and sustainability of a project:</p>  Design Develop Deploy <p>A SIA should be carried out to determine the ethical permissibility of the project. As a starting point, the team should refer to the SUM Values  for the considerations of the possible effects of the project on individual wellbeing and public welfare. This should include a stakeholder engagement and involvement component in the initial SIA through methods established in the initial Project Summary Report (PS Report), so public views can be considered in ways that are proportional to potential project impacts and appropriate to team positionality.</p> <p>This will bolster the inclusion of a diversity of voices and opinions into the design and development process through the participation of a more representative range of stakeholders. The Design Phase SIA includes a revisitation of the Project Summary Report, where engagement objectives and methods for the Development Phase SIA were first established. These, and other relevant project revisions should be reflected in an update to the PS Report.</p> <p>Once a model has been trained, tested, and validated, the project team should revisit the initial SIA to confirm that the AI system to be implemented is still in line with the evaluations and conclusions of the original assessment. This check-in should be logged on the Development Phase section of the SIA with any applicable changes added and discussed. </p> <p>The method of stakeholder engagement that accompanies the SIA process will have been initially established in the PS report and revisited in the Design Phase SIA. This report should be revisited again during the Development Phase SIA and updated where needed. At this point the team must also set a time frame for re-assessment once the system is in operation. Time frames for these re-assessments should be decided by the team on a case-by-case basis but should be proportional to the scale of the potential impact of the system on the individuals and communities it will affect.</p> <p>Once an AI system has gone live, the team should iteratively revisit and re-evaluate the SIA. These check-ins should be logged on the Deployment Phase section of the SIA with any applicable changes added and discussed. Deployment-Phase SIAs should focus both on evaluating the existing SIA against real world impacts and on considering how to mitigate the unintended consequences that may have ensued in the wake of the deployment of the system. As with each SIA iteration, the PS report is revisited at this point, when objectives, methods, and time frames for the next Deployment Phase SIA are established.</p>"},{"location":"skills-tracks/aeg/chapter3/impact/#skills-for-conducting-sias","title":"Skills for Conducting SIA's","text":""},{"location":"skills-tracks/aeg/chapter3/impact/#weighing-the-values-and-considering-trade-offs","title":"Weighing the values and considering trade-offs","text":"<p>Taking the SUM values as a starting point of conversation and for the SIA, there will come times when these values come into conflict with one another and decisions will have to be made on which value to prioritise. Within a team, discussion should be encouraged on how to weigh the values against one another and how to consider trade-offs should use-case specific circumstances arise when the values come into tension with each other.</p> <p>For instance, there may be circumstances where the use of an AI system could optimally advance the public interest only at the cost of safeguarding the wellbeing or the autonomy of a given individual. In other cases, the use of an AI system could preserve the wellbeing of a particular individual only at the cost of the autonomy of another or of the public welfare more generally.</p> <p>This issue of adjudicating between conflicting values has long been a crucial and thorny dimension of collective life, and the problem of discovering reasonable ways to overcome the disagreements that arise as a result of the plurality of human values has occupied thinkers for just as long. Nonetheless, over the course of the development of modern democratic and plural societies, several useful approaches to managing the tension between conflicting values have emerged.</p>"},{"location":"skills-tracks/aeg/chapter3/impact/#consequences-based-and-principles-based-approaches-to-balancing-values","title":"Consequences-based and principles-based approaches to balancing values","text":"<p>Let's go back to some concepts introduced in chapter 1 that will be useful when trying to balance the tension between values: consequences-based moral thinking or consequentialism and principles-based moral thinking or deontology. These can be seen as procedural tools for thinking through a given dilemma in weighing values.</p> <p>As a quick reminder, a consequence-based approach asks that, in judging the moral correctness of an action, one prioritise considerations of the goodness produced by an outcome. In other words, because what matters most is the consequences of ones actions, the goodness of these consequences should be maximised. Standards of right and wrong (indicators of what one ought to do) are determined, on this view, by whether the action taken maximises overall goodness of the consequences rather than by the principles or standards one applies when acting.</p> <p>A principles-based approach takes the opposite tack. The rightness of an action is determined, from this standpoint, by the intentional application of a universally applicable standard, maxim, or principle. Rather than basing the morality of conduct on the ends served by it, this approach anchors rightness in the duty or obligation of the individual agent to follow a rationally determined (and therefore \u201cuniversalisable\u201d) principle. </p> <p>For deontological or principles-based ethics, the integrity of the principled action and intention matters most, and so justified constraints, which are rooted in the priority to act according to moral standards, must be put on the pursuit of the achievement of one\u2019s goals.</p> <p>Knowing when to prioritise consequences and when to prioritise principles in moral deliberations is a tricky matter, and applying either a consequentialist or deontological approach (or both) may make sense depending upon the context. </p> <p>To take a familiar example, in deontologically following the principle \u2018Thou shall not lie,\u2019 you would be justifiably constrained from stealthily deceiving and misleading others to get ahead in your job. The principle matters here. But, in a different situation, say, where lying to a murderer, who appears at your front door, would save an innocent victim whom you are concealing in your cellar, the prioritisation of consequences makes more sense.</p> <p>Consider another example: In an overburdened council, the introduction of an automated system for making sites available for development would vastly expedite housing delivery. The implementation of this AI system would thus produce a consequence that could be beneficial to the public.</p> <p>Yet, it may, among other things, simultaneously do damage to the value of Connect (which safeguards interpersonal dialogue, meaningful human connection and social cohesion) by eliminating time intensive consultation processes that facilitate interpersonal communication, trust building, and social bonding between council staff and residents. How then could one go about weighing the value of improving public welfare against the value of respecting the integrity of interpersonal relations?</p> <p>One way would be to place each side of this comparison under the rubric of either consequences or principles and then measure them up against each other accordingly. From one perspective, the publicly beneficial consequences of improving service delivery might outweigh the publicly harmful consequences of impairing social cohesion. On a different view, such a trade-off would be unacceptable, because the principle of respecting the integrity of social cohesion trumps any solidarity-harming but publicly beneficial consequences whatsoever. The answers to these questions will always be tricky, but deliberation between team members and stakeholders should always be a part of arriving to any sort of consensus (which we will focus on in the next section).</p> <p>Getting clear on the consequences and the principles involved in a specific case of conflicting values will allow team members to get a better picture of the practical and moral stakes at play in a particular project. It will also help the team get a sharper idea of the proportionality of using an AI technology to achieve a desired outcome given both its potential ethical impacts and the social needs to which it is responding. When drawn upon for guidance, consequentialism and deontology can provide a kind of procedural scale upon which to place, measure, and weigh conflicting values. They are practical tools that can be used within meaningful deliberation.</p>"},{"location":"skills-tracks/aeg/chapter3/impact/#ensuring-meaningful-and-inclusive-deliberation","title":"Ensuring meaningful and inclusive deliberation","text":"<p>The most general approach is to encourage mutually respectful, sincere, and well-informed dialogue, so that reasons from all affected voices can be heard and considered. Deliberations that have been inclusive, open, and impartial tend to generate better and more inferentially sound conclusions, so approaching the adjudication of conflicting values in this manner will likely improve mutual understanding of the rationales and perspectives, which inform those values.</p> <p>Whether or not this ends up being the case in each conversational context, the importance of cultivating a culture of innovation, which encourages reciprocally respectful, open, non-coercive, and mutually accountable communication must be stressed. The success of the modern sciences (which have been built on the dynamic foundations of inclusive, rational, and democratic communication) is perhaps evidence enough to support the validity of this emphasis.</p> <p></p> <p>Here, procedural ethics is crucial. The importance of meaningful dialogue in balancing values is rooted in central role played in it by the rational exchange and assessment of ideas and beliefs. The validity of the claims we make in conversations about values is bound by practices of giving and asking for reasons. A claim about values that is justified is one that convinces by the unforced strength of the better or more compelling argument. Rational justification and persuasive reason-giving are, in fact, central elements of legitimate and consensus-oriented moral decision making. And, along the same lines, claims made about moral values or properties are subject to critical evaluation vis-\u00e0-vis their inferential strengths and weaknesses.</p> <p>Another way to understand the importance of meaningful dialogue in balancing values has to do with the way that such a procedural view of ethical thinking enables open communication about prioritising values without imposing substantive views about the values themselves. Instead, an emphasis on rational communication in deliberations looks to secure a justified and equitable process of exchanging and evaluating reasons. It starts with the question: what are the enabling conditions in the interpersonal communication of values and beliefs that allow interlocutors to come to defensible and rationally acceptable moral judgments and reason-based consensus?</p> <p>To answer this question, moral thinkers over the past century have endeavoured to reconstruct the practical assumptions behind and presuppositions of rational communication (a summary of the most essential of such assumptions and presuppositions is provided below). Creating a reflective and practicable awareness of these assumptions and presuppositions among members of a team can play a crucial role in creating an innovation environment that is optimally conducive meaningful and inclusive deliberation.</p> <p>Preconditions of meaningful deliberation:</p> <p>Impartiality</p> <p>Interlocutors engaging in meaningful deliberation must consider the interests of all those who are affected by their actions equally. Thinking impartially involves taking on the view of others to try to put oneself in their place.</p> <p>Non-coercion</p> <p>Meaningful deliberation must be free from any sort of implicit or explicit coercion, force, or restriction that would prevent the open and unconstrained exchange of reasons.</p> <p>Sincerity</p> <p>Meaningful deliberation must be free from any sort of deception or duplicity that would prevent the authentic exchange of reasons. Interlocutors must mean what they say.</p> <p>Consistency and coherence</p> <p>Arguments and positions offered in meaningful deliberation must be clear, free from contradictions, and hold together collectively in an understandable way.</p> <p>Mutual respect and egalitarian reciprocity</p> <p>All interlocutors must be treated with respect and given equal opportunity to contribute to the conversation. All voices are worthy of equal consideration in processes of exchanging reasons.</p> <p>Inclusivity and publicity</p> <p>Anyone whose interests are affected by an issue and who could make a contribution to better understanding it must not be excluded from participating in deliberation. All relevant voices must be heard and all relevant information considered.</p>"},{"location":"skills-tracks/aeg/chapter3/impact/#addressing-and-mitigating-power-dynamics-that-may-obstruct-meaningful-and-inclusive-deliberation","title":"Addressing and mitigating power dynamics that may obstruct meaningful and inclusive deliberation","text":"<p>The stewardship of meaningful and inclusive dialogue is critical to safeguarding the collective weighing up of values. However, there is an important potential barrier to meaningful deliberation that challenges its feasibility and must be addressed. As guiding assumptions of rational communication, norms like sincerity, impartiality, non-coercion, and inclusiveness may strike some as overly idealistic. In the real world, discussions are rarely fully inclusive, informed, and free of assertive manipulation, coercion, and deception. Rather, deliberation and dialogue are often steered by and crafted to protect the interests of the dominant. Likewise, differential power relationships (for instance, divergent educational backgrounds that derive from differential socioeconomic privileges) create power imbalances that fundamentally challenge the conditions of reciprocity and equal footing that are needed for justified and equitable communication.</p> <p>A team should confront these obstacles to meaningful deliberation head-on through a power-aware approach to facilitating collaborative reflection, dialogue, and engagement. An awareness of and sensitivity to the differential relationships of power that can suppress the full participation of disadvantaged or marginalised voices can better encourage an inclusive, open, and equal opportunity conversation between participants. </p> <p>Clear-headed explorations of power dynamics between civil servants, scientists, citizens, domain experts, and policymakers can assist in avoiding the kind of deficiencies of representation and empowerment that risk reinforcing existing power structures and inequalities. This may involve active mitigation measures like the provision of training, upskilling, and technical resources to those who have lacked access to them. Above all, the norms of meaningful deliberation makes teams aware of the possible distortions of communication (i.e. a lack of egalitarian reciprocity, non-coercion, sincerity, etc.) that must be tackled and rectified for the hurdles of power disparities to be scaled.</p>"},{"location":"skills-tracks/aeg/chapter4/","title":"Fairness &amp; Bias Mitigation, Transparency, Explainability, and Governance","text":"Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk))"},{"location":"skills-tracks/aeg/chapter4/#chapter-outline","title":"Chapter Outline","text":"<ul> <li>Introduction to Fairness</li> <li>AI Fairness</li> <li>Bias Mitigation</li> <li>Accountability</li> <li>Governance</li> </ul> <p>Chapter Summary</p> <p>In this chapter we explore various issues that arise within AI systems. We will look at the elements of AI fairness, and the strategies to conduct bias mitigation. Finally, we will touch upon the concepts of accountability and governance, and how they can be integrated into the AI project lifecycle.</p> <p>Learning Objectives</p> <p>In this chapter, you will:</p> <ul> <li>Learn about the landscape of meanings the concept of fairness has, as well as how to apply it in the context of AI.</li> <li>Understand the different elements of AI fairness and how they relate to one another.</li> <li>Look at the concept of bias in AI systems, the different kinds of biases which may arise, as well as how to mitigate them.</li> <li>Familiarise yourself with AI accountability, its main components, and how it relates to AI governance.</li> </ul>"},{"location":"skills-tracks/aeg/chapter4/accountability/","title":"Accountability in AI","text":""},{"location":"skills-tracks/aeg/chapter4/accountability/#introduction-to-accountability","title":"Introduction to accountability","text":"<p>What does accountability mean? It means that humans are answerable for the parts they play across the entire AI design, development, and deployment. It also demands that the results of this work are traceable from start to finish.</p> <p>According to the principle of fairness, designers and implementers are held accountable for being equitable and for not harming anyone through bias or discrimination. According to the principle of sustainability, designers and implementers are held accountable for producing AI innovation that is safe and ethical in its outcomes and wider impacts. And according to the principle of explainability, designers and implementers are held accountable for making sure that any decision the AI system makes, or provides support for humans to make, can be adequately explained to relevant stakeholders. Therefore, the principle of accountability is an end-to-end governing principle.</p> <p>Responsible AI project delivery requires confronting two relevant challenges to accountability.</p> <p>Accountability gap</p> <p>Automated decisions are not self-justifiable. Whereas human agents can be called to account for their judgments and decisions in instances where those judgments and decisions affect the interests of others, the statistical models and underlying hardware that compose AI systems are not responsible in the same morally relevant sense. This creates an accountability gap that must be addressed so that clear and imputable sources of human answerability can be attached to decisions assisted or produced by an AI system.</p> <p>Complexity of AI production processes</p> <p>Establishing human answerability is not a simple matter when it comes to the design, development, and deployment of AI systems. This is due to the complexity and multi-agent character of the production and use of these systems. Typically, AI project delivery workflows include department and delivery leads, technical experts, data procurement and preparation personnel, policy and domain experts, implementers, and others. Due to this production complexity, it may become difficult to answer the question of who among these parties involved in the production of AI systems should bear responsibility if these systems\u2019 uses have negative consequences and impacts.</p> <p>Meeting the special requirements of accountability, which are born out of these two challenges, calls for a sufficiently fine-grained concept of what would make an AI project properly accountable. This concept can be broken down into two sub-components of accountability, answerability and auditability:</p> <p>Answerability</p> <p>The principle of accountability demands that the onus of justifying algorithmically supported decisions be placed on the shoulders of the human creators and users of those AI systems. This means that it is essential to establish a continuous chain of human responsibility across the whole AI project delivery workflow. Making sure that accountability is effective from end to end necessitates that no gaps be permitted in the answerability of responsible human authorities from first steps of the design of an AI system to its algorithmically steered outcomes.</p> <p>Answerability also demands that explanations and justifications of both the rationale underlying the results of an AI system and the processes behind their production and use be offered by competent human authorities in plain, understandable, and coherent language. These explanations and justifications should be based upon sincere, consistent, sound, and impartial reasons that are accessible to non-technical hearers.</p> <p>Auditability</p> <p>Whereas the notion of answerability responds to the question of who is accountable for an automation supported outcome, the notion of auditability answers the question of how the designers and implementers of AI systems are to be held accountable. This aspect of accountability has to do with demonstrating both the responsibility of design, development, and deployment practices and the justifiability of outcomes.</p> <p>Auditability also has to do with traceability; It refers to the process by which all stages of the AI innovation lifecycle from data collection and model selection to system deployment, updating, and deprovisioning are documented in a way that is accessible and easily understood.</p> <p>The project team must ensure that every step of the process of designing and implementing an AI project is accessible for audit, oversight, and review by appropriate parties. Successful auditability requires builders and implementers of algorithmic systems to:</p> <ul> <li> <p>keep records and to make available information that enables monitoring of the soundness and diligence of the innovation processes that produced these systems</p> </li> <li> <p>keep track of the accountable parties within an organisation\u2019s project team and others involved in the supply chain (where system components are procured)</p> </li> <li> <p>keep track of the governance actions taken across the entire AI innovation workflow</p> </li> <li> <p>keep records and make accessible information that enables monitoring of data provenance and analysis from the stages of collection, pre-processing, and modelling to training, testing, and deploying. This is the purpose of the Dataset Factsheet.</p> </li> </ul> <p>Moreover, auditability requires the team to enable peers and overseers to probe and to critically review the dynamic operation of the system in order to ensure that the procedures and operations which are producing the model\u2019s behaviour are safe, ethical, and fair. Practically transparent algorithmic models must be built for auditability, reproducible, and equipped for end-to-end recording and monitoring of their data processing.The deliberate incorporation of both of these elements of accountability (answerability and auditability) into the AI project lifecycle may be called accountability-by-design.</p> <p>Key Concept: Accountability-by-design</p> <p>AI systems must be designed to facilitate end-to-end answerability and auditability. This requires both responsible humans-in-the-loop across the entire design and implementation chain as well as activity monitoring protocols that enable end-to-end oversight and review.</p>"},{"location":"skills-tracks/aeg/chapter4/accountability/#types-of-accountability","title":"Types of accountability","text":"<p>Accountability deserves consideration across the entire design and implementation workflow. As a best practice, the team should actively consider the different demands that accountability-by-design requires before and after the roll out of the AI project. We will refer to the process of ensuring accountability during the design and development stages of the AI project as \u2018anticipatory accountability\u2019. This is because the team is anticipating the project\u2019s accountability needs prior to it being completed.</p> <p>Following a similar logic, we will refer to the process of addressing accountability after the start of the deployment of your AI project as \u2018remedial accountability\u2019. This is because after the initial implementation of a system, the team is remedying any of the issues that may be raised by its effects and potential externalities. These two subtypes of accountability are sometimes referred to as ex-ante (or before-the-event) accountability and ex-post (after-the-event) accountability respectively.</p>"},{"location":"skills-tracks/aeg/chapter4/accountability/#anticipatory-accountability","title":"Anticipatory accountability","text":"<p>Treating accountability as an anticipatory principle entails that the project team takes as of primary importance the decisions made and actions taken by them prior to the outcome of an algorithmically supported decision process. This kind of ex ante accountability should be prioritised over remedial accountability, which focuses instead on the corrective or justificatory measures that can be taken after that automation supported process had been completed.</p> <p>Ensuring that the AI project delivery processes are accountable prior to the actual application of the system in the world will bolster the soundness of design and implementation processes and thereby more effectively preempt possible harms to individual wellbeing and public welfare or other adverse impacts.</p> <p>Likewise, by establishing strong regimes of anticipatory accountability and by making the design and delivery process as open and publicly accessible as possible, the project team will put affected stakeholders in a position to make better informed and more knowledgeable decisions about their involvement with these systems in advance of potentially harmful impacts. Doing so will also strengthen the public narrative and help to safeguard the project from reputational harm.</p> <p>Example</p> <p>During the preprocessing phase of an AI production lifecycle, technical members of a project team are deciding which features to include, and which to leave out. To safeguard sufficient anticipatory accountability, they make sure to log which team members are involved in making these decisions and record the rationale behind the choices made.</p>"},{"location":"skills-tracks/aeg/chapter4/accountability/#remedial-accountability","title":"Remedial Accountability","text":"<p>While remedial accountability should be seen, along these lines, as a necessary fallback rather than as a first resort for imputing responsibility in the design, development and deployment of AI systems, strong regimes of remedial accountability are no less important in providing necessary justifications for the bearing these systems have on the lives of affected stakeholders.</p> <p>Putting in place comprehensive auditability regimes as part of your accountability framework and establishing transparent design and use practices, which are methodically logged throughout the AI project delivery lifecycle, are essential components for this sort of remedial accountability.</p> <p>One aspect of remedial accountability that you must pay close attention to is the need to provide explanations to affected stakeholders for algorithmically supported decisions.</p> <p>Offering explanations for the results of algorithmically supported decision-making involves furnishing decision subjects and other interested parties with an understandable account of the rationale behind the specific outcome of interest. It also involves furnishing the decision subject and other interested parties with an explanation of the ethical permissibility, the fairness, and the safety of the use of the AI system.</p> <p>Example</p> <p>After receiving an unfavourable decision in their recruitment process, a job applicant seeks assurance and justification that the resume filtering AI system used in their process was not biased or discriminatory. To safeguard sufficient remedial accountability, the implementers of the system draw on the logged records of bias-mitigation measures and fairness-aware design practices to demonstrate the fair and non-discriminatory practices behind the production of the system. They also provide an explanation of the rationale behind the applicant\u2019s negative result, showing that the determinative factors behind the system\u2019s output were fair and reasonable.</p>"},{"location":"skills-tracks/aeg/chapter4/aifairness/","title":"AI Fairness as a contextual and multi-valent concept","text":"<p>AI Fairness must be treated as a contextual and multi-valent concept that manifests in a variety of ways and in a variety of social, technical, and sociotechnical environments. It must, accordingly, be understood in and differentiated by the specific settings in which it arises. This means that our operating notion of AI fairness should distinguish between the kinds of fairness concerns that surface in</p> <ol> <li>the context of the social world that precedes and informs approaches to fairness in AI/ML innovation activities\u2014i.e., in general normative and ethical notions of fairness, equity, and social justice, in human rights laws related to equality, non-discrimination, and inclusion, and in anti-discrimination laws and equality statutes;</li> <li>the data context\u2014i.e. in criteria of fairness and equity that are applied to responsibly collected and maintained datasets;</li> <li>the design, development, and deployment context\u2014i.e. in criteria of fairness and equity that are applied (a) in setting the research agendas and policy objectives that guide decisions made about where, when, and how to use AI systems and (b) in actual model design and development and system implementation environments as well as in the technical instrumentalisation of formal fairness metrics that allocate error rates and the distribution of outcomes through the retooling of model architectures and parameters; and</li> <li>the ecosystem context\u2014i.e. in criteria of fairness and equity that are applied to the wider economic, legal, cultural, and political structures or institutions in which the AI project lifecycle is embedded\u2014and to the policies, norms, and procedures through which these structures and institution influence actions and decisions throughout the AI innovation ecosystem.</li> </ol> <p>Each of these contexts will generate different sets of fairness concerns. In applying the principle of discriminatory non-harm to the AI project lifecycle we will accordingly break down the principle of fairness into six subcategories that correspond to their relevant practical contexts:</p> <p>Data Fairness: The AI system is trained and tested on properly representative, fit-for-purpose, relevant, accurately measured, and generalisable datasets.</p> <p>Application Fairness: The policy objectives and agenda-setting priorities that steer the design, development, and deployment of an AI system and the decisions made about where, when, and how to use it do not create or exacerbate inequity, structural discrimination, or systemic injustices and are acceptable to and line up with the aims, expectations, and sense of justice possessed by impacted people.</p> <p>Model Design and Development Fairness: The AI system has a model architecture that does not include target variables, features, processes, or analytical structures (correlations, interactions, and inferences) which are discriminatory, unreasonable, morally objectionable, or unjustifiable or that encode social and historical patterns of discrimination.</p> <p>Metric-Based Fairness: Lawful, clearly defined, and justifiable formal metrics of fairness have been operationalised in the AI system have been made transparently accessible to relevant stakeholders and impacted people. </p> <p>System Implementation Fairness: The AI system is deployed by users sufficiently trained to implement it with an appropriate understanding of its limitations and strengths and in a bias-aware manner that gives due regard to the unique circumstances of affected individuals. </p> <p>Ecosystem Fairness: The wider economic, legal, cultural, and political structures or institutions in which the AI project lifecycle is embedded\u2014and the policies, norms, and procedures through which these structures and institution influence actions and decisions throughout the AI innovation ecosystem \u2014do not steer AI research and innovation agendas in ways that entrench or amplify asymmetrical and discriminatory power dynamics or that generate inequitable outcomes for protected, marginalised, vulnerable, or disadvantaged social groups.</p>"},{"location":"skills-tracks/aeg/chapter4/aifairness/#data-fairness","title":"Data fairness","text":"<p>Responsible data acquisition, handling, and management is a necessary component of algorithmic fairness. If the results of an AI project are generated by a model that has been trained and tested on biased, compromised, or skewed datasets, affected stakeholders will not be adequately protected from discriminatory harm. Data fairness therefore involves the following key elements:</p> <ul> <li> <p>Representativeness      Depending on the context, either under representation or over representation of demographic, marginalised or legally protected groups in the data sample may lead to the systematic disadvantaging of vulnerable or marginalised stakeholders in the outcomes of the trained model. To avoid such kinds of sampling and selection biases, domain expertise is crucial, for it enables the assessment of the fit between the data collected or procured and the underlying population to be modelled. </p> </li> <li> <p>Fit-for-Purpose and Sufficiency      An important question to consider in the data collection and procurement process is: Will the amount of data collected be sufficient for the intended purpose of the project? The quantity of data collected or procured has a significant impact on the accuracy and reasonableness of the outputs of a trained model. A data sample not large enough to represent with sufficient richness the significant or qualifying attributes of the members of a population to be classified may lead to unfair outcomes. Insufficient datasets may not equitably reflect the qualities that should rationally be weighed in producing a justified outcome that is consistent with the desired purpose of the AI system. Members of the project team with technical and policy competences should collaborate to determine if the data quantity is, in this respect, sufficient and fit-for-purpose.</p> </li> <li> <p>Source Integrity and Measurement Accuracy      Effective bias mitigation begins at the very commencement of data extraction and collection processes. Both the sources and instruments of measurement may introduce discriminatory factors into a dataset. When incorporated as inputs in the training data, biased prior human decisions and judgments\u2014such as prejudiced scoring, ranking, interview-data or evaluation\u2014will become the \u2018ground truth\u2019 of the model and replicate the bias in the outputs of the system. In order to secure discriminatory non-harm, you must do your best to make sure your data sample has optimal source integrity. This involves securing or confirming that the data gathering processes involved suitable, reliable, and impartial sources of measurement and sound methods of collection.</p> </li> <li> <p>Timeliness and Recency      If your datasets include outdated data then changes in the underlying data distribution may adversely affect the generalisability of your trained model. Provided these distributional drifts reflect changing social relationship or group dynamics, this loss of accuracy with regard to the actual characteristics of the underlying population may introduce bias into your AI system. In preventing discriminatory outcomes, you should scrutinise the timeliness and recency of all elements of the data that constitute your datasets.   </p> </li> <li> <p>Relevance, Appropriateness and Domain Knowledge      The understanding and utilisation of the most appropriate sources and types of data are crucial for building a robust and bias-mitigating AI system. Solid domain knowledge of the underlying population distribution and of the predictive or classificatory goal of the project is instrumental for choosing optimally relevant measurement inputs that contribute to the reasonable determination of the defined solution. You should make sure that domain experts collaborate closely with your technical team to assist in the determination of the optimally appropriate categories and sources of measurement.</p> </li> </ul> <p>To ensure the uptake of best practices for responsible data acquisition, handling, and management across your AI project delivery workflow, you should initiate the creation of a Dataset Factsheet at the alpha stage of your project. This factsheet should be maintained diligently throughout the design and implementation lifecycle in order to secure optimal data quality, deliberate bias-mitigation aware practices, and optimal auditability. It should include a comprehensive record of data provenance, procurement, pre-processing, lineage, storage, and security as well as qualitative input from team members about determinations made with regard to data representativeness, data sufficiency, source integrity, data timeliness, data relevance, training/ testing/validating splits, and unforeseen data issues encountered across the workflow.</p>"},{"location":"skills-tracks/aeg/chapter4/aifairness/#application-fairness","title":"Application Fairness","text":"<p>Fairness considerations should enter into your AI project at the earliest possible stage of horizon scanning, problem selection, and project planning. This is because, the overall fairness and equity of an AI application is significantly determined by the objectives, goals, and policy choices that lie behind initial decisions to dedicate time and resources to its design, development, and deployment. For example, the choice made to build a biometric identification system, which uses live facial recognition technology to identify criminal suspects at public events, may be motivated by the objective to increase public safety and security. However, many members of the public may find this use of AI technology unreasonable, disproportionate, and potentially discriminatory. In particular, members of communities historically targeted by disproportionate levels of surveillance in law enforcement contexts may be especially concerned about the potential for abuse and harm. Appropriate fairness and equity considerations should, in this case, occur at the horizon scanning and project planning stage (e.g., as part of a stakeholder impact assessment process that includes engagement with potentially affected individuals and communities). Aligning the policy goals of a project team with the reasonable expectations and potential equity concerns of those affected is a key component of the fairness of the application.  </p> <p>Application fairness, therefore, entails that the policy objectives of an AI project are non-discriminatory and are acceptable to and line up with the aims, expectations, and sense of justice possessed by those affected.[@dobbe2018]-[@eckhouse2018]-[@green2018a]-[@green2018b]-[@mitchell2021]-[@passi2019] </p> <p>As such, whether the decision to engage in the production and use of an AI technology can be described as \u201cfairness-aware\u201d depends upon ethical and policy considerations that are external and prior to considerations of the technical feasibility of building an accurate and optimally performing system or the practical feasibility of accessing, collecting, or acquiring enough and the right kind of data. </p> <p>Beyond this priority of assuring the equity and ethical permissibility of policy goals, application fairness requires additional considerations in framing decisions made at the horizon scanning and project scoping or planning stage: </p> <p>1</p> <pre><code>Equity considerations surrounding real-world context of   the policy issue to be solved: When your project team is assessing the fairness of using an AI solution to address a particular policy issue, it is important to consider how equity considerations extend beyond the statistical and sociotechnical contexts of designing, developing, and deploying the system. Applied concepts of AI fairness and equity should not be treated, in a technology-centred way, as originating exclusively from the design and use of any particular AI system. Nor should they exclusively be treated as abstractions that can be engineered into an AI application through technical or mathematical retooling (e.g., by operationalising formal fairness criteria).[@fazelpour2020]-[@green2020]-[@leslie2020]-[@selbst2019] When designers of algorithmic systems limit their understanding of the scope of \u2018fairness\u2019 or \u2018equity\u2019 to these two dimensions, it can artificially constrain their perspectives in such a way that they erroneously treat only the patterns of bias and discrimination that arise in AI innovation practices or that can be measured, formalised, or statistically digested as actionable indicators of inequity.\n\nRather, equity considerations should be grounded in a human-centred approach, which includes reflection on and critical examination of the wider social and economic patterns of disadvantage, injustice, and discrimination that arise in the real-world contexts surrounding the policy issue in question. Such considerations should include an exploration of how such patterns of inequity may lead, on the one hand, to the disparate distribution of the risks and adverse impacts of the AI system or, on the other, to a lack of equitable access to its potential benefits. For instance, while the development of an AI chatbot to replace a human serviced medical helpline may provide effective healthcare guidance for some, it could have disparate adverse impacts on others, such as vulnerable elderly populations or socioeconomically deprived groups who may face barriers to accessing and using the app. Here, reflection on the real-world contexts surrounding the provision of this type of healthcare support yields a more informed and compassionate awareness of social and economic conditions that could impair the fairness of the application.\n</code></pre> <p>2</p> <pre><code>Equity considerations surrounding the group targeted by the AI innovation intervention: Each AI application that makes predictions about or classifies people, targets a specific subset of the wider population to which they belong. For instance, a r\u00e9sum\u00e9 filtering system that is used to select desirable candidates in a recruitment process will draw from a pool of job applicants that constitute a subgroup within the broader population. Potential equity issues may arise here because the selection of subpopulations sampled by AI applications is non-random. Instead, the sample selection may reflect particular social patterns, structures, and path dependencies that are unfair or discriminatory.[@mitchell2021] In the case of the r\u00e9sum\u00e9 filtering system, the sample may reflect long-term hiring patterns where a disproportionate number of male job candidates from elite universities (or those with similarly privileged educational backgrounds) have been actively recruited. Such practices have historically excluded people from other gender identities and socioeconomic and educational backgrounds. As a result, the pattern of inequity surfaces, in this instance, not within the sampled subset of the population but rather in the way that discriminatory social structures have led to the selection of a certain group of individuals into that subset.[@mehrabi2021]-[@olteanu2019]\n&lt;!-- (Mitchell et al., 2021). --&gt;\n&lt;!-- (Mehrabi et al., 2021; Olteanu et al., 2019) --&gt;\n</code></pre> <p>3</p> <pre><code>Equity considerations surrounding the way that the model\u2019s output shapes the range of possible decision-outcomes: AI applications that assist human decision-making shape and delimit the range of possible outcomes for the problem areas they address.[@mitchell2021] For example, a predictive risk model used in children\u2019s social care may generate an output that directly influences the space of choices available to a social worker. Because the model\u2019s target is the identification of at-risk children, it may lead to social care decisions that focus narrowly on whether a child needs to be taken into care. This centring of negative outcomes could restrict the range of viable choices open to the social worker insofar as it de-emphasises the potential availability of other strengths-based approaches (e.g., stewarding positive family functioning through social supports and identifying and promoting protective factors). These alternative decision-making paths could be closed off in the social care environment given how the predictive risk model\u2019s outputs restrictively shape the range of actions that can be taken to address the problem it is being used to inform.\n</code></pre>"},{"location":"skills-tracks/aeg/chapter4/aifairness/#model-design-and-development-fairness","title":"Model design and development fairness","text":"<p>Because human beings have a hand in all stages of the construction of AI systems, fairness-aware design must take precautions across the AI project workflow to prevent bias from having a discriminatory influence:</p>"},{"location":"skills-tracks/aeg/chapter4/aifairness/#design-phase","title":"Design phase","text":"<p>Problem formulation At the initial stage of problem formulation and outcome definition, technical and non-technical members of your team should work together to translate project goals into measurable targets.[@fazelpour2021]-[@leslie2019]-[@obermeyer2019]-[@obermeyer2021]-[@passi2019] This will involve the use of both domain knowledge and technical understanding to define what is being optimised in a formalisable way and to translate the project\u2019s objective into a target variable or measurable proxy, which operates as a statistically actionable rendering of the defined outcome.    </p> <p>At each of these points, choices must be made about the design of the algorithmic system that may introduce structural biases which ultimately lead to discriminatory harm. Special care must be taken here to identify affected stakeholders and to consider how vulnerable groups might be negatively impacted by the specification of outcome variables and proxies. Attention must also be paid to the question of whether these specifications are reasonable and justifiable given the general purpose of the project and the potential impacts that the outcomes of the system\u2019s use will have on the individuals and communities involved. </p> <p>These challenges of fairness aware design at the problem formulation stage show the need for making diversity and inclusive participation a priority from the start of the AI project lifecycle. This involves both the collaboration of the entire team and the attainment of stakeholder input about the acceptability of the project plan. This also entails collaborative deliberation across the project team and beyond about the ethical impacts of the design choices made. </p>"},{"location":"skills-tracks/aeg/chapter4/aifairness/#development-phase","title":"Development phase","text":"<p>Data pre-processing Human judgement enters into the process of algorithmic system construction at the stage of labelling, annotating, and organising the training data to be utilised in building the model. Choices made about how to classify and structure raw inputs must be taken in a fairness-aware manner with due consideration given to the sensitive social contexts that may introduce bias into such acts of classification. Similar fairness aware processes should be put in place to review automated or outsourced classifications. Likewise, efforts should be made to attach solid contextual information and ample metadata to the datasets, so that downstream analyses of data processing have access to properties of concern in bias mitigation.</p> <p>The constructive task of selecting the attributes or features that will serve as input variables for a model requires human decisions to be made about the sorts of information that may or may not be relevant or rationally required to yield an accurate and unbiased classification or prediction. Moreover, the feature engineering tasks of aggregating, extracting, or decomposing attributes from datasets may introduce human appraisals that have biasing effects.  </p> <p>At this stage, human decisions about how to group or disaggregate input features (e.g., how to carve up categories of gender or ethnic groups) or about which input features to exclude altogether (e.g., leaving out deprivation indicators in a predictive model for clinical diagnostics) can have significant downstream influences on the fairness and equity of an AI system. This applies even when algorithmic techniques are employed to extract and engineer features, or support the selection of features (e.g., to optimise predictive power). </p> <p>For this reason, discrimination awareness should play a large role at this stage of the AI model-building workflow as should domain knowledge and policy expertise. Your team should proceed in the model development stage aware that choices made about grouping or separating and including or excluding features as well as more general judgements about the comprehensiveness or coarseness of the total set of features may have significant consequences for historically marginalised, vulnerable, or protected groups.  </p> <p>Model selection The model selection stage determines the model type and structure that will be produced in the next stages. In some projects, this will involve the selection of multiple prospective models for the purpose of comparison based on some performance metric, such as accuracy or sensitivity. The set of relevant models is likely to have been highly constrained by many of the issues dealt with in previous stages (e.g., available resources and skills, problem formulation), for instance, where the problem demands a supervised learning algorithm instead of an unsupervised learning algorithm.  </p> <p>Fairness and equity issues can surface in model selection processes in at least two ways: First, they can arise when the choice between algorithms has implications for explainability. For instance, it may be the case that there are better performing models in the pool of available options but which are less interpretable than others. This difference becomes significant when the processing of social or demographic data increases the risk that biases or discriminatory proxies lurk in the algorithmic architecture. Model interpretability can increase the likelihood of detecting and redressing such discriminatory elements.  </p> <p>Second, fairness and equity issues can arise when the choice between algorithms has implications for the differential performance of the final model for subgroups of the population. For instance, where several different learning algorithms are simultaneously trained and tested, one of the resulting models could have the highest overall level of accuracy while, at the same time, being less accurate than others in the way it performs for one or more marginalised sub-groups. In cases like this, technical members of your project team should proceed with attentiveness to mitigating any possible discriminatory effects of choosing one model over another and should consult members of the wider team\u2014and impacted stakeholders, where possible\u2014about the acceptability of any trade-offs between overall model accuracy and differential performance.  </p> <p>Model training, testing, and validation</p> <p>The process of tuning hyperparameters, setting metrics, and resampling data at the training, validation, and testing stages also involves human choices that may have fairness and equity consequences in the trained model. For instance, the way your project team determines the training-testing split of the dataset can have a considerable impact on the need for external validation to ensure that the model\u2019s performance \u201cin the wild\u201d meets reasonable expectations. Therefore, your technical team should proceed with an attentiveness to bias risks, and continual iterations of peer review and project team consultation should be encouraged to ensure that choices made in adjusting the dials, parameters, and metrics of the model are in line with bias mitigation and discriminatory non-harm goals. </p> <p>Evaluating and validating model structures</p> <p>Design fairness also demands close assessment of the existence in the trained model of lurking or hidden proxies for discriminatory features that may act as significant factors in its output. Including such hidden proxies in the structure of the model may lead to implicit \u2018redlining\u2019 (the unfair treatment of a sensitive group on the basis of an unprotected attribute or interaction of attributes that \u2018stands in\u2019 for a protected or sensitive one).  </p> <p>Designers must additionally scrutinise the moral justifiability of the significant correlations and inferences that are determined by the model\u2019s learning mechanisms themselves, for these correlations and inferences could encode social and historical patterns of discrimination where these are baked into the dataset. In cases of the processing of social or demographic data related to human features, where the complexity and high dimensionality of machine learning models preclude the confirmation of the discriminatory non-harm of these inferences (for reason of their opaqueness by human assessors), these models should be avoided.[@icoturing2020]-[@mitchell2021]-[@rudin2019] In cases where this is not possible, a different, more transparent and explainable model or portfolio of models should be chosen. </p> <p>Model structures must also be confirmed to be procedurally fair, in a strict technical sense. This means that any rule or procedure employed in the processing of data by an algorithmic system should be consistently and uniformly applied to every decision subject whose information is being processed by that system. AI project teams should be able to certify that any relevant rule or procedure is applied universally and uniformly to all relevant individuals. </p> <p>Implementers of the system, in this respect, should be able to show that any model output is replicable when the same rules and procedures are applied to the same inputs. Such a uniformity of the application of rules and procedures secures the equal procedural treatment of decision subjects and precludes any rule-changes in the algorithmic processing targeted at a specific person that may disadvantage that individual vis-\u00e0-vis any other. It is important to note that the consistent and uniform application of rules over time apply to deterministic algorithms, whose parameters are static and fixed after model development. Close attention should be paid, in this sense, to the procedural fairness issues that may arise with the use of dynamic learning algorithms. This is because the parameters and inferences of such systems evolve over time and can yield different outputs for the same inputs at different points in system\u2019s lifespan. </p>"},{"location":"skills-tracks/aeg/chapter4/aifairness/#metric-based-fairness","title":"Metric-based fairness","text":"<p>As part of the safeguarding of discriminatory non-harm and diligent fairness and equity considerations, well-informed consideration must be put into how you are going to define and measure the formal metrics of fairness that can be operationalised into the AI system you are developing.  </p> <p>Metric-based fairness involves the mathematical mechanisms that can be incorporated into an AI model to allocate the distribution of outcomes and error rates for relevant subpopulations (e.g., groups with protected or sensitive characteristics). In formulating your approach to metric-based fairness, your project team will be confronting challenging issues like the justifiability of differential treatment based on sensitive or protected attributes, where differential treatment can indicate differences in the distribution of model outputs or the distribution of error rates and performance indicators like precision or sensitivity.  </p> <p>There is a great diversity of beliefs in this area as to what makes the consequences of an algorithmically supported decision allocatively equitable, fair, and just. Different approaches to metric-based fairness\u2014detailed below\u2014stress different principles: some focus on demographic parity, some on individual fairness, others on error rates equitably distributed across subpopulations. Regardless of this diversity of views, your project team must ensure that the choices made regarding formal fairness metrics are lawful and conform to governing equality, non-discrimination, and human rights laws. Where appropriate, relevant experts should be consulted to confirm the legality of such choices.   </p> <p>Your determination of metric-based fairness should heavily depend both on the specific use case being considered and the technical feasibility of incorporating your chosen criteria into the construction of the AI system. (Note that different fairness-aware methods involve different types of technical interventions at the pre-processing, modelling, or post-processing stages of production). Again, this means that determining your fairness definition should be a cooperative and multidisciplinary effort across the project team.    </p> <p>You will find below a summary table of several of the main definitions of metric-based fairness that have been integrated by researchers into formal models as well as a list of current articles and technical resources, which should be consulted to orient your team to the relevant knowledge base. (Note that this is a rapidly developing field, so your technical team should keep updated about further advances.) The first four fairness types fall under the category of group fairness and allow for comparative criteria of non-discrimination to be considered in model construction and evaluation. The final two fairness types focus instead on cases of individual fairness, where context-specific issues of effective bias are considered and assessed at the level of the individual agent.     </p> <p>Take note, though, that these technical approaches have limited scope in terms of the bigger picture issues of application and design fairness that we have already stressed. Moreover, metric-based approaches face other practical and technical barriers. For instance, to carry out group comparisons, formal approaches to fairness require access to data about sensitive/protected attributes as well as accurate demographic information about the underlying population distribution (both of which may often be unavailable or unreliable, and furthermore, the work of identifying sensitive/protected attributes may pose additional risks of bias). Metric-based approaches also face challenges in the way they handle combinations of protected or sensitive characteristics that may amplify discriminatory treatment. These have been referred to as intersectional attributes (e.g. the combination gender and race characteristics), and they must also be integrated into fairness and equity considerations. Lastly, there are unavoidable trade-offs, inconsistencies, or even incompatibilities, between mathematical definitions of metric-based fairness that must be weighed in determining which of them are best fit for a particular use case. For instance, the desideratum for equalised odds (error rate balance) across subgroups can clash with the desideratum for equalised model calibration (correct predictions of gradated outcomes) or parity of positive predictive values across subgroups.[@chouldechova2017]-[@flores2016]-[@friedler2021]-[@kleinenberg2017]-[@mitchell2021]   </p>"},{"location":"skills-tracks/aeg/chapter4/aifairness/#selecting-fairness-metrics","title":"Selecting fairness metrics","text":"<p>Demographic/statistical parity (group fairness)</p> <p>An outcome is fair if each group in the selected set receives benefit in equal or similar proportions, i.e. if there is no correlation between a sensitive or protected attribute and the allocative result.  [@calders2009]-[@denis2021]-[@dwork2011]-[@feldman2015]-[@zemel2013] This approach is intended to prevent disparate impact, which occurs when the outcome of an algorithmic process disproportionately harms members of disadvantaged or protected groups.</p> <p>True positive rate parity (group fairness)</p> <p>An outcome is fair if the \u2018true positive\u2019 rates of an algorithmic prediction or classification are equal across groups.[@zafar2017] This approach is intended to align the goals of bias mitigation and accuracy by ensuring that the accuracy of the model is equivalent between relevant population subgroups. This method is also referred to as \u2018equal opportunity\u2019 fairness because it aims to secure equalised odds of an advantageous outcome for qualified individuals in a given population regardless of the protected or disadvantaged groups of which they are members.    </p> <p>Equalised odds (group fairness)</p> <p>An outcome is fair if false positive and true positive rates are equal across groups. In other words, both the probability of incorrect positive predictions and the probability of correct positive predictions should be the same across protected and privileged groups.[@hardt2016]-[@verma2018] This approach is motivated by the position that sensitive groups and advantaged groups should have similar error rates in outcomes of algorithmic decisions.</p> <p>Positive predictive value parity (group fairness)</p> <p>An outcome is fair if the rates of positive predictive value (the fraction of correctly predicted positive cases out of all predicted positive cases) are equal across sensitive and advantaged groups.[@chouldechova2017] Outcome fairness is defined here in terms of a parity of precision, where the probability of members from different groups actually having the quality they are predicted to have is the same across groups.</p> <p>Individual fairness (individual fairness)</p> <p>An outcome is fair if it treats individuals with similar relevant qualifications similarly. This approach relies on the establishment of a similarity metric that shows the degree to which pairs of individuals are alike with regard to a specific task.[@dwork2012]-[@kusner2017]</p> <p>Counterfactual fairness (individual fairness)</p> <p>An outcome is fair if an automated decision made about an individual belonging to a sensitive group would have been the same were that individual a member of a different group in a closest possible alternative (or counterfactual) world.[@kusner2017] Like the individual fairness approach, this method of defining fairness focuses on the specific circumstances of an affected decision subject, but, by using the tools of contrastive explanation, it moves beyond individual fairness insofar as it brings out the causal influences behind the algorithmic output.  It also presents the possibility of offering the subject of an automated decision knowledge of what factors if changed, could have influenced a different outcome. This could provide them with actionable recourse to change an unfavourable decision.</p>"},{"location":"skills-tracks/aeg/chapter4/aifairness/#system-implementation-fairness","title":"System implementation fairness","text":"<p>When your project team is approaching the beta stage, you should begin to build out your plan for implementation training and support. This plan should include adequate preparation for the responsible and unbiased deployment of the AI system by its on-the-ground users. Automated decision-support systems present novel risks of bias and misapplication at the point of delivery, so special attention should be paid to preventing harmful or discriminatory outcomes at this critical juncture of the AI project lifecycle. In order to design an optimal regime of implementer training and support, you should pay special attention to the unique pitfalls of bias-in-use to which the deployment of AI technologies give rise. These can be loosely classified as decision-automation bias (more commonly just \u2018automation bias\u2019) and automation-distrust bias:</p> <p>Decision-Automation Bias I (over-reliance)</p> <p>Users of automated decision-support systems may tend to become hampered in their critical judgement, rational agency, and situational awareness as a result of their faith in the perceived objectivity, neutrality, certainty, or superiority of the AI system (Gaube et al., 2021).  This may lead to over-reliance or errors of omission, where implementers lose the capacity to identify and respond to the faults, errors, or deficiencies, which might arise over the course of the use of an automated system, because they become complacent and overly deferent to its directions and cues.[@bussone2015]</p> <p>Decision-Automation Bias II (over-reliance)</p> <p>Decision-automation bias may also lead to over-compliance or errors of commission where implementers defer to the perceived infallibility of the system and thereby become unable to detect problems emerging from its use for reason of a failure to hold the results against available information. Both over-reliance and over-compliance may lead to what is known as out-of-loop syndrome where the degradation of the role of human reason and the de-skilling of critical thinking hampers the user\u2019s ability to complete the tasks that have been automated. This condition may bring about a loss of the ability to respond to system failure and may lead both to safety hazards and to dangers of discriminatory harm.   To combat risks of decision-automation bias, you should operationalise strong regimes of accountability at the site of user deployment to steer human decision-agents to act on the basis of good reasons, solid inferences, and critical judgment.</p> <p>Automation-Distrust Bias</p> <p>At the other extreme, users of an automated decision-support system may tend to disregard its salient contributions to evidence-based reasoning either as a result of their distrust or scepticism about AI technologies in general or as a result of their over-prioritisation of the importance of prudence, common sense, and human expertise.[@longoni2019] An aversion to the non-human and amoral character of automated systems may also influence decision subjects\u2019 hesitation to consult these technologies in high impact contexts such as healthcare, transportation, and law.[@dietvorst2015] \u2018Research shows that people often prefer humans\u2019 forecasts to algorithms\u2019 forecasts,[@diab2011]-[@eastwood2012] more strongly weigh human input than algorithmic input,[@onkal2009]-[@promberger2006] and more harshly judge professionals who seek out advice from an algorithm rather than from a human.[@shaffer2013]-[@dietvorst2015] </p>"},{"location":"skills-tracks/aeg/chapter4/aifairness/#taking-account-of-the-context-of-impacted-individuals-in-system-implementation","title":"Taking account of the context of impacted individuals in system implementation","text":"<p>In cases where you are utilising a decision-support AI system that draws on statistical inferences to determine outcomes which affect individual persons (e.g., a predictive risk model that helps an adult social care work determine an optimal path to caring for an elderly patient), fair implementation processes should include considerations of the specific context of each impacted individual.[@binns2017]-[@binns2018]-[@binns2019]-[@eiselson2013] Any application of this kind of system\u2019s recommendation will be based on statistical generalisations, which pick up relationships between the decision recipient\u2019s input data and patterns or trends that the AI model has extracted from the underlying distribution of that model\u2019s original dataset. Such generalisations will be predicated on inferences about a decision subject\u2019s future behaviours (or outcomes) that are based on populational-level correlations with the historical characteristics and attributes of the members of groups to which that person belongs rather than on the specific qualities of the person themself. </p> <p>Fair and equitable treatment of decision subjects entails that their unique life circumstances and individual contexts be taken into account in decision-making processes that are supported by AI-enabled statistical generalisations. For this reason, you should train your implementers to think contextually and holistically about how these statistical generalisations apply to the specific situation of the decision recipient. This training should involve preparing implementers to work with an active awareness of the socio-technical aspect of implementing AI decision-assistance technologies from an integrative and human- centred point of view. You should train implementers to apply the statistical results to each particular case with appropriate context-sensitivity and \u2018big picture\u2019 sensibility. This means that the dignity they show to decision subjects can be supported by interpretive understanding, reasonableness, and empathy.</p>"},{"location":"skills-tracks/aeg/chapter4/aifairness/#ecosystem-fairness","title":"Ecosystem fairness","text":"<p>The AI project lifecycle does not exist in isolation from, or independent of, the wider social system of economic, legal, cultural, and political structures or institutions in which the production and use of AI systems take place. Rather, because it is embedded in these structures and institutions, the policies, norms, and procedures through which such structures and institutions influence human action also influence the AI project lifecycle itself. Inequities and biases at this ecosystem level can steer or shape AI research and innovation agendas in ways that can generate inequitable outcomes for protected, marginalised, vulnerable, or disadvantaged social groups. Such ecosystem-level inequities and biases may originate in and further reinforce asymmetrical power structures, unfair market dynamics, and skewed research funding schemes that favour or bring disproportionate benefit to those in the majority, or those who wield disproportionate power in society, at the cost of those who are disparately impacted by the discriminatory outcomes of the design, development, and use of AI technologies. </p> <p>Ecosystem-level inequities can occur across a wide range of AI research and innovation contexts. For instance, when AI-enabled health interventions such as mobile-phone-based symptom checker apps or remote AI-assisted medical triaging or monitoring are designed without regard for the barriers to access faced by protected, vulnerable, or disadvantaged groups, they will disproportionately benefit users from other, more advantaged groups. Likewise, where funding of the development of AI technologies, which significantly affect the public interest, is concentrated in the hands of firms or vendors who singly pursue commercial or financial gain, this may result in exclusionary research and innovation environments, AI systems that are built without due regard for broad fairness and equity impacts, and limitations on the development and deployment of wide-scale, publicly beneficial technology infrastructure. Moreover, widespread structural and institutional barriers to diversity and inclusion can create homogeneity on AI project teams (and among organisational policy owners) that has consequential fairness impacts on application decisions, resource allocation choices, and system deployment strategies.  </p> <p>The concept of ecosystem fairness highlights the importance of mitigating the range of inequity-generating path dependencies that originate at the ecosystem level and that are often neglected by or omitted from analyses of AI project lifecycles. Ecosystem fairness therefore focuses both on rectifying the social structures and institutions that engender indirect discrimination and on addressing the structural and institutional changes needed for the corrective modification of social patterns that engender discriminatory impacts and socio-economic disadvantage. In this way, ecosystem fairness involves the transformation of unjust economic, legal, cultural, and political structures or institutions with the aim of the universal realisation of equitable social arrangements.</p>"},{"location":"skills-tracks/aeg/chapter4/bias/","title":"Bias Mitigation","text":""},{"location":"skills-tracks/aeg/chapter4/bias/#bias-self-assessment-and-risk-management","title":"Bias Self-assessment and Risk Management","text":"<p>By pinpointing risks of bias or downstream discriminations, project teams can streamline possible solutions in a proactive, pre-emptive, and anticipatory way. This is what fairness-aware design and implementation will enable the team to do.</p> <p>At each stage of the AI project lifecycle, a collaborative Bias Self-assessment should be carried out with regard to the applicable dimension of fairness. This self-assessment consists of three steps:</p> <p>Step 1: Familiarising with biases and fairness types that are relevant to each project stage.</p> <p>Step 2: Reflecting and identifying how a particular AI project might be vulnerable to biases that may arise at each stage and pose risks to each relevant fairness type.</p> <p>Step 3: Determine and document bias risk mitigation actions that will be implemented to correct any existing problems that have been identified, strengthen areas of weakness that have possible discriminatory consequences, and take proactive bias-prevention measures in areas that have been identified as potential sources of risk.</p> <p>The Fairness Self-Assessment and Risk Mitigation template will help you go through this process. It locates a set of social, statistical, and cognitive biases within specific steps of an AI project lifecycle. These biases require ongoing reflection and deliberation to minimise the possible negative impact upon downstream activities or the risk of discriminatory outcomes.</p>"},{"location":"skills-tracks/aeg/chapter4/bias/#list-of-biases","title":"List of Biases<sup>1</sup>","text":""},{"location":"skills-tracks/aeg/chapter4/bias/#world-biases","title":"World biases","text":"<p>Historical bias: Historical bias concerns pre-existing societal patterns of discrimination and social injustice\u2014and the prejudices and discriminatory attitudes that correspond to such patterns. These patterns, prejudices, and attitudes can be drawn into every stage of the AI innovation lifecycle and be perpetuated, reinforced, or exacerbated through inequitable innovation ecosystem dynamics and the pursuit of biased application choices and research agendas. They can also arise in AI innovation contexts when historical patterns of inequity or discrimination are inadvertently or unintentionally reproduced, or even augmented, in the development and use of an AI system\u2014even when the system is functioning to a high standard of accuracy and reliability.[@mehrabi2019]-[@suresh2019] For instance, even with scientifically sound sampling and feature selection, a project will exhibit historical bias where it perpetuates (or exacerbates) socioeconomic inequalities through the outcomes it generates.</p> <p>Structural racism: Structural racism (also sometimes called systemic racism) is a form of racial discrimination that is \u2018not simply the result of private prejudices held by individuals, but is also produced and reproduced by laws, rules, and practices, sanctioned and even implemented by various levels of government, and embedded in the economic system as well as in cultural and societal norms\u2019.[@bailey2021] Other forms of discrimination such as sexism, classism, ableism, ageism, antisemitism, and transphobia can also similarly have structural or systemic aspects.</p> <p>Institutional bias: Institutional bias is \u2018a tendency for the procedures and practices of particular institutions to operate in ways which result in certain social groups being advantaged or favoured and others being disadvantaged or devalued. This need not be the result of any conscious prejudice or discrimination but rather of the majority simply following existing rules or norms\u2019.[@chandler2011] </p>"},{"location":"skills-tracks/aeg/chapter4/bias/#data-biases","title":"Data biases","text":"<p>Representation bias: When a population is either inappropriately represented (e.g. not allowing sufficient self-representation in demographic variables) or a sub-group is underrepresented in the dataset, the model may subsequently fail to generalise and under-perform for a sub-group (or sub-groups).[@feng2022]-[mehrabi2019]-[@suresh2019] For example, representation bias could arise in a symptom-checking application that has been trained on a data collected exclusively through smartphone use or online interaction as this dataset would likely underrepresent groups within the general population like elderly people who may lack access to smartphones or connectivity.</p> <p>Selection bias: Selection bias is a term used for a range of biases that affect the selection or inclusion of data points within a dataset. In general, this bias arises when an association is present between the variables being studied and additional factors that make it more likely that some data will be present in a dataset when compared to other possible data points in the space.[@mehrabi2019] For instance, where individuals differ in their geographic or socioeconomic access to an activity or service that is the site of data collection, this variation may result in their exclusion from the corresponding dataset. Likewise, where certain socioeconomically deprived or marginalised social groups are disproportionately dependent on a social service to fulfil basic needs, it may be over sampled if data is collected from the provision of that service.   </p> <p>Admission bias: Admission rate bias is a subtype of selection bias. It occurs when the exposure to a risk and the occurrence of a disease increases the likelihood of admission or referral (hence, this is also known as \u2018referral bias\u2019). As a result, the cases included in any subsequent study or dataset will systematically differ from the population at large.[@spencer2017] </p> <p>Diagnostic decision bias: Diagnostic decision bias occurs in medical datasets that are based on implicit biases in clinical judgement about patient diagnosis (for instance, where biased medical training and unrepresentative or discriminatory clinical knowledge lead to the under-diagnosis of minority or under-served groups). This leads to \u2018differential diagnosis of patients with the same pathology, on the basis of demographic features\u2019 (Straw &amp; Callison-Burch, 2020, p. 2) and, ultimately, datasets that reflect biased diagnoses rather than the true rate of disease.</p> <p>Chronological bias: Chronological bias arises when individuals in the dataset are added at different times, and where this chronological difference results in individuals being subjected to different methods or criteria of data extraction based on the time their data were recorded. For instance, if the dataset used to build a predictive risk model in children\u2019s social care spans over several years, large-scale care reforms, policy changes, adjustments in relevant statutes (such as changes to legal thresholds or definitions), and changes in data recording methods may create major inconsistencies in the data points extracted from person to person.</p> <p>Diagnostic access bias: Where barriers to patient access exist for diagnostic tests, there can be systematic exclusion of sub-groups from a dataset, resulting in the under- or over-estimation of the true prevalence or incidence of the disease.[@banerjee2017] These barriers can exist for myriad reasons, including cultural (e.g. distrust of formal healthcare institutions), geographic (e.g. remote living), socioeconomic (e.g. unable to get time off from childcare or work), among other reasons.   </p> <p>Prevalence-incidence bias: Prevalence-incidence bias occurs as a result of the timing in which cases are included in the dataset for a study.[@spencer2017] For example, excluding patients who have died could result in the appearance of decreased severity for a disease. A longer time frame between exposure and investigation could increase the likelihood of additional patients dying or recovering and being excluded from an analysis.   </p> <p>Data coding bias: Data coding bias occurs when the misrepresentation or erasure of demographic characteristics such as gender or ethnicity by biased coding systems obscures patient needs, adversely impacts patient\u2019s access to appropriate screenings, diagnosis, and treatments, and, subsequently, prejudices the datasets in which biased coding is embedded.</p> <p>Missing data bias: Missing data can cause a wide variety of issues within an AI project, and these data may be missing for a variety of reasons related to broader social factors. Missing data bias can lead to inaccurate inferences and affect the validity of the model where it is the result of non-random but statistically informative events.[@chen2021]-[@feng2022] For instance, missing data bias may arise in predictive risk models used in social care to detect potentially harmful behaviour in adolescents where interview responses and longitudinal data collected over extended periods of time are used as part of the dataset. This can be seen in cases where interview questions about socially stigmatised behaviours or traits like drug use or sexual orientation trigger fears of punishment, humiliation, or reproach and thus prompt non-responses, and in cases where data collection over time leads to the inconsistent involvement and drop-out of study participants.   </p> <p>Wrong sample-size bias: Using the wrong sample size for the study can lead to chance findings that fail to represent adequately the variability of the underlying data distribution, in the case of small samples, or findings that are statistically significant but not relevant or actionable, in the case of larger samples. Wrong sample size bias may occur in cases where model designers have included too many features in a machine learning algorithm. This is often referred to as the \u2018curse of dimensionality\u2019, a mathematical phenomenon wherein increases in the number of features or \u2018data dimensions\u2019 included in an algorithm means that exponentially more data points need to be sampled to enable good predictive or classificatory performance.</p> <p>Measurement quality bias: Measurement quality bias arises when under-resourced clinical environments lack the personnel, expertise, training capacity, and digital maturity to consistently collect high quality &amp; complete data. They can also occur during deployment when measurement tools &amp; devices are designed for dominant groups and consequently mismeasure minoritised, disadvantaged, or underserved groups.</p>"},{"location":"skills-tracks/aeg/chapter4/bias/#design-development-and-deployment-biases","title":"Design, development, and deployment biases","text":"<p>Hardware bias: Hardware bias arises where physically instantiated algorithmic systems or measurement devices are not designed to consider the diverse physiological needs of minoritised, marginalised, disadvantaged, or other non-majority stakeholders. When deployed, such systems will therefore perform less effectively for members of these groups due to their design.</p> <p>Annotation bias: Annotation bias occurs when annotators incorporate their subjective perceptions into their annotations.[@chen2021]-[kuwatly2020] Data is often annotated by trained expert annotators or crowdsourced annotators. In its simplest form, annotators can choose an inaccurate label due to fatigue or lack of focus[@hovy2021] but annotation bias can also result from positionality limitations that derive from demographic features, such as age, education, or first language,[kuwatly2020] and other systemic cultural or societal biases that influence annotators.[@chen2021] For instance, annotators may label differently facial expressions of different ethnic, age, or gender groups,[@chen2021] have different levels of familiarity with communication norms,[@hovy2021] or different understandings of what should be annotated as harmful content.[@rottger2022] When the role of annotator subjectivity is unacknowledged or annotators are not specifically trained to mitigate biases, there are greater chances that the model will incorporate annotation biases and be unfair.[@chen2021]-[@rottger2022]</p> <p>Label or label choice bias: A label (or target variable) used within an algorithmic model may not have the same meaning for all data subjects. There may be a discrepancy between what sense the designers are seeking to capture in a label, or what they are trying to measure in it, and the way that affected individuals understand its meaning.[@corbett-davies2018]-[@feng2022]-[@obermeyer2019]-[@rajkomar2018] Where there is this kind of variation in meaning for different groups within a population, adverse consequences and discriminatory impact could follow. For example, designers of a predictive model in public health may choose \u2018patient wellbeing\u2019 as their label, defining it in terms of disease prevalence and hospitalisation. However, subpopulations who suffer from health disparities and socioeconomic deprivation may understanding wellbeing more in terms of basic functionings, the food security needed for health promotion, and the absence of the social environmental stressors that contribute to the development of chronic medical conditions. Were this predictive model to be used to develop public health policy, members of this latter group could suffer from a further entrenchment of poor health outcomes.</p> <p>Measurement bias: Measurement bias addresses the choice of how to measure the labels or features being used. It arises when the measurement scale or criteria being applied fails to capture data pertaining to the concepts or constructs that are being measured in a fair and equitable manner.[@jacobs2021]-[@mehrabi2019]-[@mitchell2021]-[olteanu2019]-[@suresh2019] For example, a recidivism risk model that uses prior arrests or arrested relatives as proxies to measure criminality may surface measurement bias insofar as patterns of arrest can reflect discriminatory tendencies to over-police certain protected social groups or biased assessments on the part of arresting officers rather than true criminality.</p> <p>Cohort bias: Cohort bias is a subtype of measurement bias where model designers use categories that default \u2018to traditional or easily measured groups without considering other potentially protected groups or levels of granularity (e.g. whether sex is recorded as male, female, or other or more granular categories)\u2019.[@rajkomar2018] </p> <p>Model selection bias: Model selection bias occurs when AI designers and developers choose a model that does not sufficiently respond to the needs and requirements of the research question or problem and the domain context or use-case. This may result not only in a lack of appropriate transparency and explainability (where a complex model is chosen and the context demands interpretable results) but also in outcomes based upon model inferences that do not reflect the level of nuance needed to confront the question or problem itself.</p> <p>Evaluation bias: Evaluation bias occurs during model iteration and evaluation as a result (1) of the application of evaluative metrics that mask the differential performance of an AI model for subgroups or (2) of the use of benchmarking datasets that do not accurately represent the composition of the target population.[@suresh2019] For example, an evaluation bias may occur where performance metrics that measure only overall accuracy are applied to a trained computer vision system that performs differentially for subgroups that have different skin tones. Likewise, evaluation biases arise where the external benchmark datasets that are used to evaluate the performance of trained models are insufficiently representative of the populations to which they will be applied. In the case of computer vision, this may occur where established benchmarks overly represent a segment of the populations (such as adult light-skinned males) and thus reinforce the biased criteria for optimal performance.</p> <p>Semantic bias: Semantic bias occurs when discriminatory inferences are allowed to arise in the architecture of a trained AI model and to remain an element of the productionalised system. When historical biases are baked into datasets in the form of discriminatory proxies or embedded prejudices (e.g. word embeddings that pick up on racial or gender biases), these biases can be semantically encoded in the model\u2019s co-variates and parameters.[@hovy2022] Semantic biases occur when model design and evaluation processes fail to detect and mitigate such discriminatory aspects.</p> <p>Confounding: Confounding is a well-known causal concept in statistics, and commonly arises in observational studies. It refers to a distortion that arises when a (confounding) variable independently influences both the dependant and independent variables (e.g. exposure and outcome), leading to a spurious association and a skewed output. Clear examples of confounding can be found in the use of electronic health records (EHRs) that arise in clinical environments and healthcare processes. EHRs are observational data and often reflect not only the health status of patients, but also patients\u2019 interactions with the healthcare system. This can introduce confounders such as the frequency of inpatient medical testing reflecting the busyness or labour shortages of medical staff rather than the progression of a disease during hospitalisation, differences between onset of a disease and the date of diagnosis, and health conditions that are missing from the EHRs of a patient due to a non-random lack of testing. Contextual awareness and domain knowledge are crucial elements for identifying and redressing confounders.</p> <p>Social determinant blindness: Social determinant blindness occurs where clinical researchers and/or AI model designers act with a lack of awareness of the environmental, occupational, and life-course exposures of patients who face precarity, historical marginalisation, and discrimination. This leads to \u2018a lack of data on the complete etiologic context and exposures of a patient\u2019s health, including lack of opportunities and resources, such as broadband connectivity and social factors that can trigger adverse health outcomes\u2019.[@dankwa-mullan2022]</p> <p>Aggregation bias: Aggregation bias arises when a \u2018one-size-fits-all\u2019 approach is taken to the outputs of a trained algorithmic model (i.e. one where model results apply evenly to all members of the impacted population) even where variations in subgroup characteristics mean that mapping functions from inputs to outputs are not consistent across these subgroups.[@mehrabi2019]-[@suresh2019] In other words, in a model where aggregation bias is present, even when combinations of features affect members of different subgroups differently, the output of the system disregards the relevant variations in conditional distributions for the subgroups. This results in the loss of information, lowered performance, and, in cases where data from one subgroup is more prevalent than those of others, the development of a model that is more effective for that sub-group. Good examples of aggregation bias come up in clinical decision-support systems in medicine, where clinically significant variations between sexes and ethnicities\u2014in terms of disease aetiology, expression, complications, and treatment\u2014mean that systems which aggregate results by treating all data points similarly will not perform optimally for any subgroup.</p> <p>Reporting bias: Reporting bias arises when systems are produced without transparently reported evidence of effectiveness across demographic categories or testing for differential performance across sensitive, protected, or intersectional subgroups. This deficit in equity-aware testing and reporting can lead system developers and users to disregard or to attempt to bypass public accountability for equity, fairness, and bias mitigation. This means that transparency and accountability measures that should be in place to ensure that there are no discriminatory harms resulting from the use of the system are obscured or deprioritised.</p> <p>Population bias and training-serving skew: Population bias occurs when the demographics or characteristics of the cohort that comprises the training dataset differ from those of the original target population to whom the model is applied.[@mehrabi2019]-[@olteanu2019] For instance, a polygenic risk model that was trained primarily on data from a cohort of people of European ancestry in a country that has significant ethnic diversity is applied to people of non-European descent in that country and consequently performs worse for them. Another example arises in the social media research context where the demographic composition of internet platform users is skewed towards certain population subgroups and hence can differ from the studied target population.[@olteanu2019] Training-serving skew occurs, in a similar manner, when a model is deployed for individuals whose data is dissimilar to the dataset used to train, test, and validate the model.[@feng2022] This can occur, for instance, where a trained model is applied to a population in a geographical area different from where the original data was collected, or to the same population but at a time much later than when the training data was collected. In both cases, the trained model may fail to generalise because the new, out-of-sample inputs are being drawn from populations with different underlying distributions.</p> <p>Race correction bias: Race correction bias arises when algorithmic systems, which are adjusted or corrected for race or ethnicity, guide clinical decisions in ways that may direct more attention, care, or resources to Caucasian patients or to patients from other advantaged ethnic groups than to members of racialised and minoritised groups.[@vyas2020] They can also lead to informed mistrust, reluctance, and care avoidance on the part of racialised and minoritised groups and thus to harmful health outcomes for them. It has been argued that race correction is not only an unreliable proxy for genetic and biological difference but also fails to surface the underlying social causes of health disparities.</p> <p>Cause-effect bias: Cause-effect bias arises when users or implementers of decision-support systems, which generate inferences based upon statistical correlations, mistakenly assume that correlation implies causation without examining the validity of such an attribution.[@fazelpour2021]-[@mehrabi2019] For instance, a model for predicting pneumonia risk and hospital re-admission shows that patients with asthma have a lower risk of death than non-asthmatics, leading users to erroneously conclusion that having asthma is a protective factor in the risk scenario. However, a closer look at the model\u2019s inferences demonstrates that the correlation of asthma to lower risk was attributable to the fact that patients in the cohort known to have asthma were, as a common practice, automatically triaged to the Intensive Care Unit as a precautionary measure, and this meant that they had a 50% reduction in mortality risk.[@caruana2015]</p> <p>Implementation bias: Implementation bias refers to any bias that arises when a system is implemented or used in ways that were not intended by the designers or developers but, nevertheless, made more likely due to affordances of the system or its deployment. For example, a biometric identification system that is used by a public authority to assist in the detection of potential terrorist activity could be repurposed to target and monitor activists or political opponents.</p> <p>Decision-automation bias: Decision-automation bias arises when users of automated decision-support systems become hampered in their critical judgment, rational agency, and situational awareness as a result of their faith in the efficacy of the system. This may lead to over-reliance or errors of omission, where implementers lose the capacity to identify and respond to the faults, errors, or deficiencies, which might arise over the course of the use of an automated system, because they become complacent and overly deferent to its directions and cues. Decision-automation bias may also lead to over-compliance or errors of commission where implementers defer to the perceived infallibility of the system and thereby become unable to detect problems emerging from its use for reason of a failure to hold the results against available information.</p> <p>Automation-distrust bias: Automation-distrust bias arises when users of an automated decision-support system disregard its salient contributions to evidence-based reasoning either as a result of their distrust or scepticism about AI technologies in general or as a result of their over-prioritisation of the importance of prudence, common sense, and human expertise. An aversion to the non-human and amoral character of automated systems may also influence decision subjects\u2019 hesitation to consult these technologies in high impact contexts such as healthcare, transportation, and law.</p> <p>Dismissal bias: Dismissal bias is a \u2018conscious or unconscious desensitisation to alerts that are systematically incorrect for a protected group (e.g. an early warning score for patients with sepsis). Alert fatigue is a form of this\u2019.[@rajkomar2018]</p> <p>Status quo bias: An affectively motivated preference for \u201cthe way things are currently\u201d, which can prevent more innovative or effective processes or services being implemented. This bias is most acutely felt during the transition between projects, such as the choice to deprovision a system and begin a new project, in spite of deteriorating performance from the existing solution. Although this bias is often treated as a cognitive bias, we highlight it here as a social bias to draw attention to the broader social or institutional factors that in part determine the status quo.</p>"},{"location":"skills-tracks/aeg/chapter4/bias/#ecosystem-biases","title":"Ecosystem biases","text":"<p>Ecosystem bias: Ecosystem bias occurs when economic, legal, cultural, and political structures or institutions\u2014and the policies, norms, and procedures through which these structures and institution influence human action\u2014steer AI research and innovation agendas in ways that generate inequitable outcomes for minoritised, marginalised, vulnerable, historically discriminated against, or disadvantaged social groups.[@schwartz] Ecosystem biases, which exist in the wider social system wherein AI technologies are designed and used, may originate in and further entrench asymmetrical power structures, unfair market dynamics, and skewed research funding schemes that favour or bring disproportionate benefit to those in the majority, or those who wield disproportionate power in society, at the cost of those who are disparately impacted by the discriminatory outcomes of the design, development, and use of AI technologies.</p> <p>Privilege bias: Privilege bias occurs when health policies, institutions, and infrastructures skew the benefits of healthcare technologies and medical devices disproportionately towards privileged social groups. \u2018Models may be unavailable in settings where protected groups receive care or require technology/sensors disproportionately available to the nonprotected class\u2019.[@rajkomar2018]</p> <p>Research bias: Research bias occurs where there is a deficit in social and health equity standards to guide how AI research and innovation is funded, conducted, reviewed, published, and disseminated. It can also manifest in a lack of inclusion and diversity on research teams and in clinical trials, and limited studies incorporating representative real-world data for health insights.[@dankwa-mullan2022] Research bias additionally includes inequitable manifestations of funding structures or in incentives set by investors or funding institutions.</p> <p>Allocation discrepancy: Allocation discrepancy occurs when resources (such as extra clinical attention or social services) are withheld from a protected group because it is associated with fewer positive predictions.[@rajkomar2018]</p> <p>Optimism bias: Also known as the planning fallacy, optimism bias can lead project teams to under-estimate the amount of time required to implement a new system or plan adequately. In the context of the project lifecycle, this bias may arise during project planning, but can create downstream issues when implementing a model during the model productionalisation stage, due to a failure to recognise possible system engineering barriers.</p> <p>Law of the instrument (Maslow\u2019s hammer): This bias is best captured by the popular phrase \u2018If all you have is a hammer, everything looks like a nail\u2019. The phrase cautions against over-reliance on a particular tool or method, often one that is familiar to members of the project team. For example, a project team that is composed of experts in a specific ML technique may over-use that technique and mis-apply it in a context where a different technique would be better suited, or where it would be better not to use ML/AI technology at all.</p> <p>McNamara fallacy: McNamara fallacy describes the belief that quantitative information is more valuable than other information.[@schwartz2022] This can lead to scientistic reductivism,[@leslie2022] technochauvinism,[@broussard2018] or technological solutionism.[@morozov2013] The McNamara fallacy plays a significant role in biasing AI innovation processes when AI researchers, designers, and developers view algorithmic techniques and statistical insights as the only inputs capable of solving societal problems, thereby actively disregarding interdisciplinary understandings of the subtle historical and sociocultural contexts of inequity and discrimination.</p> <p>Informed mistrust: Informed mistrust occurs where, \u2018given historical exploitation and unethical practices, protected groups may believe that a model is biased against them. These patients may avoid seeking care from clinicians or systems that use the model or deliberately omit information. The protected group may be harmed by not receiving appropriate care\u2019.[@rajkomar2018]</p> <p>De-agentification bias: De-agentification bias occurs when social structures and innovation practices systemically exclude minoritised, marginalised, vulnerable, historically discriminated against, or disadvantaged social groups from participating or providing input in AI innovation ecosystems. \u2018Protected groups may not have input into the development, use, and evaluation of models. They may not have the resources, education, or political influence to detect biases, protest, and force correction\u2019.[@rajkomar2018]</p> <p>Feedback loops: Feedback loops occur where \u2018the clinician accepts the recommendation of a model even when it is incorrect to do so, [and therefore] the model's recommended versus administered treatments will always match. The next time the model is trained, it will learn to continue these mistakes\u2019.[@rajkomar2018]</p> <p>Positive results bias: Positive results bias, also known as publication bias, refers to the system-wide or social phenomenon of observing a skewed level of positive results published in journals, because negative or null results tend to go unpublished.[@pluddeman2017] The consequence of this can be the overestimation of efficacy for specific techniques or methods, as well as research duplication since other research teams might attempt to repeat studies that have already been performed but not published. An example of this was observed in the well-known 'reproducibility crisis' that affected the social psychology literature.</p> <p>Biases of rhetoric or spin: Biases of rhetoric or spin occur during the communication of research or development (e.g. model performance) and refer to the use of unjustified or illegitimate forms of persuasive language that lacks meaningful content or substantive evidence.[@heneghan2017] These biases relate to overemphasis of the performance or efficacy of a technique or intervention (e.g. showing comparative preference for the favoured technique to the detriment of alternatives).</p>"},{"location":"skills-tracks/aeg/chapter4/bias/#cognitive-biases","title":"Cognitive biases","text":"<p>Status quo bias: Status quo bias is an affectively motivated preference for \u201cthe way things are currently\u201d, which can prevent more innovative or effective processes or services being implemented. This bias is most acutely felt during the transition period between projects, such as the choice not to deprovision a system and begin a new project, despite deteriorating performance from the existing solution. Although this bias is often treated as a cognitive bias, we also highlight it here as a social bias to draw attention to the broader social or institutional factors that in part determine the status quo.</p> <p>Confirmation bias: Confirmation bias arises from the tendency to search for, gather, or use information that confirms pre-existing ideas and beliefs, and to dismiss or downplay the significance of information that disconfirms one\u2019s favoured hypothesis. This can be the result of motivated reasoning or sub-conscious attitudes, which in turn may lead to prejudicial judgements that are not based on reasoned evidence. For example, confirmation biases could surface in the judgment of the user of an AI decision-support application, who believes in following common sense intuitions acquired through professional experience rather than the outputs of an algorithmic model and, for this reason, dismisses its recommendations regardless of their rational persuasiveness or veracity.</p> <p>Self-assessment bias: A tendency to evaluate one\u2019s abilities in more favourable terms than others, or to be more critical of others than oneself. In the context of a project team, this could include the overly positive assessment the group\u2019s abilities (e.g. through reinforcing groupthink). For instance, during project planning, a project team may believe that their resources and capabilities are sufficient for the objective of the project, but in fact be forced to either cut corners or deliver a sub-par product.</p> <p>Availability bias: The tendency to make judgements or decisions based on the information that is most readily available (e.g. more easily recalled). When this information is recalled on multiple occasions, the bias can be reinforced through repetition\u2014known as a 'cascade'. This bias can cause issues for project teams throughout the project lifecycle where decisions are influenced by available or oft-repeated information (e.g. hypothesis testing during data analysis).</p> <p>Na\u00efve realism: A disposition to perceive the world in objective terms that can inhibit recognition of socially constructed categories. For instance, treating \u2018employability\u2019 something that is objectively measurable and, therefore, able to be predicted by a machine learning algorithm based on objective factors (e.g. exam grades, educational attainment).</p> <ol> <li> <p>Some of the biases in this list are specific to health care systems, bust most are widely applicable. They are taken from CITE.\u00a0\u21a9</p> </li> </ol>"},{"location":"skills-tracks/aeg/chapter4/fairness/","title":"Introduction to the principle of fairness","text":"<p>In this section, we will explore the complicated landscape of AI fairness definition as a preliminary step towards understanding the ways in which existing biases manifest in the design, development, and deployment of AI systems. Understanding how concepts of fairness are used in the field of AI ethics and governance is a crucial prerequisite to understanding where and how unfair biases arise across the AI project lifecycle, because relevant notions of fairness operate both as ethical and legal criteria based upon which biases can be identified across the AI project workflow and as normative yardsticks against which they can be measured and then appropriately mitigated.  </p> <p>When thinking about AI fairness, it is important to keep in mind that these technologies, no matter how neutral they may seem, are designed and produced by human beings, who are bound by the limitations of their own given contexts and by biases that can arise both in their cognitive processes and in the social environments that influence their actions and interactions. Pre-existing or historical configurations of discrimination and social injustice\u2014as well as the prejudices and biased attitudes that are shaped by such configurations\u2014can be drawn into the AI innovation lifecycle and create unfair biases at any point in the project workflow.  This is the case from the earliest stages of agenda setting, problem selection, project planning, problem formulation, and data extraction to later phases of model development and system deployment. Additionally, the datasets used to train, test, and validate AI/ML models can encode socially and historically crystallised forms of inequity and discrimination, thereby embedding biases in an algorithmic model\u2019s variables, inferences, and architecture.</p> <p>This wide range of entry points for bias and discrimination across the AI/ML innovation lifecycle has complicated the notion of fairness, since the inception of fairness-centred approaches like \u2018discrimination-aware data mining\u2019 and \u2018fair machine learning\u2019 more than a decade ago.[@barocas2016a]-[@binns2017]-[@friedman1996]-[@hajian2013].</p> <p>To be sure, possibilities for reaching consensus on a commonly accepted definition for AI/ML fairness and on how to put such a definition into practice have been hampered by the myriad technical and sociotechnical contexts in which fairness issues arise. Such prospects for consensus have also been challenged by the broad spectrum of views in society on what the concept of fairness means and how it should best be operationalised. For this reason, in this practical guidance, we take a context-based and society-centred approach to understanding AI/ML fairness that is anchored in two pillars.</p> <p>First, to understand how concepts of fairness are defined and applied in AI innovation contexts, we must begin by acknowledging that there is a plurality of views in the social world on the meaning of fairness\u2014myriad interpretations of its sense and significance within and across cultures, societies, and legal systems. For instance, the meaning of the term \u2018fairness\u2019, in its contemporary English language usage, has dozens of interpretations which include a range of related but distinctive ideas such as equity, consistency, non-discrimination, impartiality, justice, equality, honesty, and reasonableness.[@audard2014]-[@carr2017] </p> <p>Likewise, the translation of the word \u2018fairness\u2019 into other languages has proven to be notoriously difficult, with some researchers claiming that it cannot be consistently understood across different linguistic groups.[@audard2014]-[@vandenberghe2022] It is clear, from this vantage point, that fairness (and adjacent notions like equity, impartiality, justice, equality, and non-discrimination) must be approached with an appropriately nuanced responsiveness both to the many ways in which these concepts can be interpreted and to the many contexts in which they can be applied. </p> <p>Second, despite this pluralism in the understanding and application of the concept of fairness, there has been a considerable convergence around how the interrelated priorities of non-discrimination and equality constitute the justificatory nucleus of fairness concerns. Though some claim that fairness is ultimately a subjective value that varies according to individual preferences and cultural outlooks, general ethical and legal concepts of fairness are predicated on core beliefs in the equal moral status of all human beings and in the corollary right of all human beings to equal respect, concern, protection, and regard before the law. </p> <p>On this view, it is because each person possesses an intrinsic and equal moral worth that everyone deserves equal respect and concern\u2014respect and concern that is grounded in the common dignity and humanity of every person.[@dworkin2000]-[@carr2000]-[@giovanola2022]</p> <p>The normative core of fairness, in this respect, has to do with the moral duty to treat others as moral equals and to secure the membership of all in a \u2018moral community\u2019 where every person can regard themselves as having equal value.[@vlastos1984] Wrongful discrimination, along these lines, occurs when decisions, actions, institutional dynamics, or social structures do not respect the equal moral standing of individual persons.[@eidelson2015]-[@giovanola2022]-[@sangiovanni2017]</p> <p>Convergence around this centrality of equality and non-discrimination as an indispensable cornerstone of fairness concerns has also led to their widespread acceptance as normative anchors of both international human rights law and anti-discrimination and equality statutes. In human rights law, interlocking principles of equality and non-discrimination are taken to be essential preconditions for the realisation of all human rights insofar as equality and non-discrimination are implied in the guarantee of the equal enjoyment and protection of fundamental rights and freedoms to all human beings per se.[@clifford2013] For this reason, principles of equality and non-discrimination are treated as jus cogens in human rights law\u2014i.e., they are treated as peremptory or foundational norms that permeate all human rights provisions and from which no derogation is permitted in any case.[@carozza2013]-[@clifford2013]</p> <p>In anti-discrimination and equality statutes in the UK and beyond, dovetailing priorities of equality and non-discrimination likewise form principal aims and essential underpinnings of fairness concerns. In this case, equality before the law manifests as equal protection from discriminatory harassment and from both direct and indirect kinds of discrimination (UK Equality Act, 2010). In discriminatory harassment, unwanted or abusive behaviour linked to a protected characteristic violates someone\u2019s dignity, degrades their identity, or creates an offensive environment for them. For example, an employer who makes a racist remark about a protected group in the presence of an employee from that racial background would be considered to have harassed that employee based on the protected characteristic of race. </p> <p>What are protected characteristics?</p> <p>In the 2010 UK Equality Act, protected classes include age, gender reassignment, being married or in a civil partnership, being pregnant or on maternity leave, disability, race including colour, nationality, ethnic or national origin, religion or belief, sex, and sexual orientation. The European Convention on Human Rights, which forms the basis of the UK\u2019s 1998 Human Rights Act, includes as protected characteristics \u2018sex, race, colour, language, religion, political or other opinion, national or social origin, association with a national minority, property, birth or other status'.</p> <p>Direct discrimination occurs when individuals are treated adversely based on their membership in some protected class. This type of discrimination is also known as \u2018disparate treatment\u2019 because it involves instances where otherwise similarly positioned individuals receive different and more-or-less favourable treatment on the basis of differences between their respective protected characteristics. For instance, direct discrimination would occur if an otherwise well-qualified job applicant were intentionally denied an opportunity for employment because of their age, disability, or sexual orientation.  </p> <p>By contrast, indirect discrimination occurs when existing provisions, criteria, policies, arrangements, or practices\u2014which could appear on their face to be neutral\u2014disparately harm or unfairly disadvantage members of some protected class in comparison with others who are not members of that group. This type of discrimination is also known as \u2018disparate impact\u2019 because what matters here is not directly unfavourable treatment in individual cases but rather the broader disproportionate adverse effects of provisions, criteria, policies, arrangements, or practices that may subtly or implicitly disfavour members of some protected group while appearing to treat everyone equally. Indirect discrimination can therefore involve the impacts of tacitly unjust or unfair social structures, underlying inequalities, or systemic patterns of implicit historical bias that manifest unintentionally through prevailing norms, rules, policies, and behaviours. For instance, indirect discrimination would occur if a job advertisement specified that applicants needed to be native English speakers, for this would automatically disadvantage candidates of different nationalities regardless of their levels of fluency or language training.</p> <p>These three facets of anti-discrimination and equality law (harassment, direct discrimination, and indirect discrimination) have significantly shaped contemporary approaches to AI fairness.[@adams-prassl2022]-[@liu2018]-[@ntoutsi2019]-[@pessach2020]-[@watcher2021]</p> <p>Indeed, attempts to put the principle of AI fairness into practice have largely converged around the priority to do no discriminatory harm to affected people along each of these three vectors of potential injury. It is thus helpful to think about basic AI fairness considerations as tracking three corresponding questions (An example of an associated discriminatory harm is provided alongside each.):  </p> <ol> <li> <p>How could the use of the AI system we are planning to build or acquire\u2014or the policies, decisions, and processes behind its design, development, and deployment\u2014lead to the discriminatory harassment of impacted individuals (i.e., unwanted or abusive treatment of them which is linked to a protected characteristic and which violates their dignity, degrades their identity, or creates a humiliating or offensive environment for them)?</p> Example <p>An AI-enabled customer support chat bot is built from a large language model that has been pre-trained on billions of data points scraped from the internet and then customised to provide tailored responses to customer questions about the provision of a public service. After the system goes live, it is soon discovered that, when certain customer names (which are indicative of protected classes) are entered, the chatbot generates racist and sexist text responses to customer inquiries.   </p> </li> <li> <p>How could the use of the AI system we are planning to build or acquire\u2014or the policies, decisions, and processes behind its design, development, and deployment\u2014lead to the disproportionate adverse treatment of impacted individuals from protected groups on the basis of their protected characteristics?</p> Example <p>An AI system used to filter job applications in a recruitment process is trained on historic data that contains details about the characteristics of successful candidates over the past several years. Because white male applicants were predominantly hired over this time, the system learns to infer the likelihood of success based on proxy features connected to the protected characteristics of race and sex. It consequently filters out non-white and non-male job candidates from the applicant pool.    </p> </li> <li> <p>How could the use of the AI system we are planning to build or acquire\u2014or the policies, choices, and processes behind its design, development, and deployment\u2014lead to indirect discrimination against impacted individuals from protected groups?</p> Example <p>An AI-enabled medical diagnosis tool is built as a smartphone application and made available to all participants in a national health system without sufficient considerations of the barriers to access faced by some citizens. It becomes clear after the app is launched that the device disproportionately favours younger, more digitally literate, and more affluent community members, while disadvantaging both the elderly, less digitally literate population and digitally deprived people who do not have access to smartphone technologies and internet connections. </p> </li> </ol> <p>It is important to note, regarding this final question on indirect discrimination, that the Public Sector Equality Duty mandates considerations both of how to \u2018reduce the inequalities of outcome which result from socio-economic disadvantage\u2019 and of how to advance equality of opportunity and other substantive forms of equality. This means that our approach to putting the principle of AI fairness into practice must include social justice considerations that concentrate on how the production and use of AI technologies can address and rectify structural inequalities and institutionalised patterns of inequity and discrimination rather than reinforce or exacerbate them. We must consequently take a multi-pronged approach to AI fairness that integrates formal approaches to non-discrimination and equality (which focus primarily on consistent and impartial application of rules and equal treatment before the law) with more demanding substantive and transformative approaches(which focus on equalizing the distribution of opportunities and outcomes and on the fundamental importance of addressing the material pre-conditions and structural changes needed for the universal realisation of equitable social arrangements).</p> <p>What is social justice?</p> <p>Social justice is a commitment to the achievement of a society that is equitable, fair, and capable of confronting the root causes of injustice. In an equitable and fair society, all individuals are recognised as worthy of equal moral standing and are able to realise the full assemblage of fundamental rights, opportunities, and positions. </p> <p>In a socially just world, every person has access to the material means needed to participate fully in work life, social life, and creative life through the provision of proper education, adequate living and working conditions, general safety, social security, and other means of realising maximal health and well-being. </p> <p>Social justice also entails the advancement of diversity and participatory parity and a pluralistically informed recognition of identity and cultural difference. Struggles for social justice typically include accounting for historical and structural injustice coupled to demands for reparations and other means of restoring rights, opportunities, and resources to those who have been denied them or otherwise harmed.]</p>"},{"location":"skills-tracks/aeg/chapter4/fairness/#discriminatory-non-harm","title":"Discriminatory non-harm","text":"<p>While there are different ways to characterise or define fairness in the design and use of AI systems, you should consider the principle of discriminatory non-harm as a minimum required threshold of fairness. This principle directs us to do no harm to others through direct or indirect discrimination or through discriminatory harassment linked to a protected characteristic that violates the dignity of impacted individuals, degrades their identity, or creates a humiliating or offensive environment for them:</p> <p>Key Concept: Principle of Discriminatory Non-Harm (Do No Discriminatory Harm)</p> <p>The producers and users of AI systems should prioritise the identification and mitigation of biases and discriminatory influences, which could lead to direct or indirect discrimination or discriminatory harassment. This entails an end-to-end focus on how unfair biases and discriminatory influences could arise (1) in the processes behind the design, development, and deployment of these systems, (2) in the outcomes produced by their implementation, and (3) in the wider economic, legal, cultural, and political structures or institutions in which the AI project lifecycle is embedded\u2014and in the policies, norms, and procedures through which these structures and institution influence actions and decisions throughout the broader AI innovation ecosystem. Developers and users of AI systems should, in this respect, acknowledge and address discriminatory patterns that may originate in the data used to train, test, and validate the system and in the model architectures (i.e., the variables, parameters, inferences, etc.) that generate system outputs. </p> <p>Beyond this, the principle of discriminatory non-harm implies that producers and users of AI systems should ensure that their research, innovation, and implementation practices are undertaken in an optimally responsible and ethical manner, more broadly, in keeping with the historical tendency that deficiencies in the deployment and operation of faulty systems often disparately impact protected, under-represented, or disadvantaged groups.</p> <p>The principle of discriminatory non-harm applies to any AI system that processes social or demographic data (i.e., data pertaining to features of human subjects, population- and group-level traits and characteristics, or patterns of human activity and behaviour). However, the principle applies equally to AI systems that process bio-physical or biomedical data. In this case, imbalanced datasets, selection biases, or measurement errors could have discriminatory effects on impacted individuals and communities\u2014for instance, where a demographic group\u2019s lack of representation in a biomedical dataset (e.g., one used to train a diagnostic prediction model) means that the trained system performs poorly for that group relative to others that are better represented in the data.  </p> <p>Prioritising discriminatory non-harm implies that the producers and users of AI systems ensure that the decisions and behaviours of their models neither treat impacted individuals adversely based on their membership in some protected class or socioeconomic group nor, intentionally or unintentionally, generate discriminatory or inequitable impacts on affected individuals and communities that unfairly disadvantage members of some protected class or socioeconomic group in comparison with others who are not members of that class or group. It can also be seen as a proportional approach to bias mitigation because it sets a baseline for fair AI systems, while, nevertheless, creating conditions for developers and users to strive towards an ideal for fair and equitable outcomes for all people as moral equals and as members of a just community where every person can regard themselves as having intrinsic dignity and equal moral standing.  </p> <p>Finally, the scope of the principle means that, beyond designers and users, any individuals, organisations, or departments who are procuring AI systems must ensure that the vendors of such systems can demonstrate the mitigation of potential biases and discriminatory influences in the processes behind their production and in their outputs. </p>"},{"location":"skills-tracks/aeg/chapter4/governance/","title":"Governance","text":""},{"location":"skills-tracks/aeg/chapter4/governance/#putting-accountability-into-practice","title":"Putting accountability into practice","text":"<p>Now that we have explored some of the main aspects of the concept of accountability, we are ready to examine\u2013in greater detail\u2013how elements of answerability and auditability can be put into practice.</p> <p>The central importance of the end-to-end operability of good governance practices should guide your strategy to embed accountability across the project workflow. Three components are essential to creating a such a workflow:</p> <ol> <li> <p>Maintaining strong regimes of professional and institutional transparency.</p> </li> <li> <p>Establishing and maintaining a clear and accessible Process-Based Governance Framework (PBG Framework).</p> </li> <li> <p>Establishing a well-defined auditability trail for your PBG Framework through robust activity logging protocols that are consolidated digitally in a Process Log.</p> </li> </ol>"},{"location":"skills-tracks/aeg/chapter4/governance/#maintaining-professional-and-institutional-transparency","title":"Maintaining professional and institutional transparency","text":"<p>At every stage of the design and implementation of your AI project, team members should be held to rigorous standards of conduct that secure and maintain professionalism and institutional transparency. These standards should include the core values of selflessness, integrity, honesty, accountability, openness, sincerity, neutrality, objectivity, impartiality, and leadership. </p> <p>Furthermore, from start to finish of the AI project lifecycle, the design, development, and deployment process should be as transparent and as open to public scrutiny as possible with restrictions on accessibility to relevant information limited to the reasonable protection of justified confidentiality and of analytics that may tip off bad actors to methods of gaming the system of service provision or otherwise taking advantage of their insight to the detriment of the system's performance or the rest of the users.</p>"},{"location":"skills-tracks/aeg/chapter4/governance/#process-based-governance-framework","title":"Process-based governance framework","text":"<p>We have looked at some of the most important values and principles necessary for establishing responsible innovation practices in the AI project lifecycle.</p> <p>Perhaps the most vital of these measures is the effective operationalisation of these practices.The recently-adopted standard, ISO 37000, defines governance as \u2018the system by which the whole organisation is directed, controlled, and held accountable to achieve its core purpose in the long run\u2019.</p> <p>Establishing a diligent and well-conceived governance framework that covers the entire design, development, and deployment process will provide the foundation for effectively establishing needed practical actions and controls, exhaustively distributing roles and responsibilities, and operationalising answerability and auditability throughout the AI lifecycle. Organising all of the governance actions into a PBG Framework is a way to better accomplish this task.</p> <p>The purpose of a PBG Framework is to provide a template for the integrations of the norms, values, and principles, which motivate and steer responsible innovation, with the actual processes that characterise the AI design and development pipeline. Establishing a PBG framework creates the baseline conditions for ensuring that the goal of instituting an AI innovation process that is accountable-by-design is achieved.</p> <p>A PBG Framework should give the team a landscape view of the governance actions that are organising the control structures of the project workflow. Constructing a good PBG Framework will provide the team with a big picture of:</p> <p>\u2022 The relevant stages of the workflow in which actions are necessary to meet governance goals</p> <p>\u2022 The relevant team members and roles involved in each governance action</p> <p>\u2022 Explicit time frames for any necessary follow-up actions, re-assessments, and continual monitoring</p> <p>\u2022 Clear and well-defined protocols for logging activity and for instituting mechanisms to assure end-to-end auditability and appropriate documentation</p> <p>The PBG framework asks that teams not only outline the governance actions established for individual projects, but also roles involved in each action, time frames for follow-up actions, and logging protocols.</p> <p></p> <p></p> <p></p>"},{"location":"skills-tracks/aeg/chapter4/governance/#establishing-proportional-governance-actions","title":"Establishing proportional governance actions","text":"<p>Just as with the determination of proportionate stakeholder involvement, the establishment of proportionate governance protocols should involve a preliminary assessment of the potential risks and hazards of the model or tool under consideration. Low-stakes AI applications that are not safety critical, do not directly impact the lives of people, and do not process potentially sensitive social and demographic data may need less proactive governance controls and processes than high-stakes projects. </p> <p>By completing the Project Summary Report and Stakeholder Impact Assessments, the project team will need to carry out evaluations of the scope of the possible risks that could arise from the project and of the potential hazards it poses to affected individuals and groups. These assessments of the dangers posed to individual wellbeing and public welfare will help formulate proportionate governance actions to be outlined in the PBG framework.</p> <p>Notwithstanding the importance of the need for this reasonable application of proportionate governance actions,  a strong regime of accountability-by-design across the project lifecycle should nonetheless be established. It may be the case that the assessment of potential risks and adverse impacts does not sufficiently anticipate the full range of possible harms. In instances where such unforeseen harms do arise,  proper mechanisms of anticipatory accountability and corresponding documentation protocols should already be in place, so that the best practices of the project team are demonstrable.</p> <p>Here is a summary picture of where all possible governance actions fit across the project workflow:</p> <p></p> <p>Each principle can be operationalised through specific processes:</p> <p>Sustainability</p> <p>Stakeholder engagement process (SEP)</p> <p>Process facilitating a contextually informed understanding of the social environment and human factors that may be impacted by, or may impact, individual AI projects, and the uptake of proportionate stakeholder engagement and input throughout the AI lifecycle.</p> <p>Stakeholder impact assessment (SIA)</p> <p>Process facilitating the iterative evaluation of the social impact and sustainability of individual AI projects, as well as the corroboration of these potential impacts in dialogue with stakeholders, when appropriate.</p> <p>Safety</p> <p>SSA &amp; RA (Safety Self-Assessment and  Risk Management)</p> <p>Process facilitating the evaluation of how AI projects align with safety objectives through the iterative identification and documentation of risks of potential safety risks across the lifecycle, and assurance actions implemented to address these.</p> <p>Responsible data management</p> <p>Data Factsheet</p> <p>Live document facilitating the uptake of best practices for Responsible Data Management and Stewardship across the AI project workflow by facilitating the documentation of a comprehensive record of the data lineage iterative assessments of data integrity, quality, protection, and privacy.</p> <p>Fairness</p> <p>BSA &amp; RM (Bias self-assessment &amp; risk management)</p> <p>Process facilitating the  evaluation of how AI projects align with the principle of fairness through the iterative identification and documentation of risks of bias across the lifecycle, and assurance actions implemented to address these.</p> <p>Fairness position statement</p> <p>Document establishing the metric-based fairness criteria for individual AI projects, providing an explanation in plain and nontechnical language.</p> <p>Accountability</p> <p>PBG Framework</p> <p>Live document outlining governance actions, relevant team members and roles involved in each action, time frames for follow-up actions, and logging protocols, for individual AI projects. </p> <p>Explainability</p> <p>Transparency &amp; EAM (explainability assurance management)</p> <p>Iterative process aimed to facilitate the implementation and evaluation of transparency and explainability assurance activities across the project lifecycle and assist in providing clarification of AI system outputs to a range of impacted stakeholders.</p>"},{"location":"skills-tracks/aeg/chapter4/governance/#accountability-across-the-workflow","title":"Accountability across the workflow","text":"<p>The task of establishing a PBG framework for the project should be initially undertaken in the project planning step of the project alongside the Project Summary Report  The results of the Stakeholder Analysis (particularly, the scoping of potential stakeholder impacts) should inform a proportional selection of governance actions within the PBG framework. At this stage, the PBG framework will provide a prospective and provisional plotting of governance actions, roles, and responsibilities for the project. This preliminary outline of governance structures will provide the necessary information for answering the Governance Framework Reflection questions within the PS Report.</p> <p>In the PS Report, the task of reflecting on your governance framework on your involves answering the following questions:</p> <ol> <li> <p>Do established governance actions proportionally mitigate possible harms to stakeholders posed by this project? If not, how can your PBG framework be rectified to address these potential harms?</p> </li> <li> <p>Does this distribution of responsibilities outlined in the PBG Framework establish a continuous chain of human accountability throughout the design, development, and deployment of this project? If not, how can any identified gaps or breaks in the chain be rectified in the PBG Framework?</p> </li> <li> <p>How will you ensure that all team members, who are assigned roles/responsibilities understand the roles/responsibilities that have been assigned to them?</p> </li> <li> <p>If you are procuring parts or elements of the system from third-party vendors, suppliers, sub-contractors, or external developers, how are you instituting appropriate governance controls that will establish end-to-end accountability, traceability, and auditability for these procured parts or elements?</p> </li> <li> <p>If any data being used in the production of the AI system will be acquired from a vendor, supplier, or third party, how are you instituting appropriate governance controls that will establish end-to-end accountability, answerability, and auditability across the data lifecycle?</p> </li> </ol> <p>These questions (alongside the rest of the PS Report) are to be revisited and updated as part of completing each iteration of the Stakeholder Impact Assessment, at each point informing any necessary updates to the project\u2019s governance structure (and PBG framework). The PBG Framework is therefore a live document reflecting a governance structure that responds to the emerging needs across the design, development, and deployment lifecycle. It is to be updated after each revisitation of the PS report to reflect the project\u2019s current governance structure.</p> <p>The process by which these questions are answered should be as collaborative and inclusive as possible. The aim is to involve all relevant members of the project team (and any other relevant managers, operators, or vendors), so that all people involved in the workflow can share input and come to understand expectations about their roles and responsibilities. Any future revisions or updates of this part of the PS Report should likewise include all affected parties.</p>"},{"location":"skills-tracks/aeg/chapter5/","title":"Transparency, Explainability, and CARE &amp; ACT Principles","text":"Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk))"},{"location":"skills-tracks/aeg/chapter5/#chapter-outline","title":"Chapter Outline","text":"<ul> <li>Transparency and Explainability</li> <li>Consider context</li> <li>Anticipate impacts</li> <li>Reflect on Purpose, Positionality, and Power</li> <li>Engage inclusively</li> <li>Act Responsibly</li> </ul> <p>Chapter Summary</p> <p>The last chapter of the course is divided into two sections. First, we will delve into the concepts of transparency and explainability in AI, looking at the difference between the two, and the various types of AI explanations.</p> <p>In the second half of the chapter we will bring everything back together with a review of the CARE &amp; ACT principles. These principles serve as a practical tool to ensure AI systems are developed in an ethical and responsible manner. They are the following: </p> <ul> <li>Consider context</li> <li>Anticipate impacts</li> <li>Reflect on purpose, positionality, and power</li> <li>Engage inclusively</li> <li>Act transparently and responsibly</li> </ul> <p>Learning Objectives</p> <ul> <li>Familiarise yourself with the concepts of transparency and explainability in the context of AI systems and what the difference between them is.</li> <li>Understand the difference between process-based and outcome-based explanations.</li> <li>Learn about the different types of explanations that may be required in terms of AI-assisted decisions, and what is required of each type.</li> <li>Familiarise yourself with the CARE &amp; ACT principles, and how they can be used as a practical tool for thinking about ethical and responsible design and development of AI systems.</li> </ul>"},{"location":"skills-tracks/aeg/chapter5/act/","title":"Act transparently and responsibly","text":"<p>The imperative of acting transparently and responsibly is a call to all AI and data science researchers, developers, deployers and users to marshal the habits of responsible research and innovation cultivated in the CARE processes to produce systems that prioritise data stewardship and that are robust, accountable, fair, non-discriminatory, explainable, reproducible, and replicable. </p> <p>While the mechanisms and procedures which are put in place to ensure that these normative goals are achieved will differ from project to project we can summarise the following priorities that should be incorporated into a team's governance, self-assessment, and reporting practices:</p> <ul> <li> <p>Full documentation of data provenance, lineage, linkage, and sourcing:</p> <p>This involves keeping track of and documenting both responsible data management practices across the entire project lifecycle, from data extraction or procurement and data analysis, cleaning, and pre-processing to data use, retention, deletion, and updating.[@bender2018]-[@gebru2021]-[@holland2018]</p> <p>It also involves demonstrating that the data is ethically sourced, responsibly linked, and legally available for the project's purposes[@weinhardt2020] and making explicit measures taken to ensure data quality (source integrity and measurement accuracy, timeliness and recency, relevance, sufficiency of quantity, dataset representativeness), data integrity (attributability, consistency, completeness, contemporaneousness, traceability, and auditability) and FAIR data (findable, accessible, interoperable, and reusable).   </p> </li> <li> <p>Full documentation of privacy, confidentiality, consent, and data protection due diligence.</p> <p>This involves demonstrating that data has been handled securely and responsibly from beginning to end of the project's lifecycle so that any potential breaches of confidentiality, privacy, and anonymity have been prevented and any risks of re-identification through triangulation and data linkage mitigated. </p> <p>Regardless of the jurisdictions of data collection and use, the rights and interests of data subjects should always aimed to be optimally protected by adhering to the highest standards of privacy preservation, data protection, and responsible data handling and storage.   </p> </li> </ul> <ul> <li> <p>Transparent and accountable reporting of processes and results and appropriate publicity of datasets.</p> <p>This involves that results of model design, development, and deployment should be carried out in a way that enables the interpretability, reproducibility, and replicability of the results. </p> <p>Research design, analysis, and reporting should be pursued in an interpretability-aware manner that prioritises process transparency, the understandability of models, and the accessibility and explainability of the rationale behind their results.    </p> </li> <li> <p>An end-to-end process for bias self-assessment.</p> <p>This should cover all research stages as well as all sources of biases that could arise in the data, in the data collection, in the data pre-processing, in the organising, categorising, describing, annotating, structuring of data (text-as-data, in particular), and in research design and execution choices.</p> </li> </ul>"},{"location":"skills-tracks/aeg/chapter5/anticipate/","title":"Anticipate impacts","text":"<p>Anticipating impacts of an AI system involves reflecting on and assessing the potential short-term and long-term effects the system may have on impacted individuals and on affected communities and social groups, more broadly.</p> <p>Why is this kind of anticipatory reflection important? Its purpose is to safeguard the sustainability of AI projects across the entire project lifecycle instead of taking an approach of dealing with issues as they appear. There is no guarantee that a team will be able to anticipate all potential impacts, but dealing with the most relevant ones before they become a problem ensures more sustainable systems overall (it is also a much more efficient use of resources over time).</p> <p>How does one  ensure that the activities and outputs of the AI system are socially and environmentally sustainable? Project team members must proceed with a continuous responsiveness to the real-world impacts that their system could have.</p> <p>The way to translate into practice as we have seen, is through concerted and stakeholder-involving exploration of the possible adverse and beneficial effects that could otherwise remain hidden from view if deliberate and structured processes for anticipating downstream impacts were not in place. </p> <p>Attending to sustainability, along these lines, also entails the iterative re-visitation and re-evaluation of impact assessments. To be sure, in its general usage, the word \u201csustainability\u201d refers to the maintenance of and care for an object or endeavour over time. In the context of AI, this implies that building sustainability into a project is not a \u201cone-off\u201d affair.</p> <p>Rather, carrying out an initial impact assessment at the inception of a project is only a first, albeit critical, step in a much longer, end-to-end process of responsive re-evaluation and re-assessment. Such an iterative approach ensures that continuous attention is payed both to the dynamic and changing character of the project lifecycle and to the shifting conditions of the real-world environments in which studies are embedded.</p> <p>Methodical impact evaluation should involve an initial adoption of normative criteria that function as metrics for scoping and assessing the possible harms and benefits of the research and its outputs. Taking GPAI\u2019s \u201c12 Principles and Priorities of Responsible Data Innovation\u201d as an example, relevant impact assessment questions could include:</p> <ul> <li>How, if at all, could our research and its outputs impact each of the following twelve principles and priorities as they relate to all affected stakeholders, especially those who are vulnerable, marginalised, or historically discriminated against? (Affected stakeholders include research subjects and participants, subjects of data collected for or used in the study, researchers, and all other impacted people and social groups.) </li> </ul> <p>12 Principles and Priorities of Responsible Data Innovation</p> <pre><code>- Respect for and protection of human dignity\n- Interconnectivity, solidarity, and intergenerational reciprocity\n- Environmental flourishing, sustainability, and the rights of the biosphere\n- Protection of human freedom and autonomy\n- Prevention of harm and protection of the right to life and physical, psychological, and moral integrity\n- Non-discrimination, fairness, and equality\n- Rights of Indigenous peoples and Indigenous data sovereignty \n- Data protection and the right to respect of private and family life\n- Economic and social rights\n- Accountability and effective remedy\n- Democracy\n- Rule of law\n</code></pre> <ul> <li> <p>How could our research and its outputs advance each of these twelve principles and priorities or hinder their realisation?</p> </li> <li> <p>Are there particular stakeholder groups who could disproportionately enjoy the benefits of the research and its outputs, or suffer from the potential harms they generate, as these harms and benefits relate to each of the twelve principles and priorities?</p> </li> <li> <p>If things go wrong in our research or if its outputs (especially tools produced or capacities enabled) are used out-of-the-scope of their intended purpose and function, what harms could be done to stakeholders in relation to each of the twelve principles and priorities?</p> </li> </ul> <p>It is important to note here that stakeholder involvement in the impact assessment process can be a critical safeguard against evaluative blind spots and omissions. </p> <p>Methodical impact evaluation should also involve an assessment of the severity of potential adverse impacts. This brings clarity to the prioritisation of impact mitigation actions by allowing the severity levels of potential negative effects to be differentiated, elucidated, and refined. As explained in the United Nations Guiding Principles on Business and Human Rights (UNGP), assessing the severity of potential negative impacts on fundamental rights and freedoms involves consideration of their scale, scope, and remediability, where scale is defined as \u201cthe gravity or seriousness of the impact,\u201d scope as \u201chow widespread the impact is, or the numbers of people impacted,\u201d and remediability, as the \u201cability to restore those affected to a situation at least the same as, or equivalent to, their situation before the impact\u201d (UNGP, 2011, Principle 14).  </p> <p>One notable challenge faced by researchers who are assessing the severity of potential adverse impacts is identifying cumulative or aggregate downstream impacts, which can be much more difficult than identifying harms directly or proximately caused by a project.</p> <p>Discerning these impacts may require additional research and consultation with domain experts and other relevant stakeholders. This difficulty results from the fact that cumulative impacts are often incremental and more difficult to perceive, and they frequently involve complex contexts of multiple actors or projects operating in the same area or sector or affecting the same populations.[@gotzmann2020] Some \u201cbig picture\u201d questions to reflect on when assessing cumulative or aggregate impacts include:</p> <ul> <li> <p>Could the project contribute to wider scale adverse impacts when its deployment is coordinated with (or occurs in tandem with) other projects or innovation activity that serve similar functions or purposes? For example, if the impacts of a  project that aims to discover an effective method of behavioural nudging at scale are considered in combination with the proliferation of many other similar projects or computational systems in a given sector, concerns about wider cumulative effects like mass manipulation, objectification, and infringement on autonomy and human dignity become relevant.</p> </li> <li> <p>Could the project replicate, reinforce, or augment socio-historically entrenched legacy harms that create knock-on effects in impacted individuals and groups? For example, if a project analyses sensitive personal information contained in databases scraped from social media websites without gaining the proper consent of research subject in accordance with their reasonable expectations, it could add to the legacy harms of companies that have used data recklessly and eroded public trust regarding the respect of privacy and data protection rights in the digital sphere. This can create wider chilling effects on elements of open communication, information sharing, and interpersonal connection that are essential components for the sustainability of democratic forms of life.</p> </li> <li> <p>Could the production and use of the system be understood to contribute to wider aggregate adverse impacts on the biosphere and on planetary health when its deployment is considered in combination with other systems that may have similar environmental impacts? For example, a project that involves moderate levels of energy consumption in model training or data storage may be seen to contribute to significant environmental impact when considered alongside the energy consumption of similar projects across research ecosystems.</p> </li> </ul> <p>Once impacts have been evaluated and the severity of any potential harms assessed, impact prevention and mitigation planning should commence. Diligent impact mitigation planning begins with a scoping and prioritization stage. Team members (and engaged stakeholders, where appropriate) should go through all the identified potential adverse impacts and map out the interrelations and interdependencies between them as well as surrounding social factors (such as contextually specific stakeholder vulnerabilities and precariousness) that could make impact mitigation more challenging. Where prioritization of prevention and mitigation actions is necessary (for instance, where delays in addressing a potential harm could reduce its remediability), decision-making should be steered by the relative severity of the impacts under consideration. As a general rule, while impact prevention and mitigation planning may involve prioritization of actions, all potential adverse impacts must be addressed. When potential adverse impacts have been mapped out and organised, and mitigation actions have been considered, the research team (and engaged stakeholders, where appropriate) should begin co-designing an impact mitigation plan (IMP). The IMP will become the part of your transparent reporting methodology that specifies the actions and processes needed to address the adverse impacts which have been identified and that assigns responsibility for the completions of these tasks and processes. As such, the IMP will serve a crucial documenting function. </p> <p>Establishment of protocols for re-visitation and re-evaluation of the research impact assessment:</p> <p>Impact assessments must pay continuous attention both to the dynamic and changing character of the project lifecycle and to the shifting conditions of the real-world environments in which research practices, results, and outputs are embedded. There are two sets of factors that should inform when and how often initial impact assessments are re-visited to ensure that they remain adequately responsive to factors that could present new potential harms or significantly influence impacts that have been previously identified: </p> <ol> <li> <p>Lifecycle and production factors: Choices made at any point along the workflow may affect the veracity of prior impact assessments\u2014leading to a need for re-assessment, reconsideration, and amendment. For instance, design choices could be made that were not anticipated in the initial impact assessment (such choices might include adjusting the variables that are included in the model, choosing more complex algorithms, or grouping variables in ways that may impact specific groups). These changes may influence how a computational model performs, how it is explained, or how it impacts affected individuals and groups. Processes are also iterative and frequently bi-directional, and this often results in the need for revision and update. For these reasons, impact assessments must remain agile, attentive to change, and at-the-ready to evaluatively move back and forth across the decision-making pipeline as downstream actions affect upstream choices and evaluations.</p> </li> <li> <p>Environmental factors: Changes in project-relevant social, regulatory, policy or legal environments (occurring during the time in which the research is taking place) may have a bearing on how well the resulting computational model works and on how the research outputs impact affected individuals and groups. Likewise, domain-level reforms, policy changes, or changes in data recording methods may take place in the population of concern in ways that affect whether the data used to train the model accurately portrays phenomena, populations, or related factors in an accurate manner. In the same vein, cultural or behavioral shifts may occur within affected populations that alter the underlying data distribution and hamper the predictive and explanatory efficacy of a model, which has been trained on data collected prior to such shifts. All of these alterations of environmental conditions can have a significant effect on how research practices, outputs, and results impact affected individual and communities.</p> </li> </ol>"},{"location":"skills-tracks/aeg/chapter5/consider/","title":"Consider context","text":"<p>No AI system exists in a vacuum. They are all embedded in a wider socio-technical environment which will affect the way the system's deployment functions. Therefore, considering the wider context the system operates in is imperative for responsible research and innovation in AI.</p> <p>This translates into think diligently about the conditions and circumstances surrounding the system, its operation and its outputs, including the norms, values, and interests that inform the people undertaking the development of the project and that shape and motivate the reasonable expectations of the project's stakeholders.</p> <p>Some of the questions to bear in mind when considering context are:</p> <ul> <li> <p>How are these norms, values and interests influencing or steering the project and its outputs? </p> </li> <li> <p>How could they influence the the users\u2019 meaningful consent and expectations of privacy, confidentiality, and anonymity?</p> </li> <li> <p>How could they shape a project\u2019s reception and impacts across impacted communities? </p> </li> </ul> <p>Considering these questions will ensure reflection within the project team, and will help to anticipate the potential negative impacts the use of an AI system may have.</p> <p>Considering context also involves taking into account the specific domain(s), geographical location(s), and jurisdiction(s) in which the project is situated and reflecting on the expectations of affected stakeholders in these specific contexts. Some relevant questions are:</p> <ul> <li> <p>How are do the existing institutional norms and rules in a given domain or jurisdiction shape expectations regarding project goals, practices, and outputs? </p> </li> <li> <p>How do the unique social, cultural, legal, economic, and political environments in which different projects are embedded influence the conditions of data generation, the intentions and behaviours of the research subjects that are captured by extracted data, and the space of possible inferences that data analytics, modelling, and simulation can yield?    </p> </li> </ul> <p>In summary</p> <p></p> <p>All in all, contextual considerations should, at minimum, track three vectors: </p> <ol> <li> <p>The first involves considering the contextual determinants of the condition of the production of the project (e.g., thinking about the positionality of the team, the expectations of the relevant community of practice, and the external influences on the aims and means of research by funders, collaborators, and providers of data and research infrastructure).</p> </li> <li> <p>The second involves considering the context of the  users of the system (e.g., thinking about subjects\u2019 reasonable expectations of gainful obscurity and \u2018privacy in public\u2019 and considering the changing contexts of their communications such as with whom they are interacting, where, how, and what kinds of data are being shared).</p> </li> <li> <p>The third involves considering the contexts of the social, cultural, legal, economic, and political environments in which different projects are embedded as well as the historical, geographic, sectoral, and jurisdictional specificities that configure such environments (e.g., thinking about the ways different social groups\u2014both within and between cultures\u2014understand and define key values, research variables, and studied concepts differently as well as the ways that these divergent understandings place limitations on what computational approaches to prediction, classification, modelling, and simulation can achieve).</p> </li> </ol>"},{"location":"skills-tracks/aeg/chapter5/engage/","title":"Engage Inclusively","text":"<p>If reflection on power dynamics, positionality, and the purpose of the project being developed is an inwards-facing process for the AI project team, stakeholder engagement and community involvement represents the outwards-facing side of the coin. </p> <p>As we have seen, engagement and involvement with the community can bolster a project\u2019s legitimacy, social license, and democratic governance as well as ensure that its outputs will possess an appropriate degree of public accountability and transparency. </p> <p>A diligent stakeholder engagement process can help teams to:</p> <ul> <li>identify stakeholder salience, </li> <li>undertake team positionality reflection, and</li> <li>facilitate proportionate community involvement and input throughout the research project workflow. </li> </ul> <p>This process can also safeguard the equity and the contextual accuracy of impact assessments and facilitate appropriate end-to-end processes of transparent project governance by supporting their iterative revisitation and re-evaluation.</p> <p>It is important to note, however, that all stakeholder engagement processes can run the risk either of being cosmetic or tokenistic. They can be employed to grant legitimacy to projects without substantially and meaningfully engaging with the impacted communities (i.e., being one-way information flows or nudging exercises that serve as public relations instruments).[@arnstein1969a]-[@tritter2006]</p> <p>To avoid such hazards of superficiality, team members should shore up a proportionate approach to stakeholder engagement through deliberate and precise goal setting. </p> <p>Factors that affect stakeholder engagement objective</p> <ol> <li> <p>Assessment of risks of adverse impacts: As we have stressed throughout this course, stakeholder involvement in projects should be proportionate to the scope of their potential risks and hazards.</p> </li> <li> <p>Assessment of positionality: Stakeholder involvement should address positionality limitations. For instance, in cases where the identity characteristics of team members do not sufficiently reflect or represent significantly impacted groups, stakeholder participation can \u201cfill gaps\u201d in knowledge, domain expertise, and lived experience.</p> </li> <li> <p>Assessment of project needs: Stakeholder involvement should help team members strengthen their ability to frame questions and to tackle  problems. After all, those impacted by the project are the most likely to know what their problems are, and thus, what issues the project should tackle. Team members should explore the optimal means for community members to actively contribute to their practices. </p> </li> </ol> <p>Practical challenges will be encountered when trying to operationalise a stakeholder engagement process. For example, limits on available resources and tight timelines could be at cross-purposes with the degree of stakeholder involvement that is recommended by team-based assessments of research needs, potential hazards, and positionality limitations. Likewise, the chosen degree of appropriate public participation may be unrealistic or out-of-reach given the engagement barriers that arise from constraints on the capacity of vulnerable stakeholder groups to participate, difficulties in reaching marginalised, isolated, or socially excluded groups, and challenges to participation that are presented by digital divides. In these instances, project teams should take a deliberate and reflective approach to deciding on how to balance engagement goals with practical considerations and should, at all events, make explicit the rationale behind their choices and document this. </p> <p>Regardless of any potential trade-offs, the establishment of clear and explicit stakeholder engagement goals should be prioritised. Relevant questions to pose in establishing these goals include:</p> <ul> <li>Why are we engaging with stakeholders? </li> <li>What do we envision the ideal purpose and the expected outcomes of engagement activities to be? </li> <li>How can we best drawn on the insights and lived experience of participants to inform and shape our project?  </li> </ul> <p></p>"},{"location":"skills-tracks/aeg/chapter5/reflect/","title":"Reflect on Purpose, Positionality and Power","text":"<p>Another crucial element of responsible research and innovation in AI supposes that the people who design, develop, and deploy a system engage in reflexive practices that scrutinise the way potential perspectival limitations and power imbalances can exercise influence on the equity and integrity projects and on the motivations, interests, and aims that steer them. </p> <p>The imperative of reflecting on purposes, positionality, power makes explicit the importance of this dimension of inward-facing reflection. This is a complement to the more outward-facing activities of stakeholder analysis, engagement, and impact assessment. </p> <p>As we have already discussed, all individual human beings come from unique places, experiences, and life contexts that shape their perspectives, motivations, and purposes. Reflecting on these contextual attributes is important insofar as it can help team members understand how their viewpoints might differ from those around them and, more importantly, from those who have diverging cultural and socioeconomic backgrounds and life experiences. </p> <p>Identifying and probing these differences enables individual team members to better understand how their own backgrounds, for better or worse, frame the way they see others, the way they approach and solve problems, and the way they carry out research and engage in innovation. By undertaking such efforts to recognise social position and differential privilege, they may gain a greater awareness of their own personal biases and unconscious assumptions. This then can enable them to better discern the origins of these biases and assumptions and to confront and challenge them in turn.</p> <p>Social scientists have long referred to this site of self-locating reflection as \u201cpositionality\u201d.[@bourke2014]-[@kezar2002]-[@merriam2001] When people take their own positionalities into account, and make this explicit, they can better grasp how the influence of their respective social and cultural positions potentially creates research strengths and limitations. On the one hand, one\u2019s positionality\u2014with respect to characteristics like  ethnicity, race, age, gender, socioeconomic status, education and training levels, values, geographical background, etc.\u2014can have a positive effect on an individual\u2019s contributions to a project; the uniqueness of each person\u2019s lived experience and standpoint can play a constructive role in introducing insights and understandings that other team members do not have. On the other hand, one\u2019s positionality can assume a harmful role when hidden biases and prejudices that derive from a person\u2019s background, and from differential privileges and power imbalances, creep into decision-making processes undetected and subconsciously sway the purposes, trajectories, and approaches of projects.</p> <p>When taking positionality into account, team members should reflect on their own positionality matrix. They should ask: To what extent do my personal characteristics, group identifications, socioeconomic status, educational, training, &amp; work background, team composition, &amp; institutional frame represent sources of power and advantage or sources of marginalisation and disadvantage? How does this positionality influence the team's ability to identify &amp; understand affected stakeholders and the potential impacts of the project? Answering these questions involves probing several other areas of self-ascription related to each researcher\u2019s contextual attributes:</p> <p></p> <p>A solid grasp on positionality allows team members to better interrogate and reflect on the power dynamics that could unduly influence research purposes and trajectories. Such reflections on power should involve an investigation of how power operates, and where it manifests, both across the  project lifecycle and in the real-world environments the project is situated. Leslie et al., propose a series of guiding questions that can be used as a reflective tool to help make potentially noxious power dynamics explicit:[@lesliedavid2022]</p> <ul> <li> <p>What, if any, power imbalances exist between me (or my team) and the communities impacted by our project?</p> </li> <li> <p>Do the projects I currently pursue reinforce or challenge these imbalances?</p> </li> <li> <p>How, if at all, do these imbalances result in unjust exercises of power? </p> </li> <li> <p>Are my current activities entrenching or combating such exercises of power?</p> </li> <li> <p>What are my interests (or my team\u2019s interest) in collecting or procuring data and in using these to build models and answer questions? </p> </li> <li> <p>How, if at all, are these interests similar to or different from the interests of those in the communities that research impacts?</p> </li> <li> <p>How, if at all, do any power imbalances that exist between me (or my  team) and impacted communities influence the pursuit of these interests in my (or my team\u2019s) agendas? </p> </li> <li> <p>How, if at all, do I (or my team) exploit power imbalances to pursue these interests?  </p> </li> <li> <p>What other actors hold power and influence over the agendas I pursue and the ways I collect or procure data and build and implement models and data applications?</p> </li> <li> <p>How reliant am I on the data, tools, models, and digital infrastructure (connectivity, computing resources, and data assets) provided by other actors?</p> </li> <li> <p>What are the interests of these actors? How are they similar to or different from my interests and from those of the members of the communities impacted by my data work?</p> </li> <li> <p>What, if any, power imbalances exist between these actors and me (and my firm or organisation)?</p> </li> <li> <p>What is the history of these power imbalances? Are current policies and available resources reinforcing or contesting these imbalances?</p> </li> <li> <p>How, if at all, do these imbalances result in unjust exercises of power? Are current policies and available resources enabling or combating such exercises of power?</p> </li> <li> <p>What does the institutional context of my team look like (taking into account the authority structure within my team(s), wider policy-ownership and power hierarchies in my organisation, levels of decision-making autonomy, and opportunities to voice concerns)? </p> </li> <li> <p>Does this institutional context enable my practices to safeguard the public interest and to ensure that standards and governance regimes in the research ecosystem are working towards just and societally beneficial outcomes?</p> </li> </ul>"},{"location":"skills-tracks/aeg/chapter5/transparency/","title":"Transparency and Explainability","text":""},{"location":"skills-tracks/aeg/chapter5/transparency/#introduction-to-transparency-and-explainability","title":"Introduction to transparency and explainability","text":""},{"location":"skills-tracks/aeg/chapter5/transparency/#defining-transparent-ai","title":"Defining transparent AI","text":"<p>Transparency as a principle of AI ethics differs a bit in meaning from the everyday use of the term. The common dictionary understanding of transparency defines it as either </p> <p>(1) the quality an object has when one can see clearly through it or</p> <p>(2) the quality of a situation or process that can be clearly justified and explained because it is open to inspection and free from secrets. </p> <p>Transparency as a principle of AI ethics encompasses both of these meanings:</p> <p>On the one hand, transparent AI involves the interpretability of a given AI system, i.e. the ability to know how and why a model performed the way it did in a specific context and therefore to understand the rationale behind its decision or behaviour. This sort of transparency is often referred to by way of the metaphor of \u2018opening the black box\u2019 of AI. It involves content clarification and intelligibility or explicability.</p> <p>On the other hand, transparent AI involves the justifiability of both of the processes that go into its design and implementation and of its outcome. It therefore involves the soundness of the justification of its use. In this more normative meaning, transparent AI is practically justifiable in an unrestricted way if one can demonstrate that both the design and implementation processes that have gone into the particular decision or behaviour of a system and the decision or behaviour itself are sustainable, safe, fair, and driven by responsibly managed data. </p>"},{"location":"skills-tracks/aeg/chapter5/transparency/#process-based-vs-outcome-based","title":"Process-based vs outcome-based","text":"<p>The two-pronged definition of transparency as a principle of AI ethics asks that the project team thinks about transparent AI </p> <ul> <li> <p>in terms of the process behind it (the design and implementation practices that lead to an algorithmically supported outcome); and,</p> </li> <li> <p>in terms of its product (the content and justification of that outcome).</p> </li> </ul> <p>This also means that explanations are provided to impacted stakeholders that demonstrate how the team and all others involved in the development of the system acted responsibly when choosing the processes behind its design and deployment; and make the reasoning behind the outcome of that decision clear.</p> <p>Process-based explanation of AI systems are about demonstrating that good governance processes and best practices have been followed throughout the design, development and use of the AI system. This entails demonstrating that considerations of sustainability, safety, fairness, and responsible data management were operative end-to-end in the project lifecycle. </p> <p>For example, if trying to explain the fairness and safety of a particular AI-assisted decision, one component of this explanation will involve establishing that adequate measures across the system\u2019s production and deployment have been taken to ensure that its outcome is fair and safe.</p> <p>Outcome-based explanations of AI systems are about clarifying the results of a specific decision. They involve explaining the reasoning behind a particular algorithmically generated outcome in plain, easily understandable, and everyday language that is socially meaningful to impacted stakeholders (understandable in terms of the contextual factors and relationships that it implicates). </p> <ul> <li>If there is meaningful human involvement in the decision-making process,  it should also be made clear to the affected individual how and why a human judgement that is assisted by an AI output was reached.</li> </ul> <p>In addition, an adequate explanation will also need to confirm that the actual outcome of an AI decision meets criteria previously established in the design process to ensure that the AI system is being used in a fair, safe, and ethical way. An explanation to affected stakeholders should also include a demonstration that a specific decision or behaviour of the system is sustainable, safe, fair, and driven by data that has been responsibly managed. </p> <p></p>"},{"location":"skills-tracks/aeg/chapter5/transparency/#six-main-types-of-explanations","title":"Six main types of explanations","text":"<p>Abstract</p> <ol> <li>Rationale explanation</li> <li>Responsibility explanation</li> <li>Data explanation</li> <li>Fairness explanation</li> <li>Safety and performance explanation</li> <li>Impact explanation</li> </ol>"},{"location":"skills-tracks/aeg/chapter5/transparency/#rationale-explanation","title":"Rationale explanation","text":"<p>What does this explanation help people understand?</p> <p>It is about the \u2018why?\u2019 of an AI decision. It helps people understand the reasons that led to a decision outcome, in an accessible way.</p> <p>What does this type of explanation include?</p> <ul> <li> <p>How the system performed and behaved to get to that decision outcome.</p> </li> <li> <p>How the different components in the AI system led it to transform inputs into outputs in a particular way. This will help communicate which features, interactions, and parameters were most significant.</p> </li> <li> <p>How these technical components of the logic underlying the result can provide supporting evidence for the decision reached.</p> </li> <li> <p>How this underlying logic can be conveyed as easily understandable reasons to decision recipients.</p> </li> <li> <p>How do the system\u2019s results apply to the concrete context and life situation of the affected individual.</p> </li> </ul> <p>What rationale explanations might answer:</p> <ul> <li> <p>Will the selected algorithmic model, or set of models, provide a degree of interpretability that corresponds with its impact on affected individuals?</p> </li> <li> <p>Are the supplementary explanation tools being used to help make the complex system explainable good enough to provide meaningful and accurate information about its underlying logic?</p> </li> </ul> <p>What information goes into rationale explanations</p> <p>As with the other types of explanation, rationale explanations can be process-based or outcome-based.</p> <p>Process-based explanations clarify:</p> <ul> <li> <p>How the procedures set up help provide meaningful explanations of the underlying logic of the AI model\u2019s results.</p> </li> <li> <p>How these procedures are suitable given the model\u2019s particular domain context and its possible impacts on the affected decision recipients and wider society.</p> </li> <li> <p>How the system\u2019s design and deployment workflow has been set up so that it is appropriately interpretable and explainable, including its data collection and preprocessing, model selection, explanation extraction, and explanation delivery procedures.</p> </li> </ul> <p>Outcome-based explanations provide:</p> <ul> <li> <p>The formal and logical rationale of the AI system \u2013 how the system is verified against its formal specifications. In this way, one can verify that the AI system will operate reliably and behave in accordance with its intended functionality.</p> </li> <li> <p>The technical rationale of the system\u2019s output \u2013 how the model\u2019s components (its variables and rules) transform inputs into outputs, so that the role these components play in producing that output is known. By understanding the roles and functions of the individual components, it is possible to identify the features and parameters that significantly influence a particular output.</p> </li> <li> <p>Translation of the system\u2019s workings \u2013 its input and output variables, parameters and so on \u2013 into accessible everyday language. This enables those in charge of the AI system to clarify, in plain and understandable terms, what role these factors play in reasoning about the real-world problem that the model is trying to address or solve.</p> </li> <li> <p>Clarification of how a statistical result is applied to the individual concerned.</p> </li> </ul>"},{"location":"skills-tracks/aeg/chapter5/transparency/#responsibility-explanation","title":"Responsibility explanation","text":"<p>What does this explanation help people understand?</p> <p>It helps people understand \u2018who\u2019 is involved in the development and management of the AI model, and \u2018who\u2019 to contact for a human review of a decision.</p> <p>What does this type of explanation include?</p> <ul> <li> <p>Who is accountable at each stage of the AI system\u2019s design and deployment, from defining outcomes for the system at its initial phase of design, through to providing the explanation to the affected individual at the end.</p> </li> <li> <p>Definitions of the mechanisms by which each of these people will be held accountable, as well as how the design and implementation processes of the AI system have been made traceable and auditable.</p> </li> </ul> <p>What information goes into responsibility explanations</p> <p>Process-based explanations clarify:</p> <ul> <li> <p>The roles and functions across the organisation that are involved in the various stages of developing and implementing an AI system, including any human involvement in the decision-making. If the system, or parts of it, are procured, then information about the providers or developers involved should be included.</p> </li> <li> <p>Broadly, what the roles do, why they are important, and where overall responsibility lies for management of the AI model \u2013 who is ultimately accountable.</p> </li> <li> <p>Who is responsible at each step from the design of an AI system through to its implementation to make sure that there is effective accountability throughout.</p> </li> </ul> <p>Outcome-based explanations:</p> <p>Because a responsibility explanation largely has to do with the governance of the design and implementation of AI systems, it is, in a strict sense, entirely process-based. Even so, there is important information about post-decision procedures that should be provided:</p> <ul> <li> <p>Cover information on how to request a human review of an AI-enabled decision or object to the use of AI, including details on who to contact, and what the next steps will be (e.g., how long it will take, what the human reviewer will take into account, how they will present their own decision and explanation).</p> </li> <li> <p>Give individuals a way to directly contact the role or team responsible for the review (this does not necessarily have to be a specific individual within the organisation).</p> </li> </ul>"},{"location":"skills-tracks/aeg/chapter5/transparency/#data-explanation","title":"Data explanation","text":"<p>What does this explanation help people understand?</p> <p>Data explanations are about the \u2018what\u2019 of AI-assisted decisions. They let people know what data about them were used in a particular AI decision, as well as any other sources of data. Generally, they also help individuals understand more about the data used to train and test the AI model. </p> <p>What does this type of explanation include?</p> <ul> <li> <p>How the data used to train, test, and validate an AI model was managed and utilised from collection through processing and monitoring.</p> </li> <li> <p>Which data was used in a particular decision and how.</p> </li> </ul> <p>What information goes into data explanations</p> <p>Process-based explanations include:</p> <ul> <li> <p>What training/testing/validating data was collected, the sources of that data, and the methods that were used to collect it.</p> </li> <li> <p>Who took part in choosing the data to be collected or procured and who was involved in its recording or acquisition. </p> </li> <li> <p>How procured or third-party provided data was vetted.</p> </li> <li> <p>How data quality was assessed and the steps that were taken to address any quality issues discovered, such as completing or removing data.</p> </li> <li> <p>What the training/testing/validating split was and how it was determined.</p> </li> <li> <p>How data pre-processing, labelling, and augmentation supported the interpretability and explainability of the model.</p> </li> <li> <p>What measures were taken to ensure the data used to train, test, and validate the system was representative, relevant, accurately measured, and generalisable.</p> </li> <li> <p>How potential bias and discrimination in the dataset have been mitigated.</p> </li> </ul> <p>Outcome-based explanations:</p> <ul> <li> <p>Clarify the input data used for a specific decision, and the sources of that data. This is outcome-based because it refers to the AI system\u2019s result for a particular decision recipient.</p> </li> <li> <p>In some cases, the output data may also require an explanation, particularly where the decision recipient has been placed in a category which may not be clear to them. For example, in the case of anomaly detection for financial fraud identification, the output might be a distance measure which places them at a certain distance away from other people based on their transaction history. Such a classification may require an explanation.</p> </li> </ul>"},{"location":"skills-tracks/aeg/chapter5/transparency/#fairness-explanation","title":"Fairness explanation","text":"<p>What does this explanation help people understand?</p> <p>The fairness explanation is about helping people understand the steps have been taken (and will continue to be taken) to ensure an AI decisions are generally unbiased and equitable.  It also gives people an understanding of whether or not they have been treated equitably themselves.</p> <p>What does this type of explanation include?</p> <p>An explanation of fairness can relate to several stages of the design, development and deployment of AI systems:</p> <p>A) Data fairness:  The system is trained and tested on properly representative, relevant, accurately measured, and generalisable datasets (note that this dataset fairness component will overlap with data explanation). </p> <p>This may include showing that the data used is:</p> <ul> <li> <p>as representative as possible of all those affected;</p> </li> <li> <p>sufficient in terms of its quantity and quality, so it represents the underlying population and the phenomenon being modelled;</p> </li> <li> <p>assessed and recorded through suitable, reliable and impartial sources of measurement and has been sourced through sound collection methods;</p> </li> <li> <p>up-to-date and accurately reflects the characteristics of individuals, populations and the phenomena that is being modeled; and</p> </li> <li> <p>relevant by calling on domain experts to help the team understand, assess and use the most appropriate sources and types of data to serve the project's objectives.</p> </li> </ul> <p>B) Design fairness:</p> <p>It needs to be appropriately shown the AI model's architectures do not include target variables, features,processes, or analytical structures (correlations, interactions, and inferences) which are unreasonable or unjustifiable. </p> <p>This may include showing that the following has been done:</p> <ul> <li> <p>Any underlying structural biases that may play a role in translating the objectives into target variables and measurable proxies have been identified. When defining the problem at the start of the AI project, these biases could influence what system designers expect target variables to measure and what they statistically represent.</p> </li> <li> <p>Biases in the data pre-processing phase have been mitigated by taking into account the sector or organisational context in which the AI system is being operated. When this process is automated or outsourced, one should be able to show that what was done has been reviewed and that oversight was maintained throughout. </p> </li> <li> <p>Information on the context of the metadata should also be included, so that those coming to the pre-processed data later on have access to the relevant properties when they undertake bias mitigation.</p> </li> <li> <p>Mitigated bias when the feature space was determined (i.e., when relevant features were selected as input variables for your model). Choices made about grouping or separating and including or excluding features, as well as more general judgements about the comprehensiveness or coarseness of the total set of features, may have consequences for protected groups of people.</p> </li> <li> <p>Mitigated bias when tuning parameters and setting metrics at the modelling, testing and evaluation stages (i.e., into the trained model). The AI development team should iterate the model and peer review it to help ensure that how they choose to adjust the dials and metrics of the model are in line with your objectives of mitigating bias.</p> </li> <li> <p>Mitigated bias by watching for hidden proxies for discriminatory features in the trained model, as these may act as influences on a model\u2019s output. Designers should also look into whether the significant correlations and inferences determined by the model\u2019s learning mechanisms are justifiable.</p> </li> </ul> <p>C) Metric-based fairness:  This is about making sure that the model does not have discriminatory or inequitable impact on the lives of the people it affects. This may include showing that:</p> <ul> <li> <p>The formal definition(s) of fairness that has been chosen is made explicit, as well as the reason why this decision. Data scientists can apply different formalised fairness criteria to choose how specific groups in a selected set will receive benefits in comparison to others in the same set, or how the accuracy or precision of the model will be distributed among subgroups; and</p> </li> <li> <p>the method applied in operationalising the formalised fairness criteria, for example, by re-weighting model parameters; embedding trade-offs in a classification procedure; or re-tooling algorithmic results to adjust for outcome preferences.</p> </li> </ul> <p>D) Implementation fairness:  The AI system is deployed by users sufficiently trained to implement it responsibly and without bias. This may include showing that implementers of the AI system have been appropriately prepared and trained to:</p> <ul> <li> <p>avoid automation bias (over-relying on the outputs of AI systems) or automation-distrust bias (under-relying on AI system outputs because of a lack of trust in them);</p> </li> <li> <p>use its results with an active awareness of the specific context in which they are being applied. They should understand the particular circumstances of the individual to which that output is being applied; and</p> </li> <li> <p>understand the limitations of the system. This includes understanding the statistical uncertainty associated with the result as well as the relevant error rates and performance metrics.</p> </li> </ul> <p>What information goes into fairness explanations</p> <p>This explanation is about providing people with appropriately simplified and concise information on the considerations, measures and testing you carry out to make sure that your AI system is equitable and that bias has been optimally mitigated. Fairness considerations come into play through the whole lifecycle of an AI model, from inception to deployment, monitoring and review.</p> <p>Process-based explanations include:</p> <ul> <li> <p>the chosen measures to mitigate risks of bias and discrimination at the data collection, preparation, model design and testing stages;</p> </li> <li> <p>how these measures were chosen and how managed informational barriers to bias-aware design such as limited access to data about protected or sensitive traits of concern have been managed; and</p> </li> <li> <p>the results of the initial (and ongoing) fairness testing, self-assessment, and external validation \u2013 showing that the chosen fairness measures are deliberately and effectively being integrated into model design. This can be done by showing that different groups of people receive similar outcomes, or that protected characteristics have not played a factor in the results.</p> </li> </ul> <p>Outcome-based explanations include:</p> <ul> <li> <p>details about how the formal fairness criteria were implemented in the case of a particular decision or output;</p> </li> <li> <p>presentation of the relevant fairness metrics and performance measurements in the delivery interface of your model. This should be geared to a non-technical audience and done in an easily understandable way; and</p> </li> <li> <p>explanations of how others similar to the individual were treated (i.e., whether they received the same decision outcome as the individual). For example, information generated from counter-factual scenarios could be used to show whether or not someone with similar characteristics, but of a different ethnicity or gender, would receive the same decision outcome as the individual.</p> </li> </ul>"},{"location":"skills-tracks/aeg/chapter5/transparency/#safety-and-performance-explanation","title":"Safety and performance explanation","text":"<p>What does this explanation help people understand? The safety and performance explanation helps people understand the measures that have been put in place, and the steps that have been taken (and are continuously being taken) to maximise the accuracy, reliability, security and robustness of the decisions the AI model helps make. It can also be used to justify the type of AI system chosen, such as comparisons to other systems or human decision makers.</p> <p>Key concepts:</p> <pre><code>**Accuracy:** the proportion of examples for which  model generates a correct output. This component may also include other related performance measures such as precision, sensitivity (true positives), and specificity (true negatives). Individuals may want to understand how accurate, precise, and sensitive the output was in their particular case.\n\n**Reliability:** how dependably the AI system does what it was intended to do. If it did not do what it was programmed to carry out, individuals may want to know why, and whether this happened in the process of producing the decision that affected them.\n\n**Security:** the system is able to protect its architecture from unauthorised modification or damage of any of its component parts; the system remains continuously functional and accessible to its authorised users and keeps confidential and private information secure, even under hostile or adversarial conditions.\n\n**Robustness:** the system functions reliably and accurately in practice. Individuals may want to know how well the system works if things go wrong, how this has been anticipated and tested, and how the system has been immunised from adversarial attacks.\n</code></pre> <p>What information goes into safety and performance explanations</p> <p>Process-based explanations include:</p> <p>For accuracy:</p> <ul> <li>How is accuracy measured (e.g., maximising precision to reduce the risk of false negatives).</li> <li>Why those measures were chosen, and what the assurance process behind it was.</li> <li> <p>What was done at the data collection stage to ensure that the training data was up-to-date and reflective of the characteristics of the people to whom the results apply.</p> </li> <li> <p>What kinds of external validation has undertaken to test and confirm your model\u2019s \u2018ground truth\u2019.</p> </li> <li> <p>What the overall accuracy rate of the system was at testing stage.</p> </li> <li> <p>What is done to monitor this (e.g., measuring for concept drift over time).</p> </li> </ul> <p>For reliability:</p> <ul> <li> <p>How it is measured and what the assurance process behind it is.</p> </li> <li> <p>Results of the formal verification of the system\u2019s programming specifications, i.e., how encoded requirements have been mathematically verified.</p> </li> </ul> <p>For security:  - How it is measured and what the assurance process behind it is, e.g., how limitation have been set on who is able to access the system, when, and how.</p> <ul> <li>How the security of confidential and private information that is processed in the model has been managed.</li> </ul> <p>For robustness:</p> <ul> <li> <p>How it is measured.</p> </li> <li> <p>Why the specific measures were chosen.</p> </li> <li> <p>What the assurance process behind it is, e.g., how the system has been stress-tested to understand how it responds to adversarial intervention, implementer error, or skewed goal-execution by an automated learner (in reinforcement learning applications).</p> </li> </ul> <p>Outcome-based explanations:</p> <p>While one might not be able to guarantee accuracy at an individual level, one should be able to provide assurance that, at run-time, an AI system operated reliably, securely, and robustly for a specific decision.</p> <ul> <li> <p>In the case of accuracy and the other performance metrics, however the results of cross-validation (training/ testing splits) and any external validation carried out, should be included in the model's delivery interface.</p> </li> <li> <p>Other relevant information that could be included is:  information related to the system\u2019s confusion matrix (the table that provides the range of performance metrics) and ROC curve (receiver operating characteristics)/ AUC (area under the curve). Include guidance for users and affected individuals that makes the meaning of these measurement methods, and specifically the ones that have been chosen, easily accessible and understandable. This should also include a clear representation of the uncertainty of the results (e.g., confidence intervals and error bars).</p> </li> </ul>"},{"location":"skills-tracks/aeg/chapter5/transparency/#impact-explanation","title":"Impact explanation","text":"<p>What does this explanation help people understand?</p> <p>An impact explanation helps people understand how the effects that an AI decision-support system may have on an individual, i.e., what the outcome of the decision means for them, have been considered. </p> <p>It is also about helping individuals to understand the broader societal effects that the use of this AI system may have. This can help reassure people that the use of AI will be of benefit. Impact explanations are therefore often well suited to delivery before an AI-assisted decision has been made. </p> <p>What does this type of explanation include?</p> <p>Demonstrate that thought has been put into how an AI system will potentially affect individuals and wider society. It is important to clearly show affected individuals the process done to determine these possible impacts.</p> <p>What information goes into impact explanations</p> <p>Process-based explanations include:</p> <ul> <li> <p>Showing the considerations given to the AI system\u2019s potential effects, how these considerations were undertaken, and the measures and steps taken to mitigate possible negative impacts on society, and to amplify the positive effects.</p> </li> <li> <p>Information on the plan in place to monitor and re-assess impacts while the system is deployed should also be included.</p> </li> </ul> <p>Outcome-based explanations:</p> <p>Although impact explanations are mainly about demonstrating that appropriate forethought has been given into the potential \u2018big picture\u2019 effects, it is also important to consider how to help recipients understand the impact of the AI-assisted decisions that specifically affect them. For instance, one might explain the consequences for the individual of the different possible decision outcomes and how, in some cases, changes in their behaviour would have brought about a different outcome with more positive impacts. This use of counterfactual assessment would help decision recipients make changes that could lead to a different outcome in the future or allow them to challenge the decision.</p>"},{"location":"skills-tracks/aeg/chapter5/transparency/#putting-the-principle-of-transparency-and-explainability-into-practice","title":"Putting the Principle of Transparency and Explainability into Practice","text":"<p>Success</p> <ul> <li>Task 1 (Project Planning): Select priority explanations by considering the domain, use case and potential impacts</li> <li>Task 2 (Data Extraction or Procurement, Data Analysis): Collect and pre-process data in an explanation-aware manner</li> <li>Task 3 (Model Selection): Build a system that is able to extract relevant information for a range of explanation types</li> <li>Task 4 (Model Reporting): Translate the rationale of the system\u2019s results into useable and easily understandable reasons</li> <li>Task 5: (User Training) Prepare implementers to deploy the AI system</li> <li>Task 6 (Model Reporting): Consider how to build and present an explanation</li> </ul> <p>There are a number of tasks both to help in the design and deployment of appropriately transparent and explainable AI systems and to assist in providing clarification of the results these systems produce to a range of impacted stakeholders (from operators, implementers, and auditors to decision recipients). These tasks make up Transparency and Explainability Assurance Management for AI projects, offering a systematic approach to:</p> <ul> <li>Designing, developing, and deploying AI projects in a transparent and explanation-aware fashion; and selecting, extracting and delivering explanations that are differentiated according to the needs and skills of the different audiences they are directed at.</li> </ul> <p>Task 1 (Project Planning): Select priority explanations by considering the domain, use case and potential impacts</p> <ul> <li> <p>Understanding the different types of explanation will serve to identify the dimensions of an explanation that decision recipients will find useful.  In most cases, explaining AI-assisted decisions involves identifying what is happening in the AI system and who is responsible. That means prioritising the rationale and responsibility explanation types.</p> </li> <li> <p>The setting and the sector are important in figuring out what kinds of explanation one should be able to provide. Therefore considering the domain context and use case is crucial to prioritise which explanations the team should be prepared to give.</p> </li> <li> <p>In addition, consider the potential impacts of the particular use of the AI system to determine which other types of explanation should be provided. This will also help in thinking about how much information is required, and how comprehensive it should be.</p> </li> <li> <p>Choosing what to prioritise is not an exact science. Hopefully the explanations prioritised will coincide with what the majority of the people impacted want to know, but it is unlikely that every individual will have all their questions answered. Having a clear and documented rationale for the explanations prioritised will probably also be useful for your own accountability or auditing purposes.</p> </li> </ul> <p>Task 2 (Data Extraction or Procurement, Data Analysis): Collect and pre-process data in an explanation-aware manner</p> <p>The data collected and pre-processed before inputting it into the system has an important role to play in the ability to derive each explanation type. Careful labelling and selection of input data can help provide information for your rationale explanation.</p> <p>Providing details about who is responsible at each stage of data collection and pre-processing is part of being more transparent. This is part of the responsibility explanation (information can be drawn from Workflow Governance Map, covered in the AI accountability section of this course).</p> <p>Drawing from the dataset factsheet can aid in providing data explanations, including the following information:</p> <ul> <li>the source of the training data;</li> <li>how it was collected;</li> <li>assessments about its quality; and</li> <li>steps taken to address quality issues, such as completing or removing data</li> </ul> <p>Check the data used within the model to ensure it is sufficiently representative of those it is making decisions about. Another issue to consider is whether pre-processing techniques, such as re-weighting, are required.  These decisions should be documented in the Bias Self-Assessments, and will help construct the fairness explanation.</p> <p>Task 3 (Model Selection): Build a system that is able to extract relevant information for a range of explanation types</p> <p>Deriving the rationale explanation is key to understanding an AI system (as well as complying with parts of the GDPR). It requires looking \u2018under the hood\u2019 and helps in gathering the information needed for some of the other explanations, such as safety and fairness. </p> <p>However, this is a complex task that requires knowing when to use more and less interpretable models and how to understand their outputs. To choose the right AI model for the particular explanation needs, one should think about the domain the system will be working in, and the potential impact of the system.</p> <p>When selecting a model for an AI project, it is important to consider whether:</p> <ul> <li>there are costs and benefits of using a newer and potentially less explainable AI model;</li> <li>the data used requires a more or less explainable system;</li> <li>the use case and domain context encourage choosing an inherently interpretable system; </li> <li>the processing needs lead to the selection of a \u2018black box\u2019 model; and</li> <li>the supplementary interpretability tools that help  explain a \u2018black box\u2019 model (if chosen) are appropriate given the context.</li> </ul> <p>To extract explanations from inherently interpretable models, look at the logic of the model\u2019s mapping function by exploring it and its results directly. On the other hand, there are many techniques used to extract explanations from \u2018black box\u2019 systems. Make sure that they provide a reliable and accurate representation of the system\u2019s behaviour.</p> <p>Task 4 (Model Reporting): Translate the rationale of the system\u2019s results into useable and easily understandable reasons</p> <p>Once the rationale of the underlying logic of the AI model has been extracted, the statistical output needs to be incorporated into the wider decision-making process.</p> <p>Implementers of the outputs from the AI system will need to recognise the factors that they see as legitimate determinants of the outcome they are considering.</p> <p>Decision recipients should be able to easily understand how the statistical result has been applied to their particular case.</p> <p>Task 5: (User Training) Prepare implementers to deploy the AI system</p> <p>In cases where decisions are not fully automated, implementers need to be meaningfully involved.</p> <p>This means that they need to be appropriately trained to use the model\u2019s results responsibly and fairly.</p> <p>Their training should cover: - the basics of how machine learning works; - the limitations of AI and automated decision-support technologies; - the benefits and risks of deploying these systems to assist decision-making, particularly how they help humans come to judgements rather than replacing that judgement; and - how to manage cognitive biases, including both decision-automation bias and automation-distrust bias.</p> <p>Task 6 (Model Reporting): Consider how to build and present an explanation Gathering together the information gained when implementing Tasks 1-4 is the first step towards building an explanation. This includes reviewing the information and determine how this provides an evidence base for the process-based or outcome-based explanations.</p> <p>Additionally, it is important to revisit the contextual factors to establish which explanation types should be prioritised.</p> <p>The way an explanation is presented depends on the way AI-assisted decisions are made, and on how people might expect those responsible for the AI system to deliver explanations without using AI.</p> <p>Explanations can la 'layered' by proactively providing individuals with the prioritised explanations first and making additional explanations available in further layers. This helps to avoid information (or explanation) overload.</p> <p>Delivering explanations should be thought of as a conversation, rather than a one-way process. People should be able to discuss a decision with a competent human being.</p> <p>Providing an explanation at the right time is also important. Proactively engaging with customers by making information available on how the AI system is used and how it aids in making decisions, can increase the trust and awareness.</p>"},{"location":"skills-tracks/dj/","title":"About this Course","text":"<p>In this course we explore the emerging movement of data justice, which seeks to apply a social justice-oriented approach to examining the range of social, political, and material concerns arising within our increasingly datafied society. </p> <p>The course begins with an introduction to data justice through a brief overview of the movement\u2019s history and investigates how its efforts can be advanced in its second wave. It then follows with the six pillars of data justice, which serve as guiding priorities for reorienting data justice research and practice. Finally, the course examines instances of data justice in action through a range of grassroots initiatives that are being undertaken by impacted communities across the world. You will be hearing directly from individuals and communities that have been impacted by unequitable data practices and learn more about how they have been mobilising to challenge these through a three-part documentary series.</p> <p>This chapter is guided by the work undertaken by the Advancing Data Justice Research and Practice (ADJRP) project and its current understandings of data justice through inclusive and intercultural approaches.</p> <p>Course Information</p> <ul> <li>Title: Data Justice Research and Practice</li> <li>Course Leaders: Semeli Hadjiloizou, Andr\u00e9s Dominguez and Ann Borda</li> <li>Last updated: October 2024</li> <li>Status: In Development</li> <li>Citation Information: Hadjiloizou, S., Dominguez, A., and Borda. A., (2024) Data Justice Research and Practice. Turing Commons (Alan Turing Institute). https://alan-turing-institute.github.io/turing-commons/skills-tracks/dj/ </li> </ul>"},{"location":"skills-tracks/dj/#table-of-contents","title":"Table of Contents","text":"<ul> <li> <p> Introduction to Data Justice</p> <p>This module introduces the concept and practice of \u2018data justice\u2019 through a brief review of the movement\u2019s history so far and how its efforts can be advanced.</p> <p> Go to module</p> </li> <li> <p> The Six Pillars of Data Justice</p> <p>This module introduces the six pillars of data justice: power, equity, access, identity, participation, and knowledge. Each pillar is explored through its own dedicated subchapter and through a set of guiding considerations that can be used as a framework for furthering the data justice movement.</p> <p> Go to module</p> </li> <li> <p> Data Justice in Action</p> <p>This chapter dives into the grassroots initiatives undertaken by impacted communities across the world, namely through the ADJRP project\u2019s three-part data justice documentary series.</p> <p> Go to module</p> </li> </ul>"},{"location":"skills-tracks/dj/#who-is-this-course-for","title":"Who is this course for?","text":"<p>Primarily, this course is aimed for researchers and practitioners with an active interest in understanding and challenging the range of social, political, and material concerns arising within our increasingly datafied society. This doesn't mean you have to be a data scientist or developer of machine learning algorithms. You could also be an ethicist, sociologist, social justice activist, or someone with an interest in law and public policy.</p>"},{"location":"skills-tracks/dj/#learning-objectives","title":"Learning Objectives","text":"<p>In this course you will: </p> <ul> <li>familiarise yourself with the concept, \u2018data justice\u2019 in the context of data and data-intensive technologies; </li> <li>learn about the history of the data justice movement so far; and </li> <li>explore how the second wave of data justice seeks to advance research and practice through inclusive and intercultural approaches.  </li> </ul> <p>Key Terms</p> <p>As you complete this module, you will come across several key terms that may be unfamiliar to you. We have provided definitions for these below.</p> <ul> <li>Datafication: Refers to a process of encoding or representing various aspects of our lives and the world around us into quantifiable data. This process can be applied to people, objects, and other social or natural phenomena (e.g., economies, voting patterns, ecologies).</li> <li>Datafied: Refers to something that has undergone the process of datafiction.</li> <li>Data justice: This term helps to build a critical understanding of the ethical challenges posed by an increasing presence and use of digital data in society. Data justice contextualises such issues within wider social justice concerns.</li> <li>Sociotechnical: Refers to the notion that all technologies are socially shaped \u2013their design, operation, and use are always shaped by human values and culture. The term is commonly used to emphasise interactions between people, technology, and the wider organisational or societal contexts in which they are situated (e.g., companies or institutions).</li> <li>Global majority: Refers to people of African, Asian, Indigenous, Latin American, and other racial or ethnic groups who constitute about 85% of the world's population. This collective term challenges the racialisation and othering of \u2018ethnic minorities\u2019 by emphasising the signifiance of these broad communities in representing the majority of the global population. It is often used in place of other terms that are deemed outdated or problematic, such as BAME (Black, Asian, and Minority Ethnic) or BIPOC (Black, Indigenous, and People of Colour).</li> <li>Intersectional: Refers to the interconnectedness of social categories, such as race, class, sexual orientation, gender, etc. More specifically, it is used to emphasise where a person or group is more likely to experience comparative disadvantage by virtue of being a member in multiple overlapping social categories.</li> <li>Social justice: Social justice is a commitment to the achievement of a society that is equitable, fair, and capable of confronting the root causes of injustice. In an equitable and fair society, all individuals are recognised as worthy of equal moral standing and are able to realise the full assemblage of fundamental rights, opportunities, and positions.</li> <li>Technocentrism: Refers to a value system centred heavily on technology and the belief that technology is the key driving force of change in society, also known as technological determinism.</li> </ul>"},{"location":"skills-tracks/dj/dj-100-1/","title":"What is data justice?","text":"<p> \u2018Impacted Communities\u2019 illustration by Johnny Lighthands, Creative Commons Attribution-ShareAlike 4.0 International</p> <p>Data-intensive technologies are increasingly deployed and used for diverse applications across domains, such as healthcare, policing, and education. Although such technological advances may offer various opportunities, there is a growing body of research and practice that highlights how the proliferation of data-intensive technologies exacerbate longstanding social inequities, or even contribute to the generation of new ones.</p> <p>As with any sociotechnical phenomenon, data-intensive technologies are neither neutral nor apolitical. They come into being through a mixture of human values, behaviours, and decisions of the creators of such technologies<sup>1</sup>. Researchers have studied the role of social structures within and around data-intensive technologies across intersecting social dimensions, including class, race, and gender. They have shown that algorithms and systems of classification are necessarily shaped by historical patterns like socio-economic, racial, and gender disparities in technical professions, and other manifestations of discrimination in society, and that they could reinforce such patterns of inequality<sup>2</sup>.</p> <p>Illustrative example: Facial recognition technologies</p> <p>The ways in which data is collected, processed, and used can have significant impacts on the outcome of the system whether it is assisting with the provision of social services or determining what videos you may want to watch based on your past viewing history. If data about certain groups are scarce, incomplete, or missing, this could have significant impacts on the overall output of a model. </p> <p>To illustrate the point, we can use the example of facial recognition technologies which are trained to recognise faces of individuals. In an example illustrated by Joy Buolamwini and Timnit Gebru<sup>3</sup>, a facial recognition classifier performed the worst on female faces with darker skin due to the underrepresentation of females with darker skin and individuals with darker skin in general in the datasets.</p> <p>Due to the lack of representation of women and darker-skinned women in the datasets, the classifier was more likely to fail to recognise their faces, leading to potential harms like wrongful arrests and negative stereotyping which could in turn reinforce historical patterns of discrimination towards these marginalised groups. In this instance, the dataset, often called the training set (a dataset used to train the model on past historical patterns) was unrepresentative and therefore led to harmful impacts. This illustrates how the ways in which this data is collected and the information it contains is critical and has real-world impacts on those for whom the outcome of the model is intended for.</p> <p>In response to such injustices reflected in data, there has been a growing movement of researchers, practitioners, and civil society groups seeking to address, challenge, and reimagine current practices of datafication. Data justice has emerged as a framework to characterise the multifaceted efforts to identify and enact ethical paths to social justice in an increasingly datafied world <sup>4</sup>.</p> <p>For a quick recap of the emerging movement of data justice, take a look at the short infographic video below.</p> <ol> <li> <p>Winner, Langdon. 1980. \u2018Do Artifacts Have Politics?\u2019 Daedalus 109(1): 121\u201336.\u00a0\u21a9</p> </li> <li> <p>Eubanks, V. (2018). Automating inequality: How high-tech tools profile, police, and punish the poor. St. Martin\u2019s Press; Benjamin, R. (2019). Race after technology: Abolitionist tools for the new Jim code. Polity Books; D\u2019Ignazio, C., &amp; Klein, L. F. (2020). Data feminism. MIT Press.\u00a0\u21a9</p> </li> <li> <p>Buolamwini, J., &amp; Gebru, T. (2018, January). Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency (pp. 77-91). PMLR. https://proceedings.mlr.press/v81/buolamwini18a.html \u21a9</p> </li> <li> <p>Taylor, L. (2017). What is data justice? The case for connecting digital rights and freedoms globally. Big Data &amp; Society, July-December, 1-14. https://doi.org/10.1177/2053951717736335 \u21a9</p> </li> </ol>"},{"location":"skills-tracks/dj/dj-100-2/","title":"A brief history of data justice literature","text":"<p>Before the emergence of a dedicated body of literature on the concept of data justice, responses to the increasing datafication of society tended to focus primarily on issues of data protection, privacy, and security <sup>1</sup>.</p> <p>The first wave of data justice scholarship\u2014emerging in the pathbreaking work undertaken by the Data Justice Lab at Cardiff University and the Global Data Justice project at the Tilburg Institute for Law, Technology, and Society\u2014sought to move beyond this limited view by situating the ethical challenges posed by datafication in the wider context of social justice concerns. This initial data justice research sought to be more responsive to the real-world conditions of power asymmetries, discrimination, and exploitation that have come to define the \u201cdata-society nexus\u201d <sup>2</sup>.</p> <p>Data-society nexus</p> <p>In \u201cExploring data justice: Conceptions, applications, and directions\u201d, Dencik, Hintz, Redden, and Trer\u00e9 describe:</p> <p>\u201cThis shifts the focus of the data-society nexus away from simple binaries that frame the debate in terms of trade-offs or \u2018good\u2019 vs. \u2018bad\u2019 data in which data is an abstract technical artefact. Instead, data is seen as something that is situated and necessarily understood in relation to other social practices\u201d (2019, p. 873).</p> <p>This first wave of data justice approached critical ethical questions primarily through a focus on surveillance, information capitalism, and the political economy of data. This focus on the political and economic forces surrounding datafication<sup>3</sup>, however, was less concerned with the underlying sources of data injustice linked with deeper socially, culturally, and historically entrenched structures of domination.  Further to this, it has been noted that much of the academic discourse around data-intensive technologies has been dominated by global north perspectives, interests, and values<sup>4</sup>.  Approaches to data justice have yet a long way to go in incorporating and engaging with global majority visions of ethical and just ways of working, being, and interconnecting with people and the planet that are rooted in non-Western belief systems.</p> <p>The agenda of data justice aspires to encompass a sufficiently broad reach that recognises the plurality of ways of being and the living contexts of all individuals and communities impacted by datafication and digital infrastructures globally. For this reason, the inclusion of non-Western knowledges, world views and values that might shape possible data governance futures is a crucial precondition of advancing data justice research and practice.</p> <p>From Shmyla Khan, Digital Rights Foundation</p> <p>Migrants and refugees are inherently vulnerable and precarious bodies, often occupying a liminal space within the imagination of body politic as well as the state. </p> <p>Speaking from the experience of Pakistan, surveillance, datafication, and exclusion of these bodies has been central to the nation-building process. Dealing with several waves of migrants, first after partition from British India and then the influx of migrant populations from newly independent Bangladesh provide good insight into the post-colonial national-building process. In the first wave it was integral to the nation that Muslims coming from across the newly-imposed Indian border be absorbed within the country, Pakistan Citizenship Act,1952 provides an expansive definition of who can claim to be a citizen. However, we see state practice change with the influx of migrants and displaced persons after the 1971 war, as Bihari migrants flowed in from Bangladesh. Many of these migrants still lack official citizenship and documentation despite having a strong claim of citizenship. Many of them are concentrated in informal settlements, with their families denied national identification to this day in 2022. They repeatedly face issues with registration into the National Database and Registration Authority (NADRA), unable to become data subjects in the eyes of the state.</p> <p>The third wave of migration in the country has been refugees from across the border with Afghanistan in wake of the Soviet invasion in the 1980s and has continued with the rule of the Taliban and US invasion. These refugees have been systemically denied citizenship, even when next generations have laid claim to legal birth right citizenship. However, the state has sought to look at these bodies from the prism of national security and surveillance--biometric Proof of Registration (PoR) cards are issued to refugees by NADRA. Despite being datafied, these bodies are still looked upon with suspicion\u2014there are regular purging drives by NADRA to cancel registration of registration of documentation for refugees or anyone suspected of being Afghan. These bodies are coded as security risks, their informal settlements often razed to the ground on flimsy suspicions of crime -- always existing in that liminal space despite registration and datafication.</p> <ol> <li> <p>Leslie, D., Katell, M., Aitken, M., Singh, J., Briggs, M., Powell, R., Rincon, C., Chengeta, T., Birhane, A., Perini, A., Jayadeva, S., &amp; Mazumder, A., (2022). Advancing data justice research and practice: An integrated literature review. http://dx.doi.org/10.2139/ssrn.4073376 \u21a9</p> </li> <li> <p>Dencik, L., Hintz, A., Redden, J., &amp; Trer\u00e9, E. (2019). Exploring data justice: Conceptions, applications, and directions. Information, Communication &amp; Society, 22(7), 873-881. https://doi.org/10.1080/1369118X.2019.1606268 \u21a9</p> </li> <li> <p>Dencik, L., Hintz, A., &amp; Cable, J. (2016). Towards data justice? The ambiguity of anti-surveillance resistance in political activism. Big Data &amp; Society, 3(2), https://doi.org/10.1177/2053951716679678 \u21a9</p> </li> <li> <p>Aggarwal, N. (2020). Introduction to the special issue on intercultural digital ethics. Philosophy &amp; Technology, 33(4), 547-550. https://doi.org/10.1007/s13347-020-00428-1; Mhlambi, S. (2020). From rationality to relationality: Ubuntu as an ethical and human rights framework for artificial intelligence governance. Carr Center Discussion Paper Series, 2020(009). https://carrcenter.hks.harvard.edu/files/cchr/files/ccdp_2020-009_sabelo_b.pdf; Birhane, A. (2021). Algorithmic injustice: a relational ethics approach. Patterns, 2(2), 100205. https://doi.org/10.1016/j.patter.2021.100205 \u21a9</p> </li> </ol>"},{"location":"skills-tracks/dj/dj-100-3/","title":"Widening approaches to data justice research and practice","text":"<p>A second wave of data justice scholarship sought to widen the lens of data justice through intercultural and inclusive approaches. For instance, Global majority scholars have begun to centre relational notions of personhood and community arising across non-Western systems of belief ranging from Ubuntu<sup>1</sup>, Buddhism<sup>2</sup>, and Confucianism<sup>3</sup>, to various expressions of Indigenous values.</p> <p>Relational notions of personhood and community</p> <p>In \u201cAlgorithmic injustice: a relational ethics approach\u201d, Birhane explains: </p> <p>\u201cContrary to the rationalist and individualist worldview, relational perspectives view existence as fundamentally co-existent in a web of relations. Various schools of thought can be grouped under the umbrella of the relational framework with a core commonality of interdependence, relationships, and connectedness. Relational-centered approaches include Black feminist (Afro-feminist) epistemologies, embodied and enactive approaches to cognitive science, Bakhtinian dialogism, ubuntu (the sub-Saharan African philosophy), and complexity science\u201d (2021, p. 3).</p> <p>Launched in 2021, the Advancing Data Justice Research and Practice (ADJRP) project was motivated to contribute to such an agenda. The project was a collaboration between the Global Partnership on AI (GPAI), The Alan Turing Institute, 12 policy pilot partners, and participants and communities across the globe. It aimed to offer practical guidance and conceptual framings for expanding understandings of data justice and has led to diverse research outputs, including a three-part documentary series, an in-depth literature review, a repository of case studies, three Data Justice in Practice Guides, and more. The content of this module is adapted from and builds on the contributions of the ADJRP project.</p> <p>The following short infographic video gives a brief overview of the ADJRP project.</p> <p>The ADJRP project was guided by its effort to reorient data justice research and practice both critically and constructively. It sought to pave an alternative path to data justice through open and inclusive processes that centre global majority perspectives. To tackle the reorientation of data justice, the ADJRP project explores the three axes of where, when, and who.</p>"},{"location":"skills-tracks/dj/dj-100-3/#where","title":"Where","text":"<p>Broadening the where of data justice does not simply mean a shift in geospatial focus from the global minority to the global majority. Rather, it shifts attention to the conditions of data injustice that exist between high-income countries or regions and low-and-middle income countries or regions as well as within them. This approach underlines how patterns of inequality and discrimination that affect Indigenous, marginalised, and vulnerable social groups within low-and-middle income countries or regions are likely only to be magnified by the wider inequalities that exist between these countries or regions and wealthier ones. </p> <p>Reorienting the where of data justice, therefore, means recognising body and territorial relationships at both domestic and global levels. This reorientation seeks to enable more robust interactions between a multiplicity of voices, experiences, and frameworks as well as foster greater cross-fertilisation of insights from divergent sites of experience.</p> <p>Illustrative example: Bhoomi \u2013 Land records management system in India</p> <p>Bhoomi is a digital registry of \u2018rights, tenancy, and crops\u2019 produced by the state government of Karnataka, India. The registry is part of an open data effort to increase both the uniformity and availability of official records including land-ownership records. While initially developed by the department of revenue for taxation purposes, the information can be viewed publicly online and at internet kiosks. It is reportedly used extensively by real estate developers.</p> <p>Critics have argued that the Bhoomi registry has disenfranchised members of the Dalit caste, considered to be the lowest class in the social hierarchy system of Hinduism<sup>4</sup>, whose claims are often not documented in official records but are well supported by other means. Research has found that Dalits face discrimination in Indian society and often live in poverty<sup>5</sup>. They nevertheless have longstanding claims to land. However, the informal and historical knowledge that supports these claims cannot be easily accommodated in the flattened landscape of a relational database, such as the Bhoomi registry, and so are more easily dismissed or overruled.</p> <p>Furthermore, Bhoomi may be an example of \u2018open data under conditions of unequal capabilities\u2019<sup>6</sup>. Like many digital resources, the Bhoomi registry is more likely to be accessible to people with computational and interpretive skills, who are also more likely to hold greater social and political power in society. In Karnataka, there have been mass evictions of Dalits and others in urban slums that were deemed desirable for redevelopment and in which the ability to present conflicting ownership claims based in local knowledge was diminished by the Bhoomi registry.</p>"},{"location":"skills-tracks/dj/dj-100-3/#when","title":"When","text":"<p>The dimension of when highlights how temporal understandings of data injustice must not be limited to only the past few decades of datafication. Instead, it is important that understandings of data (in)justice capture longer-term historical patterns. This entails avoiding the short-sighted and deceptive notion that frames technological development as somehow \u2018unprecedented\u2019 or \u2018disruptive\u2019, which contributes to obscuring important continuities throughout the past, present, and future of innovation. </p> <p>Reorienting the when of data justice aims to situate contemporary occurrences of data risks and harms within broader, ongoing histories of discrimination, inequality, and oppression.</p> <p>Illustrative example: Algorithmic profiling in lending practices, United States</p> <p>Corporate and financial surveillance has extracted and amassed large volumes of data that may be used to economically and racially profile individuals through discriminatory pricing practices. Not only have advertisers made use of behavioural profiling to provide differential pricing for goods, but research also suggests that vulnerable populations are subject to predatory lending practices<sup>7</sup>. However, variations in lending practices that lead to disparate treatment of social groups predate the use of data-driven technology. For instance, Black and Hispanic borrowers have, in the past, been offered discriminatory sub-prime loans, rates, or excessive fees compared to those offered to equally qualified white borrowers.</p> <p>Regarded as \u2018reverse redlining\u2019, predatory subprime mortgage loans are targeted towards individuals identified as vulnerable, reversing the practice of redlining wherein goods are not made available to minority neighbourhoods<sup>8</sup>. Non-white, non-wealthy, and less-tech savvy individuals and communities are often disadvantaged through current credit-scoring systems. Subsequent late payments are accepted as objective indicators for credit scoring, despite originating from discriminatory practices. This then feeds forward into a loop that further impacts credit scores.</p> <p>The equity of impact is dependent on the prevalence of bias in the data and software. While bias in lending is illegal, proxy datasets\u2014available in abundance\u2014and biased correlations can be used in discriminatory practices. Similarly, although exploitative loans are banned or restricted, some lenders continue to operate through online platforms which have actively solicited their advertising. Moreover, when machine learning models lack interpretability, the reasons behind lending practices cannot adequately be presented.</p>"},{"location":"skills-tracks/dj/dj-100-3/#who","title":"Who","text":"<p>Reorienting the who of data justice research and practice entails centring the knowledge and lived experience of communities impacted by data injustices. The notion of data justice has largely been defined and led by academics situated in global minority institutions. When centring impacted communities, it is worth noting how activism, community organising, and advocacy work that aims to challenge data harms and advance social justice have a longstanding trajectory that precedes (and inspires) the establishment of the term \u2018data justice\u2019 itself. </p> <p>In this way, rethinking the who of data justice requires amplifying and taking the lead from the perspectives that are rooted in the lived experience of individuals and communities who are impacted by datafication---especially members of those groups which have been historically disempowered and marginalised.</p> <p>Illustrative example: amandla.mobi, South Africa</p> <p>amandla.mobi is an independent, community advocacy organisation run by Black women in South Africa that has developed a petition app for civil society to build support around specific public interest campaigns. Their goal is to mobilise the collective power of Black women from low-income backgrounds, hold political and corporate interests to account, and shift power in ways that laws, policies, norms, and values benefit low- income Black women and their communities. They do this by acting at critical moments in a targeted, coordinated, and strategical way.</p> <p>Their campaigns relate to a wide range of issues, such as gender-based violence, police brutality, and climate change, and address the power structures that maintain injustices. Campaigns are created considering what the community cares most about and the civic tools and tactics at their disposal. Petitions are then circulated on social media for supporters to sign. One of the campaigns run by amandla.mobi is entitled \u201cData Must Fall\u201d, which helped to increase access to mobile internet. amandla.mobi achieved this through submissions to the Independent Communications Authority of South Africa (ICASA), protests outside of the high court, amongst other actions, with the goal to reduce mobile networks discriminatory behaviour towards socio-economically marginalised groups, resulting in data prices dropping by 30% to 50%.</p> <ol> <li> <p>Birhane, A. (2021). Algorithmic injustice: a relational ethics approach. Patterns, 2(2), 100205. https://doi.org/10.1016/j.patter.2021.100205; Eze, M. O. (2008). What is African communitarianism? Against consensus as a regulative ideal. South African Journal of Philosophy= Suid-Afrikaanse Tydskrif vir Wysbegeerte, 27(4), 386-399. https://doi.org/10.4314/sajpem.v27i4.31526; Gyekye, K. (1992). Person and community in Akan thought. In Wiredu, K., &amp; Gyekye, K. (Eds.), Person and community: Ghanaian philosophical studies. (pp. 101-122). Council for Research in Values and Philosophy; Kalumba, K. M. (2020). A defense of Kwame Gyekye\u2019s moderate communitarianism. Philosophical Papers, 49(1), 137\u2013158. https://doi.org/10.1080/05568641.2019.1684840; Mbiti, J. S. (1970). Concepts of God in Africa. SPCK Publishing; Menkiti, I. A. (1984). Person and community in African traditional thought. In R. Wright (Ed.), African philosophy: An introduction (3<sup>rd</sup> edition, pp. 171-181). University Press of America; Mhlambi, S. (2020). From rationality to relationality: Ubuntu as an ethical and human rights framework for artificial intelligence governance. Carr Center Discussion Paper Series, 2020(009). https://carrcenter.hks.harvard.edu/files/cchr/files/ccdp_2020-009_sabelo_b.pdf; Ogunbanjo, G. A., &amp; van Bogaert, D. K. (2005). Communitarianism and communitarian bioethics. South African Family Practice, 47(10), 51-53. https://doi.org/10.1080/20786204.2005.10873305 \u21a9</p> </li> <li> <p>Hongladarom, S. (2016). A Buddhist theory of privacy. In A Buddhist theory of privacy (pp. 57-84). Springer. Hongladarom S., &amp; Ess, C. (Eds.). (2007). Information technology ethics: Cultural perspectives. IGI Global; Vallor, S. (2016). Technology and the virtues: A philosophical guide to a future worth wanting. Oxford University Press.\u00a0\u21a9</p> </li> <li> <p>Jing, S., &amp; Doorn, N. (2020). Engineers\u2019 moral responsibility: A Confucian perspective. Science and Engineering Ethics, 26(1), 233-253. https://doi.org/10.1007/s11948-019-00093-4; Wong, P. H. (2012). Dao, harmony and personhood: Towards a Confucian ethics of technology. Philosophy and Technology, 25(1), 67\u201386. https://doi.org/10.1007/s13347-011-0021-z; Wong, P. H., &amp; Wang, T. X. (Eds.). (2021). Harmonious technology: A Confucian ethics of technology. Routledge; Yu, E., &amp; Fan, R. (2007). A Confucian view of personhood and bioethics. Journal of Bioethical Inquiry, 4(3), 171- 179. https://doi.org/10.1007/s11673-007-9072-3 \u21a9</p> </li> <li> <p>Sankaran, S., Sekerdej, M., &amp; Von Hecker, U. (2017). The role of Indian caste identity and caste inconsistent norms on status representation. Frontiers in psychology, 8, 487. https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00487/full \u21a9</p> </li> <li> <p>Das, M. B., &amp; Mehta, S. K. (2012). Poverty and social exclusion in India. World Bank. http://hdl.handle.net/10986/26336 \u21a9</p> </li> <li> <p>Johnson, J. A. (2018). Toward information justice: Technology, politics, and policy for data in higher education administration (Vol. 33). Springer. https://link.springer.com/book/10.1007/978-3-319-70894-2 \u21a9</p> </li> <li> <p>Bartlett, R., Morse, A., Stanton, R., &amp; Wallace, N. (2018). Consumer-lending discrimination in the era of fintech. Unpublished working paper. University of California, Berkeley. https://lending-times.com/wp-content/uploads/2018/11/discrim.pdf \u21a9</p> </li> <li> <p>Gilman, S. (2021). Proliferating predation: Reverse redlining, the digital proliferation of inferior social welfare products, and how to stop it. Harvard Civil Rights- Civil Liberties Law Review (CR-CL), 56, 169.\u00a0\u21a9</p> </li> </ol>"},{"location":"skills-tracks/dj/dj-100-index/","title":"Introduction to Data Justice","text":"<p> \u2018Data Justice Pillars\u2019 illustration by Johnny Lighthands, Creative Commons Attribution-ShareAlike 4.0 International.</p>"},{"location":"skills-tracks/dj/dj-100-index/#chapter-outline","title":"Chapter Outline","text":"<ul> <li>What is data justice?</li> <li>A brief history of data justice literature</li> <li>Widening approaches to data justice research and practice</li> </ul> <p>Chapter Summary</p> <p>In this chapter we explore the emerging movement of data justice, which seeks to apply a social justice-oriented approach to examining the range of social, political, and material concerns arising within our increasingly datafied society.</p> <p>This chapter is guided by the work undertaken by the Advancing Data Justice Research and Practice (ADJRP) project and its current understandings of data justice through inclusive and intercultural approaches. </p> <p>Learning Objectives</p> <p>In this chapter you will: - familiarise yourself with the concept of \u2018data justice\u2019 in the context of data and data-intensive technologies; - learn about the history of the data justice movement so far; and - explore how the second wave of data justice seeks to advance research and practice through inclusive and intercultural approaches. </p>"},{"location":"skills-tracks/dj/dj-101-1/","title":"Power","text":"<p> \u2018Power' illustration by Johnny Lighthands, Creative Commons Attribution-ShareAlike 4.0 International.</p> <p>The first pillar, power, refers to the levels at which power operates and manifests in the collection, analysis, and use of data in the world. It provides a basis from which to examine power and to raise critical awareness of its importance, presence, and influence.</p> <p>This pillar guides the examination of power through three main considerations: interrogating and critiquing power, challenging power, and empowering people. The following sections explore each of these elements in detail, offering a framework for investigating the manifestation of power in the context of data and data-intensive technologies, such as AI.</p> <p>For a brief overview of the power pillar, take a look at the infographic video below.</p>"},{"location":"skills-tracks/dj/dj-101-1/#interrogate-and-critique-power","title":"Interrogate and critique power","text":"<p>Power dynamics are present and manifest in many different places through a variety of ways. It is therefore crucial to:  - understand the levels at which power operates in data innovation ecosystems; - understand how power manifests and materialises in data innovation ecosystems; and - question power asymmetries at their sources and raise critical awareness of their effects, presence, and influence.</p>"},{"location":"skills-tracks/dj/dj-101-1/#understand-the-levels-at-which-power-operates-in-data-innovation-ecosystems","title":"Understand the levels at which power operates in data innovation ecosystems","text":"<p>Identifying and reflecting on the levels at which power operates is critical for critiquing power dynamics present in data innovation ecosystems. The following diagram aims to guide this interrogation, by identifying the different levels of power operating in the collection and use of data in the world. </p> <p> Understanding the levels at which power operates in the collection and use of data.</p>"},{"location":"skills-tracks/dj/dj-101-1/#the-geopolitical-level","title":"The geopolitical level","text":"<p>For example, high-income nation-states and transnational corporate actors can control access to technological capabilities and pursue their own interests on the global stage. In doing this, they can exercise significant influence on which countries or regions are able to access and develop digital and data processing capacities. </p>"},{"location":"skills-tracks/dj/dj-101-1/#the-infrastructural-level","title":"The infrastructural level","text":"<p>For example, States and large corporations can decide which impacted communities, domestically and globally, are able to access the benefits of connectivity and data innovation, and they can control the provision of essential digital goods and services that directly affect the public interest. </p>"},{"location":"skills-tracks/dj/dj-101-1/#the-organisational-and-political-level","title":"The organisational and political level","text":"<p>For example, governments and companies can control data collection and use in intrusive and involuntary ways\u2014especially where the public have no choice but to utilise the services they provide or must work in the environments they manage and administer. </p>"},{"location":"skills-tracks/dj/dj-101-1/#the-policy-legal-and-regulatory-level","title":"The policy, legal, and regulatory level","text":"<p>For example, large international standards bodies, transnational corporations, trade associations, and nation states, can exercise disproportionate amounts of influence in setting international policies, standards, and regulation related to the governance of digital goods and services and data innovation. Large corporations can exercise their power to lobby governments to prevent regulatory oversight. </p>"},{"location":"skills-tracks/dj/dj-101-1/#the-cultural-level","title":"The cultural level","text":"<p>For example, power can operate through the way that large tech companies use relevance-ranking, popularity-sorting, and trend-predicting algorithms to sort users into different, and potentially polarising, digital publics or groups. </p>"},{"location":"skills-tracks/dj/dj-101-1/#the-psychological-level","title":"The psychological level","text":"<p>For example, tech companies can use algorithmically personalised services  to curate the desires of targeted data subjects. This can allow for the control or manipulation of consumer behaviour but also play an active and sometimes damaging role in identity formation, mental well-being, and personal development. </p> <p>Illustrative example: Homeland Card - Identity Document, Venezuela</p> <p>Venezuela\u2019s Homeland Card (Carnet de La Patria) is a national ID card that serves as a digital payment system and is used by the Venezuelan government to provide access to food, healthcare, pensions, and other social benefits to citizens. Citizens are incentivised to enrol in the card programme via rewards including bonuses, with over half of the population being enrolled in the card programme. </p> <p>The Homeland Card has been critiqued by activists who stress that it is being used as a surveillance tool aided by digital telecommunication corporations. It has been reported that the card is linked to databases storing cardholders\u2019 personal information including medical history, social media presence, residential addresses, and political party membership. The use of the Homeland card in the context of a humanitarian emergency where most citizens depend on benefits for survival has raised concerns for opponents that citizens\u2019 information is being used to exclude individuals from accessing vital services based on their behaviours and political affiliations and that the ID system is serving as a method of control and electoral coercion<sup>1</sup>.</p> <p>### Understand how power manifests and materialises in data innovation ecosystems</p> <p> How power manifests and materialises in the collection and use of data in the world.</p> <p>Power can surface in several different ways. The following diagram aims to guide this critique of the different ways power manifests and materialises in the collection and use of data in the world. </p>"},{"location":"skills-tracks/dj/dj-101-1/#decision-making-power","title":"Decision-making power","text":"<p>Here, an individual or organisational actor A has power over B to the extent that A can influence the preferences of B. Decision-making power is seen, for instance, in the way that government agencies collect and use data to build predictive risk models about citizens and data subjects or to allocate the provision of social services (and then act on the corresponding algorithmic outputs). </p>"},{"location":"skills-tracks/dj/dj-101-1/#ideological-power","title":"Ideological power","text":"<p>This kind of power is exercised where people\u2019s perceptions, understandings, and preferences are shaped by a system of ideas or beliefs in a way which leads them\u2014frequently against their own interests\u2014to accept or even welcome their place in the existing social order and power hierarchy. For example, the priorities of \u201cattention capture\u201d and \u201cscreen-time maximisation\u201d, that are pursued by certain social media and internet platforms, can groom users within the growing ecosystem of compulsion-forming reputational platforms to embrace the algorithmically manufactured comforts of life-logging, status-updating, and influencer-watching all while avoiding confrontation with realities of expanding inequality and social stagnation. </p>"},{"location":"skills-tracks/dj/dj-101-1/#normalising-power","title":"Normalising power","text":"<p>Normalising power manifests in the way that the ensemble of dominant knowledge structures, scientifically authoritative institutions, administrative techniques, and regulatory decisions work in tandem to maintain and \u2018make normal\u2019 the status quo of power relations. Where tools of data science and statistical expertise come to be used as techniques of knowledge production that claim to yield a scientific grasp on the inner states or properties of observed individuals, forms of normalising or disciplinary power can arise. Data subjects who are treated merely as objects of prediction or classification and who are therefore subjugated as objects of authoritative knowledge become sitting targets of disciplinary control and scientific management. </p>"},{"location":"skills-tracks/dj/dj-101-1/#agenda-setting-power","title":"Agenda-setting power","text":"<p>Here, an individual or organisational actor A has power over B to the extent that A sets the agenda that B then must fall in line with by virtue of A\u2019s control over the terms of engagement that set practical options within A\u2019s sphere of influence and interest. Agenda-setting power means that A can shoehorn the behaviour of B into a range of possibilities that is to A acceptable, tolerable, or desired. This kind of power is explicit, for example, in practices of regulatory capture, where large tech corporations secure light touch regulation through robust lobbying and legal intervention. </p> <p>Illustrative example: Workplace surveillance</p> <p>With the proliferation of productivity tools has come the rise of workplace surveillance. These apps, when employed by companies, can pose serious challenges to data justice. For instance, Crossover, a talent management company, produced a productivity tool entitled WorkSmart. One of the facets of WorkSmart includes taking screenshots of employees\u2019 workstations and producing \u2018focus scores\u2019 and \u2018intensity scores\u2019 based on their keystrokes and app use. The producers of similar workplace surveillance software often cite preventing insider trading, sexual harassment, and inappropriate behaviour as primary incentives behind the development of these types of apps<sup>2</sup>.</p> <p>Other workplace surveillance apps include Wiretap, which monitors workplace chat forums for threats, intimidation, and other forms of harassment, as well as Digital Reasoning, which \u2018searches more for subtle indicators of possible wrongdoing, such as context switching\u2019, e.g., switching off a workplace app like Slack to use encrypted apps like Signal instead<sup>2</sup>. In an explainer piece by Mateescu and Nguyen<sup>3</sup>, various other types of employee monitoring and surveillance technologies are detailed including behavioural prediction and flagging tools, biometric and health data tracking, remote monitoring and time-tracking, and gamification and algorithmic management. The authors also explore the range of harms these technologies can exact. These include the augmentation of biased and discriminatory workplace practices, the creation of power imbalances between employees and their managers/organisation, and the decrease in workers\u2019 autonomy and agency. Additionally, the authors point out that the use of granular digital tracking and surveillance apps is often motivated by employers\u2019 desire to bolster \u2018cost-cutting\u2019 practices surrounding worker pay, benefits, and standards. </p> <p>Another type of workplace surveillance app closely related to these is the fitness tracker. Biometric wearables are becoming more common across company wellness programmes. While these technologies have often been cited to help decrease company health insurance premiums<sup>3</sup><sup>4</sup> the data gathered by some of these fitness apps can reveal physical location and occasionally sensitive information including family medical histories and diets. In one case, an employer incentivised employees through a US$1 a day gift card to use Ovia, a pregnancy-tracking app. This allowed the company to see aggregated health data collected via the app<sup>5</sup>. While the reasons that employers cite for using apps like these range from boosting employee well-being to decreasing overall company healthcare spending, there remain significant risks of data reidentification and intrusive tracking, among others<sup>5</sup>.</p> <p>Increasing workplace surveillance and monitoring has also been accompanied by higher expectations for employee outputs and quotas. However, achieving these objectives has led to numerous instances of strain and injury in labour-heavy industries. For example, the second largest employer in the United States, Amazon, has monitored and evaluated employees through ADAPT, a proprietary software that not only evaluates employee productivity but also automates termination<sup>6</sup>. This automated management structure continues to operate despite thousands of injury reports and medical advice against the creation of strenuous conditions.</p>"},{"location":"skills-tracks/dj/dj-101-1/#understand-power-at-its-sources-and-raise-critical-awareness-of-its-presence-and-influence","title":"Understand power at its sources and raise critical awareness of its presence and influence","text":"<p>Interrogations of where and how power operates are first steps in a longer journey of questioning and critical analysis. An active awareness of power dynamics in data innovation ecosystems should also lead to further questions:  - What are the interests of those who wield power or benefit from existing social hierarchy? - How do these interests differ from other stakeholders who are impacted by or impact data practices and their governance?  - How do power imbalances shape the differing distribution of benefits and risks among different groups who possess varying levels of power?  - How do power imbalances result in potentially unjust outcomes for marginalised, vulnerable, or historically discriminated against groups? </p>"},{"location":"skills-tracks/dj/dj-101-1/#challenge-power","title":"Challenge power","text":"<p>Mobilise to push back against societally and historically entrenched power structures and to work toward more just and equitable futures. while the questioning and critiquing of power are essential dimensions of data justice, its purpose of achieving a more just society demands that unequal power dynamics that harm of marginalise impacted individuals and communities must be challenged and transformed. </p> <p>Illustrative example: Haki na Sheria (HSI), Kenya</p> <p>Established in 2010, Haki na Sheria Initiative (HSI) is an NGO in Kenya. Driven by the inequalities faced by marginalised communities in Northern Kenya, HSI seeks to end systematic discrimination and empower communities to \u2018understand, demand, and effectively claim their human rights and obligations\u2019<sup>7</sup>. Amongst its work, HSI focuses on issues of environmental justice, gender, and statelessness.</p> <p>Kenyan Somalis, Nubians, and other minoritised communities have long faced structural obstacles when applying for government IDs. Unlike other Kenyans, they are often required to provide additional supporting documents or go through long vetting processes. Not only does this deprive them from a smooth process\u2014 and in most cases, from Kenyan citizenship documents\u2014, but also from services and opportunities these documents enable, such as access to schools, work, travel, public services, and the opening of bank accounts<sup>8</sup>. In addition to this, the government has recently introduced the National Integrated Identity Management System (NIIMS), which aims to digitise identity management. However, because proof of identity is required to obtain a digital ID, marginalised communities suffer from discrimination and exclusion<sup>9</sup>.</p> <p>In response to the far-reaching harmful impacts of the use of digital IDs, HSI has raised awareness of the adverse effects of NIIMS on already marginalised communities. This includes a lack of proper safeguards to protect the rights of minoritised communities. HSI has also challenged the legality of the system, and in 2013 it created a platform where community members can receive advice on how to obtain citizenship documentation. The HSI paralegal team additionally provides people with information on how to \u2018navigate government bureaucracy\u2019<sup>9</sup> and demand justice and equality. </p>"},{"location":"skills-tracks/dj/dj-101-1/#empower-people","title":"Empower people","text":"<p>People must be empowered to marshal democratic agency and collective will to pursue social solidarity, political equity, and liberation. It is common to think about power primarily in a negative way, that is, in terms of unjust exercise of control (abuses of power), coercion, or self-interested influence. But this tells only half the story. Power can also be understood, more positively, in terms of its role in attaining social goods, for example through democracy or collective action. A response within the data justice movement to problematic imbalances of power is empowerment. Empowerment can be understood in terms of concerted and collective use of power through strategic action. When people and communities come together in the shared pursuit of social justice through mutually beneficial practices of knowledge sharing, pedagogy, deliberation, collaboration, dialogue, action, and resistance, power becomes constructive and opens transformative possibilities for the advancement of data justice, social solidarity, and political equity.</p> <p>Illustrative example: Pollicy, Uganda</p> <p>Pollicy is a feminist collective that gathers data scientists, technologists, creatives, and academics to improve data and craft better life experiences. Their work emerges in response to the domination of large and foreign technology companies in African markets and to policies implemented by some governments that, according to Pollicy, have undermined freedom of expression, digital inclusion, and access to information and markets. Their mission is to improve data literacy among different stakeholders, promote the use of responsible data within civil society organisations and government agencies to improve service delivery, and foster the debate about how to use data in an ethical and responsible way. Underlying their work is the certainty of the need to re-imagine digital futures that consider the needs of traditionally marginalised groups rather than trying to fit them within existing frameworks. Therefore, they envision design processes of data collection, analysis, and data release where traditionally marginalised groups are consulted and have a seat at the table. </p> <p>Pollicy is currently undertaking projects that cover research, trainings, workshops, and toolkits. It has explored, for instance, digital extractivism in Africa. Within the project \u201cAutomated Imperialism, Expansionist Dreams\u201d, they documented existing or potential policy responses to a set of problems, ranging from the hiring of digital workers in African countries by tech companies in a context of unequal power dynamics and lack of labour protections to the collection of users\u2019 data through zero-rating and other communication solutions and their exploitation for profit. The document also provided recommendations on how to address the root causes of these problems. Pollicy has also developed a project titled \u201cAfro Feminist Data Futures\u201d, which approaches the production, sharing, and use of gender data as a way to empower feminist movements in sub-Saharan Africa. They explored the ways in which this knowledge can be translated into actionable recommendations for technology companies sharing non-commercial datasets<sup>10</sup>.</p> <ol> <li> <p>Berwick, A. (2018, November 14). A new Venezuelan ID, created with China\u2019s ZTE, tracks citizen behavior. Reuters. https://www.reuters.com/investigates/special-report/venezuela-zte/ \u21a9</p> </li> <li> <p>Solon, O. (2017). Big Brother isn\u2019t just watching: Workplace surveillance can track your every move. The Guardian.https://www.theguardian.com/world/2017/nov/06/workplace-surveillance-big-brother-technology \u21a9\u21a9</p> </li> <li> <p>Mateescu, A., &amp; Nguyen, A. (2019). Explainer: Workplace Monitoring &amp; Surveillance. Data &amp; Society. https://datasociety.net/wp-content/uploads/2019/02/DS_Workplace_Monitoring_Surveillance_Explainer.pdf \u21a9\u21a9</p> </li> <li> <p>Bort, J. (2014, July 8). This Company Saved A Lot Of Money By Tracking Their Employees With Fitbits. Business Insider. https://www.businessinsider.com/company-saved-money-with-fitbits-2014-7 \u21a9</p> </li> <li> <p>Harwell, D. (April 2019). Is your pregnancy app  sharing your intimate data with your boss?. The Washington Post. https://www.washingtonpost.com/technology/2019/04/10/tracking-your-pregnancy-an-app-may-be-more-public-than-you-think/?arc404=true \u21a9\u21a9</p> </li> <li> <p>For a more in-depth evaluation of physical injury and harms at Amazon\u2019s fulfilment centre, read: Evans, W. (2019, November 25). Ruthless Quotas at Amazon Are Maiming Employees. The Atlantic. https://www.theatlantic.com/technology/archive/2019/11/amazon-warehouse-reports-show-worker- injuries/602530/\u00a0\u21a9</p> </li> <li> <p>See Haki na Sheria at http://hakinasheria.org/ \u21a9</p> </li> <li> <p>Bashir, Y. (2020, November 18). Viewpoint: Using Community Paralegals to Promote Inclusion in Digital IDs in Kenya. Good ID. https://www.good-id.org/en/articles/power-to-the-people-using-community-paralegals-to-promote-inclusion-in-digital-ids-in-kenya/ \u21a9</p> </li> <li> <p>Wired Opinion. (2020, February 5). Digital IDs Make Systemic Bias Worse. Wired. https://www.wired.com/story/opinion-digital-ids-make-systemic-bias-worse \u21a9\u21a9</p> </li> <li> <p>Iyer, N., Achieng, G., Borokini, F., Ludger, U., Iyer, N., Syabani, Y., &amp; Syabani, Y. (2021). Automated Imperialism, Expansionist Dreams: Exploring Digital Extractivism in Africa. Pollicy. https://archive.pollicy.org/wp-content/uploads/2021/06/Automated-Imperialism-Expansionist-Dreams-Exploring-Digital-Extractivism-in-Africa.pdf \u21a9</p> </li> </ol>"},{"location":"skills-tracks/dj/dj-101-2/","title":"Equity","text":"<p> \u2018Equity' illustration by Johnny Lighthands, Creative Commons Attribution-ShareAlike 4.0 International.</p> <p>The pillar of equity addresses the need to confront the root causes of data injustices. This pillar also highlights the importance of interrogating the choices that are made about the acquisition and use of data, particularly where the goal or purpose is to target and intervene in the lives of historically marginalised or vulnerable populations.  </p> <p>The examination of equity in this framework considers four core dimensions: confronting issues of equity at the earliest possible stage of a project, transforming historically rooted patterns of inequity, combatting disadvantage and negative characterisation in data collection, and pursuing measurement justice and statistical equity. The following sections explore each of these elements in detail, offering a framework for investigating concerns of equity in the context of data and data-intensive technologies.  </p> <p>For a brief overview of the equity pillar, take a look at the infographic video below. </p>"},{"location":"skills-tracks/dj/dj-101-2/#confront-issues-of-equity-at-the-earliest-stage-of-the-project-before-any-data-are-collected-or-used","title":"Confront issues of equity at the earliest stage of the project, before any data are collected or used","text":"<p>The choice to acquire and use data can itself be a question of justice. This decision becomes especially pertinent where the goal or purpose of a data practice is to target and intervene in the lives of historically marginalised or vulnerable populations. Here, the question may not be \u2018how can we repair an imperfect system or make it more effective\u2019, but rather \u2018does a particular use or appropriation of data enable or disable oppression?\u2019; and \u2018does it preserve or combat harmful relations of power?\u2019 A perfectly engineered system employed by an oppressive regime (either governmental or commercial) can facilitate and potentially amplify data injustice. </p>"},{"location":"skills-tracks/dj/dj-101-2/#transform-historically-rooted-patterns-of-domination-and-entrenched-power-differentials","title":"Transform historically rooted patterns of domination and entrenched power differentials","text":"<p>Data equity demands the transformation of historically rooted patterns of domination and entrenched power differentials. Concerns raised surrounding elements of data innovation practices like data security, data protection, algorithmic bias, and privacy are an important subset of data equity considerations. However, the transformative potential of data equity to advance social justice comes in a step earlier and digs a layer deeper. It starts with questions of how longer-term patterns of inequality, coloniality, and discrimination seep into and penetrate data innovation practices and their governance. Data equity, in this deeper context, is about overhauling power imbalances and forms of oppression that manifest in harmful, unjust, or discriminatory data practices. To realise this sort of equity, those with power and privilege must be compelled to respond to and accommodate the claims of people and groups who have been marginalised by existing political and socioeconomic structures<sup>1</sup>.</p> <p>Illustrative example: Uber 'Movement' Dashboard</p> <p>Platform companies have largely relied on proprietary data and algorithmic systems as intellectual property, which make it possible to avoid explaining platform consequences and decisions at the city-level. This enabled platform companies to make so-called data-powered claims such as Uber stating that its services reduced urban traffic congestion and hence also promoted environmental goals in global cities\u2014a claim they had to finally walk back in 2019<sup>2</sup>. In the absence of data-sharing programs or regulatory frameworks that mandate data-sharing with governments, academics, and the civil society globally, it has been difficult to engage with or challenge the rhetoric of platform companies. In response, Uber launched the \u2018Movement\u2019 dashboard and announced a few partnerships with cities, academics, non-profits, and others in the Global North. Not only has the portal been relatively inactive since its launch but, as researchers have revealed, the dashboard constrains inquiries and hence substantially limits any critical research about gig platforms\u2019 effects on cities<sup>3</sup>.</p> <p>Not only this, but Uber also leaned into the \u2018data privacy\u2019 narrative in 2019 when the New York City Taxi and Limousine Commission wanted to introduce an amendment requiring ride hailing companies to share more ride data that would then also be publicly available online. Uber, the same company that was exposed and penalised for stalking its consumers through the \u2018God\u2019s View\u2019 software<sup>4</sup>, liberally used by its own engineers and data scientists to spy on ex-partners, celebrities, and others, argued, just like Facebook/Meta did with its data-sharing partnerships that beyond a point, granular data activity and identifiers would not allow for companies to retain user anonymity. Importantly, between Global North tech companies and their less powerful oversight bodies, the former certainly has an edge when it comes to maintaining privacy, security, and confidentiality of personal data since these companies have been able to invest in the storage, security, and maintenance infrastructure while the same may not always be possible for a government agency and certainly not so for academics or non-profit actors. This disparity only increases when porting and advocating for data-sharing in service of research and advocacy in global South contexts, with the added risk that in case of a data breach, there may not be robust and realistic recourse to data privacy and protection laws outside of Europe, United States, and Canada.</p>"},{"location":"skills-tracks/dj/dj-101-2/#combat-discriminatory-racialised-or-single-axis-forms-of-data-collection-and-use-that-centre-on-disadvantage-and-negative-characterisation","title":"Combat discriminatory, racialised, or \u2018single axis\u2019 forms of data collection and use that centre on disadvantage and negative characterisation","text":"<p>Data equity involves confronting and combating statistical representations of marginalised, vulnerable, and historically discriminated against social groups that focus mainly or entirely on measurements of \u2019disparity, deprivation, disadvantage, dysfunction, and difference\u2019, the \u20195 D\u2019s\u2019<sup>5</sup>. Approaches to statistical measurement and analysis that centre on disadvantage and negative characterisation produce feedforward effects which further entrench and amplify existing structures of inequity, discrimination, and domination. </p> <p> Single axis modes of statistical representation; adopted from the 5 D's presented by Kukutai and Taylor (2016).</p> <p>Illustrative example: Jordan Open-Source Association (JOSA)</p> <p>Based in the capital city of Amman, the Jordan Open-Source Association (JOSA) is a NPO aimed at promoting openness in technology through free access to non-personal information\u2014such as software, content, network protocols and architecture\u2014as well as the protection of personal information through robust legislation and technological frameworks. </p> <p>Most recently, one of their projects has been aimed at improving the gender gap on Wikipedia with a particular focus on Arab women under the \u2018WikiGap Challenge\u2019 <sup>6</sup>. Wikipedia has played a pivotal role in free access to information through its process of community-led information gathering, editing, and publication. The barriers that may exist to prevent access to relevant information and knowledge on a plethora of themes and subjects are being mitigated through the platform. Notwithstanding debates on using Wikipedia pages as a reliable reference, studies have noted the prevalence of Wikipedia usage and the progress within the platform to promote high-impact journal citations and well-referenced entries<sup>7</sup>. The platform has increasingly emphasised the necessity and role of open-source journal articles in its editing guidelines. Nevertheless, there exist significant gaps in the available literature with gender being a leading factor of imbalance. Among other projects, JOSA has noted and worked towards minimising the gaps in representation and information on Arab heritage and women through the global Wikigap edit-a-thon<sup>8</sup>. The organisation\u2019s work includes events (comprising hackathons, public panels, and workshops) alongside the periodic release of blogposts wherein the challenges of censorship, corporate and government surveillance, arbitrary detention, digital rights, and data justice are given a space of prominence.</p>"},{"location":"skills-tracks/dj/dj-101-2/#pursue-measurement-justice-and-statistical-equity","title":"Pursue measurement justice and statistical equity","text":"<p>Measurement justice and statistical equity involve focusing on collecting and using data about marginalised, vulnerable, and historically discriminated against communities in a way that advances social justice. It requires using data in ways which draw on their strengths rather than primarily on perceived weaknesses, and approaches analytics constructively with community- defined goals that are positive and progressive rather than negative, regressive, and punitive. This constructive approach necessitates a focus on socially licenced data collection and statistical analysis, on individual- and community-advancing outcomes, strengths-based approaches, and on community-guided prospect modelling<sup>9</sup>.</p> <p>More proactive research is needed to explore how positive (individual- and community-advancing) outcomes can be integrated into data analytics that involve marginalised, vulnerable, and historically discriminated against communities. Part of developing such prospect assessment models would involve inclusive, community-integrating processes of objective setting, problem formulation, and outcome definition as well as multi-stakeholder and interdisciplinary approaches to model planning and implementation. Through these processes of co-creation, the analytics would come to better reflect the best interests of the communities to which they apply. Exploring the possibilities of strengths-based, prospective approaches would also involve creating a better data landscape capable of capturing the lived experience of impacted communities, as well as patterns indicative of positive outcomes that foster their wellbeing and flourishing. At the same time, those working toward cultivating this data landscape would have to safeguard the interests of affected data- subjects\u2014in particular, those most vulnerable to over-collection and the potential harms of data misuse\u2014by working through privacy-preserving and consent-based programming.</p> <p>Illustrative example: United States Indigenous Data Sovereignty Network</p> <p>The United States Indigenous Data Sovereignty Network was formed in 2016 to ensure that data for and about Indigenous people in the US are used for their individual and collective well-being. Their mission is to decolonise data and exert Indigenous data governance to promote Indigenous data sovereignty, which \u2018derives from tribes\u2019 inherent right to govern their peoples, lands, and resources\u2019<sup>9</sup>. At the international level, they collaborate with the Te Mana Raraunga in Aotearoa New Zealand and the Ma\u0304ori Data Sovereignty Network<sup>10</sup>.</p> <p>The network has enabled the creation of a transdisciplinary community of practice and provided research information and policy advocacy. Members have developed research projects together and discussed and released policy papers and recommendations. They call for policymakers to recognise Indigenous data sovereignty as an objective to be incorporated into tribal, federal, and other forms of data policies in order to generate resources and build support for Indigenous data governance and grow tribal data capacities including the development of data warriors (Indigenous professionals and community members who are skilled at creating, collecting, and managing data).</p> <ol> <li> <p>D\u2019Ignazio, C., &amp; Klein, L. F. (2020). Data feminism. MIT Press.\u00a0\u21a9</p> </li> <li> <p>Hawkins, A. J. (2019, August 6). Uber and Lyft finally admit they\u2019re making traffic congestion worse in cities. The Verge. https://www.theverge.com/2019/8/6/20756945/uber-lyft-tnc-vmt-traffic-congestion-study-fehr-peers \u21a9</p> </li> <li> <p>Uzel, A. (2018, September 5). Putting \u201cUber Movement\u201d Data into Action \u2014 Machine Learning Approach. Towards Data Science.https://towardsdatascience.com/putting-uber-movement-data-into-action-machine-learning-approach-71a4b9ed0acd \u21a9</p> </li> <li> <p>Welch, C. (2016, January 6). Uber will pay $20,000 fine in settlement over \u2018God View\u2019 tracking. The Verge.https://www.theverge.com/2016/1/6/10726004/uber-god-mode-settlement-fine \u21a9</p> </li> <li> <p>Kukutai, T., &amp; Taylor, J. (Eds.). (2016). Indigenous data sovereignty: Toward an agenda (Vol. 38). ANU Press.\u00a0\u21a9</p> </li> <li> <p>Wikimedians of the Levant/Reports/2020/WikiGap. (2020, December 16). Wikimedia Meta-Wiki. https://meta.wikimedia.org/wiki/Wikimedians_of_the_Levant/Reports/2020/WikiGap \u21a9</p> </li> <li> <p>Duede, E. (2015, September 8). Wikipedia is significantly amplifying the impact of Open Access publications. Impact of Social Sciences.https://blogs.lse.ac.uk/impactofsocialsciences/2015/09/08/wikipedia-amplifying-impact-of-open-access/ \u21a9</p> </li> <li> <p>Rawashdeh, S. (2019, October 8). \u2018Edit-a-thon\u2019 seeks to update online information on Jordanian heritage. Jordan Times.http://www.jordantimes.com/news/local/%E2%80%98edit-thon%E2%80%99%C2%A0seeks-update-online-information-jordanian-heritage \u21a9</p> </li> <li> <p>Leslie, D., Holmes, L., Hitrova, C. &amp; Ott, E. (2020). Ethics review of machine learning in children's social care. What works for children's social care. https://whatworks-csc.org.uk/wp-content/uploads/WWCSC_Ethics_of_Machine_Learning_in_CSC_Jan2020_Accessible.pdf \u21a9\u21a9</p> </li> <li> <p>See United States Indigenous Data Sovereignty Network at https://usindigenousdata.org/about-us \u21a9</p> </li> <li> <p>Russo Carroll, S., Garba, I., Rodriguez-Lonebear, D., Echo-Hawk, A., &amp; Garrison, N. (n.d.). Enhancing Genomic Research in US Through the Lens of Indigenous Data Sovereignty. https://www.law.nyu.edu/sites/default/files/Stephanie%20Russo%20Carroll.pdf \u21a9</p> </li> </ol>"},{"location":"skills-tracks/dj/dj-101-3/","title":"Access","text":"<p> \u2018Access' illustration by Johnny Lighthands, Creative Commons Attribution-ShareAlike 4.0 International.</p> <p>The pillar of access illuminates how a lack of access to the benefits of data processing is a starting point for reflection on the impacts and prospects of technological interventions. The beginning of any and all attempts to protect the interests of the vulnerable through the mobilisation of data innovation should be anchored in reflection on the concrete, bottom-up circumstances of justice and the real-world problems at the roots of lived injustice.   </p> <p>The examination of access in this framework considers four main dimensions: confronting material and structural inequality, starting from questions of access and capabilities, adopting a four-dimensional approach to equitable access, and promoting the sharing of data injustices across communities. The following sections explore each of these elements in detail, offering a framework for investigating concerns of access in the context of data and data-intensive technologies.  </p> <p>For a brief overview of the access pillar, take a look at the infographic video below. </p>"},{"location":"skills-tracks/dj/dj-101-3/#confront-real-world-problems-of-material-inequality-and-structural-injustice","title":"Confront real-world problems of material inequality and structural injustice","text":"<p>Access is about providing people tangible paths to data justice by addressing the root causes of social, political, and economic injustice. Existing sociohistorical, economic, and political patterns of disadvantage must be taken as the starting point for reflection on the equitable access, because these create material conditions of injustice and a lack of access to the benefits of data processing. The beginning of any and all attempts to expand equitable access should be anchored in reflection on the concrete, bottom-up circumstances of justice, in its historical and material preconditions. Combatting the real-world problems at the roots of lived injustice should be a first priority. </p> <p>Illustrative example: Body &amp; Data, Nepal</p> <p>Body &amp; Data was set up in Nepal in 2017 with the objective of improving understanding and access to information for women, queer people, and marginalised groups within the digital space. The organisation primarily conducts legal research and analysis, workshops to improve digital rights and safety, and advocacy programmes at the intersection of feminist and digital rights spaces. </p> <p>The organisation has worked to produce guidelines for an inclusive and intersectional Nepali language<sup>1</sup>.  This working document approaches the role of data privacy through a critical feminist lens. With a goal to improve accessibility and safety in digital spaces, the organisation has also mapped out relevant legislation on gender-based online violence. They are likewise working to improve understanding of Nepal\u2019s Information Technology Bill by providing specific critiques on the implications of surveillance on the freedom of sexual minorities and marginalised communities. Their most recent publications include an evaluation of the harms of misinformation in the COVID-19 pandemic. This evaluation also includes an analysis of the mechanisms needed to mitigate further proliferation of misinformation through fact-checking tools and impact assessments which consider the needs of marginalised or vulnerable minority groups.</p>"},{"location":"skills-tracks/dj/dj-101-3/#start-from-questions-of-access-and-capabilities","title":"Start from questions of access and capabilities","text":"<p>Beyond the critical demand to advance \u2018access to representation\u2019, data justice thinking must focus on equitably opening access to data through responsible data sharing; equitably advancing access to research and innovation capacity; equitably advancing access to the benefits of data work; and equitably advancing access to capabilities to flourish.  </p>"},{"location":"skills-tracks/dj/dj-101-3/#equitably-open-access-to-data-through-responsible-data-sharing","title":"Equitably open access to data through responsible data sharing","text":"<p>Calls for \u2019open data\u2019 sometimes run the risk of oversimplification and appropriation by market forces which could end up curtailing equitable access. The concept of \u2019open data\u2019 itself must be bounded and qualified. At all times, those who share data ought to remain critically aware of the moral claims and rights of the individuals and communities where the data came from, of the real-world impacts of data sharing on those individuals and communities, and of the practical barriers and enablers of equitable and inclusive research. There is also a need to consider the right of communities to access and benefit from the use of their data. Building on this, community-rights based approaches to data access and data sharing should include a strong participatory component. Here equitably opening access to community data entails the democratic governance of data collection and use as well as robust regimes of social license and public consent. </p> <p>Illustrative example: Challenges to data sharing and open data, Africa</p> <p>Within the Western scientific community, the calls for data sharing and open data have been gaining momentum. Data sharing is often regarded as a hallmark of transparency, scientific advancement, and generally a good practice that contributes to better science. These Western based initiatives for data sharing and open data also find their way into the African continent. While data sharing and open data indeed open opportunities for better science, a justice-oriented approach to data sharing/open data practices recognises that such practices are highly complex and contentious issues<sup>2</sup>.</p> <p>Without careful consideration and mitigation of concerns such as unjust historical pasts, structural challenges, colonial legacies, and uneven power structures, such initiatives risk exacerbating existing inequalities and benefiting the most powerful stakeholders<sup>2</sup>.\u201cParachute research\u201d, the practice of Global North researchers absconding with data to their home countries, for example, is one of the concerns that arise with open data in the absence of safeguards that protect data workers (data collectors, data subjects, and other individuals and groups that deal with the task of data management and documentation)<sup>2</sup>. Without safeguards in place, non-African researchers not only benefit from African generated data, but they are also afforded the opportunity to narrate African stories\u2014at times contributing to deficit narratives. In a systematic review that examined African authorship proportions in the biomedical literature published between 1980 \u2013 2016 where research was originally done in Africa, scholars found that African researchers are significantly under-represented in the global health community, even when the data originates from Africa<sup>3</sup>.</p>"},{"location":"skills-tracks/dj/dj-101-3/#equitably-advance-access-to-research-and-innovation-capacity","title":"Equitably advance access to research and innovation capacity","text":"<p>Long-standing dynamics of global inequality may undermine reciprocal sharing between research collaborators from high-income countries (HICs) and those from low-/middle-income countries (LMICs). Given asymmetries in resources, infrastructure, and research capabilities, data sharing between LMICs and HICs, and the transnational opening of data, can lead to inequity and exploitation. Moreover, data originators from LMICs may generate valuable datasets that they are then unable to independently and expeditiously utilise for needed research, because they lack the aptitudes possessed by scientists from HICs, who are the beneficiaries of arbitrary asymmetries in education, training, and research capacitation. In redressing these access barriers, emphasis must be placed on \u2018the social and material conditions under which data can be made useable, and the multiplicity of conversion factors required for researchers to engage with data\u2019. Equalising know-how and capability is a vital counterpart to equalising access to resources, and both together are necessary preconditions of just data sharing. Data scientists and developers engaging in international research collaborations should focus on forming substantively reciprocal partnerships where capacity-building and asymmetry-aware practices of cooperative innovation enable participatory parity and thus greater research access and equity. </p>"},{"location":"skills-tracks/dj/dj-101-3/#equitably-advance-access-to-the-capabilities-of-individuals-communities-and-the-biosphere-to-flourish","title":"Equitably advance access to the capabilities of individuals, communities, and the biosphere to flourish","text":"<p>This involves prioritising individual, social, and planetary well-being as well as an understanding that the attainment of well-being necessitates the stewardship of the human capabilities that are needed for all to freely realise a life well-lived. A capabilities- and flourishing-centred approach to just access demands that data collection and use be considered in terms of the affordances they provide for the ascertainment of well-being, flourishing, and the actualisation of individual and communal potential for these. It demands a starting point in ensuring that \u2018practices of living\u2019 enable the shared pursuit of the fullness, creativity, harmony, and flourishing of human and biospheric life (what Abya Yala Indigenous traditions of Bolivia and Ecuador have called \u2018living well\u2019 or sumak kawsay in Quechua, suma qama\u00f1a in Aymara, or buen vivir in Spanish). </p>"},{"location":"skills-tracks/dj/dj-101-3/#adopt-a-four-dimensional-approach-to-confronting-questions-of-equitable-access","title":"Adopt a four-dimensional approach to confronting questions of equitable access","text":"<p> Four-dimensional approach to equitable access.</p> <p>Concerns with equitable access should: - Concentrate on the equitable distribution of the harms and benefits of data use. This is the dimension of distributive justice. - Examine the material preconditions necessary for the universal realisation of justice. This is the dimension of capabilities-centred social justice. - Rights the wrongs of the past so that justice can operate as a corrective dynamic in the present. This is the dimension of restorative and reparational justice. - Rectify the identity claims of those who have faced representational injury. This is the dimension of representational and recognitional justice.  </p> <p>This four-dimensional approach to equitable access should use the ethical tools provided by the principles of social justice to assess the equity of existing social institutions, while also interrogating the real-world contextual factors that need to change for the universal realisation of the potential for human flourishing and reciprocal moral regard to become possible. It should likewise enable the reparation of historical injustices by instituting processes and mechanisms for reconciliation and restitution. While the first three of these facets remain integral to the advancement of access as it relates to data justice research and practice, they tend to focus primarily on addressing present harms and making course corrections oriented to a more just future. Restorative justice reorients this vision of the time horizons of justice. It takes aim at righting the wrongs of the past as a redeeming force in the present.  </p>"},{"location":"skills-tracks/dj/dj-101-3/#promote-the-airing-and-sharing-of-data-injustices-across-communities-through-transparency-and-data-witnessing","title":"Promote the airing and sharing of data injustices across communities through transparency and data witnessing","text":"<p>Datafication makes possible the greater visibility of everyday social experience. It has been argued that this expanded visibility is a \u2018double-edged sword\u2019 that calls for a balancing of \u2018the need to be counted, and thus potentially served and represented, against the potential for the abuse of power over those who are identified and monitored\u2019<sup>4</sup>.  However, increasing visibility should also be harnessed in positive ways to promote emancipatory transformation by exposing lived injustices, historical abuses, and moral harms<sup>5</sup>. The growth of a networked and connected global society multiplies the transformative power of observation and communication, enabling the far-reaching airing and sharing of previously hidden inequities and mistreatment. The witnessing of injustice both through proximate data work and through the employment of digital media at-a-distance should be marshalled as a force for change and as an opportunity to expand justice by means of transparency and voice.  </p> <p> Various types of transparency.</p> <p>The role of transparency in the airing and sharing of potentially unjust data practices must also be centred. Transparency extends both to outcomes of the use of data systems and to the processes behind their design, development, and implementation. </p>"},{"location":"skills-tracks/dj/dj-101-3/#process-transparency","title":"Process transparency","text":"<p>Process transparency requires that the design, development, and implementation processes underlying the decisions or behaviours of data systems are accessible for oversight and review so that justified public trust and public consent can be ascertained. </p>"},{"location":"skills-tracks/dj/dj-101-3/#outcome-transparency","title":"Outcome transparency","text":"<p>Outcome transparency demands that stakeholders are informed of where data systems are being used and how and why such systems performed the way they did in specific contexts. Outcome transparency therefore requires that impacted individuals can understand the rationale behind the decisions or behaviours of these systems, so that they can contest objectionable results and seek effective remedy. Such information should be provided in a plain, understandable, non-specialist language and in a manner relevant and meaningful to those affected. </p>"},{"location":"skills-tracks/dj/dj-101-3/#professional-and-institutional-transparency","title":"Professional and institutional transparency","text":"<p>Professional and institutional transparency requires that, at every stage of the design and implementation of a project, responsible team members should be identified and held to rigorous standards of conduct that secure and maintain professionalism and institutional transparency. These standards should include the core, justice-promoting values of integrity, honesty, and sincerity as well as positionality-aware modes of neutrality, objectivity, and impartiality. All professionals involved in the research, development, production, and implementation of data-intensive technologies are, first and foremost, acting as fiduciaries of the public interest and must, in keeping with these core justice-promoting values, put the obligations to serve that interest above any other concerns.</p> <p>Illustrative example: 7amleh - The Arab Centre for the Advancement of Social Media, Palestine</p> <p>Rooted in a mission to create a free and fair digital space for Palestine, 7amleh\u2014The Arab Center for the Advancement of Social Media was set up as a NPO that advocates, provides training, and produces research on digital rights, activism, and security capacity-building for Palestinians. Numerous instances of digital violations recorded by 7amleh have catalysed efforts to monitor the violations and provide avenues to limit the moderation of Palestinian content on social media. </p> <p>7amleh\u2019s research in 2021 has focused on digital surveillance in East Jerusalem, the perception of Palestinian CSOs, assessment of digital performance, hate speech, privacy and data protection, guides for journalists, and digital rights abuses. The organisation has also submitted statements on digital rights violations and digital erasure of Palestinians to both international agencies like the UN and European Union as well as technology companies<sup>6</sup>. 7amleh has campaigned against Google in opposition to its failure to label Palestine on maps as well as against Facebook for its disproportionate and harmful moderation of Palestinian content on the platform<sup>7</sup>. Maintaining the importance of journalism in reporting digital rights violations in the region and worldwide, 7amleh has provided a detailed guide for journalists which highlights the importance of diversifying resources, upholding the safety of individuals, and providing a platform for the unheard<sup>8</sup>.</p> <ol> <li> <p>ody &amp; Data. (2020). \u092c\ufffd\u0921 \u090f\u0923\u093f\u094d\u25cc \u093f\u25cc\u25cc\u093e\u091f\u093e\u092e\u093e \u0928\u092a\u0947\u093e\u0932\ufffd \u092d\u093e\u0937\u093e\u0915\u094b \u0938\u092e\u093e\u0935\u0947\u0936\u0940 \u0924\u0925\u093e \u092e\u0948\u0924\u094d\u0930\u0940\u092a\u0923\u0930\u094d \u092d\u093e\u0937\u093e\u0915\u094b \u092a\u094d\u0930\u092f\u094b\u0917 \u0938\u092e\u094d\u092c\u0928\u094d\u0927\u0940 \u0938\u093e\u091d\u093e \u092c\u091d\u0941 \u25cc\u093e\u0908. https://bodyanddata.org/wp-content/uploads/2020/06/Nepali-Language-Guide.pdf \u21a9</p> </li> <li> <p>Abebe, R., Aruleba, K., Birhane, A., Kingsley, S., Obaido, G., Remy, S. L., &amp; Sadagopan, S. (2021). Narratives and counternarratives on data sharing in Africa. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency (pp.329-341).https://dl.acm.org/doi/abs/10.1145/3442188.3445897 \u21a9\u21a9\u21a9</p> </li> <li> <p>Mbaye, R., Gebeyehu, R., Hossmann, S., Mbarga, N., Bih-Neh, E., Eteki, L., Thelma, O., Oyerinde, A., Kiti, G., Mburu, Y., Haberer, J., Siedner, M., Okeke, I. &amp; Boum, Y. (2019). Who is telling the story? A systematic review of authorship for infectious disease research conducted in Africa, 1980\u20132016. BMJ global health, 4(5), e001855. https://gh.bmj.com/content/4/5/e001855 \u21a9</p> </li> <li> <p>Martin, A., &amp; Taylor, L. (2021). Exclusion and inclusion in identification: Regulation, displacement and data justice. Information Technology for Development, 27(1), 50-66. https://doi.org/10.1080/02681102.2020.1811943 \u21a9</p> </li> <li> <p>Gray, J. (2019). Data witnessing: Attending to injustice with data in Amnesty International\u2019s Decoders project. Information, Communication &amp; Society, 22(7), 971- 991. https://doi.org/10.1080/1369118X.2019.1573915 \u21a9</p> </li> <li> <p>Circulated by the Security General in accordance with Economic and Social Council resolution 1996/31; A/HRC/47/NGO/2021\u00a0\u21a9</p> </li> <li> <p>7amleh. (2020a, July 20). Join the Twitter Storm on Google Maps this Wednesday, 22<sup>nd</sup> July at 8pm Palestine time. https://7amleh.org/2020/07/20/join-the-twitter-storm-on-google-maps-this-wednesday-22nd-july-at-8pm-palestine-time \u21a9</p> </li> <li> <p>7amleh. (2021, June 24). Reporting Digital Rights: Guide for Journalists on Reporting Digital Rights Issues. https://7amleh.org/2021/06/24/reporting-digital-rights-guide-for-journalists-on-reporting-digital-rights-issues \u21a9</p> </li> </ol>"},{"location":"skills-tracks/dj/dj-101-4/","title":"Identity","text":"<p> \u2018Identity' illustration by Johnny Lighthands, Creative Commons Attribution-ShareAlike 4.0 International.</p> <p>The pillar of identity addresses the social character of data. It aims to do so by problematising the construction and categorisation of data, which is shaped by the social, cultural, and historical contexts from which it is derived.  </p> <p>The examination of identity in this framework considers three core dimensions: interrogating, understanding, and critiquing harmful categorisations, challenging reification and erasure, and combatting harms of representation. The following sections explore each of these elements in detail, offering a framework for investigating concerns of identity in the context of data and data-intensive technologies.  </p> <p>For a brief overview of the identity pillar, take a look at the infographic video below. </p>"},{"location":"skills-tracks/dj/dj-101-4/#interrogate-understand-and-critique-harmful-categorisations","title":"Interrogate, understand, and critique harmful categorisations","text":"<p>The construction and categorisation of data, particularly when it is about people, is a fundamentally social activity that is undertaken by humans whose views of the world are, in part, the product of cultural contexts and historical contingencies. As such, the construction and categorisation of data is shaped by the sociocultural conditions and historical contexts from which it is derived. The social character of data coupled with the sorting and clustering that proceeds from its cleaning and pre-processing can lead to categorisations that are racialised, misgendered, or otherwise discriminatory. This can involve the employment of binary categorisations and constructions\u2014for example, gender binaries (male/female) or racial binaries (white/non-white)\u2014that are oriented to dominant groups and that ought to be critically scrutinised and questioned. Data justice calls for examining, exposing, and critiquing histories of racialisation and discriminatory systems of categorisation reflected in the way data is classified and the social contexts underlying the production of these classifications. </p> <p>Illustrative example: Feminist Internet</p> <p>Although the internet has allowed individuals and communities across the globe to interact and exchange information, the rise of platforms on the internet has been accompanied by a plethora of incidents of misogyny, sexism, hate, and abuse. The algorithms that have come to define contemporary cyberspace have also contributed to discriminatory practices, from biased image recognition to inadequate monitoring, that endanger many individuals, particularly women, trans folk, and people of colour. With historical exclusion and contemporary societal practices, scores of vulnerable groups lack access to the internet which, in many cases, has allowed for the predominance of transatlantic male-perspectives and objectives<sup>1</sup>.</p> <p>The Feminist Principles of the Internet arose in reaction to the current internet environment by drafting 17 principles organised into five main categories: Access, Movements, Economy, Expression, and Embodiment<sup>1</sup>. Such a framework is aimed at enabling women\u2019s movements to navigate technology-related issues. Similarly, the Feminist Internet has also been established in London as a collective aimed at promoting equity of rights, freedoms, privacy, and data protection irrespective of race, class, gender, gender identity, age, beliefs, or abilities. The collective works towards providing tools and a space to engage in critical creative practices to address issues including online abuse and harassment, anti-transgender media representation, and censorship, amongst others.</p>"},{"location":"skills-tracks/dj/dj-101-4/#challenge-reification-and-erasure","title":"Challenge reification and erasure","text":"<p>In the construction and categorisation of data, system designers and developers can mistakenly treat socially constructed, contested, and negotiated categories of </p> <p>identity as fixed and natural classes. When this happens, the way that these designers and developers categorise identities can become naturalised and reified. This can lead to the inequitable imposition of fixed attributes to classify people who do ascribe to these categorisations or who view them as fluid and inapplicable to the way they identify or regard their themselves. </p> <p>Illustrative example</p> <p>For instance, the designers of a data system may group together a variety of non-majority racial identities under the category of \u201cnon-white\u201d, or they may record gender only in terms of binary classification and erase the identity claims of non-binary and trans people.  </p> <p>In a similar way, designers and developers can produce and use data systems that disparately injure people who possess unacknowledged intersectional characteristics of identity which render them vulnerable to harm, but which are not recognised in the bias mitigation and performance testing measures taken by development teams.  </p> <p>Illustrative example</p> <p>For instance, a facial recognition system could be trained on a dataset that is primarily populated by images of white men, thereby causing the trained system to systematically perform poorly for women with darker skin. If the designers of this system have not taken into account the vulnerable intersectional identity (in this case, women with darker skin) in their bias mitigation and performance testing activities, this identity group becomes invisible, and so too do injuries done to its members<sup>2</sup>.</p>"},{"location":"skills-tracks/dj/dj-101-4/#focus-on-how-struggles-for-recognition-can-combat-harms-of-representation","title":"Focus on how struggles for recognition can combat harms of representation","text":"<p>Struggles for the rectification of moral injuries to identity claims that are suffered at the hands of discriminatory data practices should be understood as struggles for recognitional justice\u2014struggles to establish the equal dignity and autonomy, and the equal moral status, of every person through the affirmation of reciprocal moral, political, legal, and cultural regard.   </p> <p>Illustrative example: First Nations Technology Council (FNTC), Canada</p> <p>While Indigenous communities account for 4.9% of Canada\u2019s population, they make up only 1.9% of the ICT jobs. Colonial practices have perpetuated and deepened digital divides. Such gaps continue to manifest in areas of representation, rights, and respect as these relate Indigenous communities\u2019 ability to access to digital technologies and to seize opportunities to develop and use them<sup>3</sup>. The First Nations Technology Council (FNTC) is an Indigenous-led, NPO that works to promote a robust Indigenous Innovation Ecosystem by connecting all 204 First Nations communities across British Columbia (BC) in Canada. The driving force behind the FNTC is a belief that intersectionality and diversity in innovative technology environments are critical for progress for all in Canada. They primarily work within the mandates of digital skill development, connectivity, information management, and technical support and services. </p> <p>FNTC has published extensive research alongside the launch of education programmes such as the notable flagship Foundations and Futures in Innovation and Technology (FiT) project, which guides students through their digital skills development journey<sup>4</sup>. Divided into two\u2014Foundations and Futures\u2014the project implements programmes to cater to Indigenous individuals at every level of the development of their digital skills. They provide student support through living allowances, tuition fees, and a Digital Elder-in-Residence. The curriculum for the Foundations has been designed by Indigenous specialists to ensure safe and navigable access, while the advanced Futures programme was designed by industry and partners.</p> <ol> <li> <p>Feminist Internet. (n.d.). What is a feminist internet? ComputerAid. Retrieved 10 March 2022 from https://www.computeraid.org/about-us/blog/what-feminist-internet \u21a9\u21a9</p> </li> <li> <p>Buolamwini, J., &amp; Gebru, T. (2018, January). Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency (pp. 77-91). PMLR. https://proceedings.mlr.press/v81/buolamwini18a.html \u21a9</p> </li> <li> <p>Pierre, D. (2022, February 7). Why The Time For Indigenous-led Innovation In Tech Is Now, And How To Support It. First Nations Technology Council. https://technologycouncil.ca/2022/02/07/why-the-time-for-indigenous-led-innovation-in-tech-is-now/ \u21a9</p> </li> <li> <p>First Nations Technology Council. (n.d.). Education Programs \u2013 First Nations Technology Council. Retrieved 10 March 2022 from https://technologycouncil.ca/education/ \u21a9</p> </li> </ol>"},{"location":"skills-tracks/dj/dj-101-5/","title":"Participation","text":"<p> \u2018Participation' illustration by Johnny Lighthands, Creative Commons Attribution-ShareAlike 4.0 International.</p> <p>The pillar of participation promotes the democratisation of data scientific research and data innovation practices. It does so by highlighting the need to involve members of impacted communities, policymakers, practitioners, and developers to collaboratively articulate shared visions for the direction that data innovation agendas should take.  </p> <p>The examination of participation in this framework considers four main dimensions: democratising data and data work, understanding data and data subjects relationally, challenging domination-preserving modes of participation, and ensuring transformational inclusiveness. The following sections explore each of these elements in detail, offering a framework for investigating concerns of participation in the context of data and data-intensive technologies.  </p> <p>For a brief overview of the participation pillar, take a look at the infographic video below.</p>"},{"location":"skills-tracks/dj/dj-101-5/#democratise-data-and-data-work","title":"Democratise data and data work","text":"<p>Prioritise meaningful and representative stakeholder participation, engagement, and involvement from the earliest stages of the data innovation lifecycle to ensure social licence, public consent, and justified public trust. The democratisation of data scientific research and data innovation practices involves bringing members of impacted communities, policymakers, practitioners, and developers together to collaboratively articulate shared visions for the direction that data innovation agendas should take. This entails the collective and democratically based determination of what acceptable and unacceptable uses of data research and innovation are, how data research and innovation should be governed, and how to integrate the priorities of social justice, non-discrimination, and equality into practices of data collection, processing, and use. </p> <p>Illustrative example: May First Technology Movement, United States and Mexico</p> <p>May First Technology is a non-profit movement organisation founded in 2005 that shares its technology as a non-profit service provider so it can be used strategically and collectively by organisations and activists in the United States and Mexico working for local struggles and global transformation. It has 850 members who own over 120 servers maintained by May First Technology and have unlimited access to its internet services. </p> <p>Members also participate in networks, coalitions, and campaigns around topics like net neutrality, data protection, and alternative connection systems. Since 2017, the May First Technology movement has brought together over 1,500 activists in the United States and Mexico through their \u201cTechnology and Revolution\u201d series. This has seen participants discuss the ways in which technology can intersect with activism and with revolution. Underlying their work is the conviction that the use, protection, and democratisation of technology is a key element for fundamental change.</p>"},{"location":"skills-tracks/dj/dj-101-5/#understand-data-and-data-subjects-relationally","title":"Understand data and data subjects relationally","text":"<p>Data collection and use should not be pursued in a way that reifies, objectifies, or commodifies data or data subjects. Where data innovation practices focus only on an individual\u2019s relationship to data (as a possession or form of property) or on an individual\u2019s privacy or data protection rights, these practices lose sight of the wider contexts of their social effects, their population- level impacts, and the interconnectedness of the people and communities who are affected by data innovation ecosystems. A relational view of data practices<sup>1</sup>,  which starts from this broader vantage point, recasts them as involving horizontal and interwoven social relationships in addition to vertical relationships between the individual data subject and the data collector, processor, or user. Understanding data and data subjects relationally entails recognising that data practices need to be situated in their social environments and governed democratically through horizontal, participation-based forms of collective action that provide coverage of a complex and multi-stakeholder ecology of interests, rights, obligations, and responsibilities. </p> <p>Illustrative example: Sursiendo, Mexico</p> <p>Sursiendo is a collective of activists working with regional organisations in Southeast Mexico to defend digital communality, collective digital rights, and hackfeminism\u2014a concept used to incorporate intersectionality in the design, development, and use of technology so that designer and activists can \u2018open the systems, hack the patriarchy.\u2019 Placing gender and equitable participation at the core of their endeavours, Sursiendo utilises the avenues of activism, communication and design, free software, popular education, art, and cultural management to contribute to their vision. </p> <p>The organisation has produced extensive literature at the nexus of gender and technology and has published tools for advocacy and the protection of rights. In 2020, they released a tool titled \u201cHerramienta del Registro Incidentes De Seguridad Digital\u201d (translated to \u201cLogging Digital Security Incidents as a Risk Mitigation Practice \u201c). Divided into two parts\u2014a tool for recording digital security incidents and a guide for registration with human rights agencies\u2014the resource is seen to be invaluable for collective action against risks in the digital sphere. The records of cybersecurity incidents (such as malware, phishing, Denial of Service, to name a few) include details on the vulnerabilities of the device being used and the capacities for the organisation or individual to intervene and mitigate the incident. These records are believed to be useful for complaint registration which can serve to prevent future incidents. Moreover, Sursiendo notes that the tool is an important mechanism for reflection, while also assisting in the identification of gaps and opportunities relevant to advocacy and capacity-building for organisations<sup>2</sup>.</p>"},{"location":"skills-tracks/dj/dj-101-5/#challenge-existing-domination-preserving-modes-of-participation","title":"Challenge existing, domination-preserving modes of participation","text":"<p>Where current justifications and dynamics of data practices reinforce or institutionalise prevailing power structures and hierarchies, the choice to participate in such practices can be counterproductive or even harmful. When options for a community\u2019s participation in data innovation ecosystems and their governance operate to normalise or support existing power imbalances and the unjust data practices that could follow from them, these options for involvement should be approached critically. A critical refusal to participate is a form of critical participation<sup>3</sup> <sup>4</sup> <sup>5</sup> <sup>6</sup> and should remain a practical alternative where extant modes of participation normalise harmful data practices and the exploitation of vulnerability.  </p> <p>Illustrative example: Feminist Data Manifest-No</p> <p>The Feminist Data Manifest-No provides a clear depiction of both gaps and harmful practices in the existing data landscape. This document intersects with notions of data feminism as they relate to advancing data justice research and practice. The Manifest-No is defined as \u2018a declaration of refusal and commitment...it refuses harmful data regimes and commits to new data futures\u2019<sup>6</sup>. The Manifest-No sets out to refute harmful data practices, while calling for a new future in which Latinx, Black, queer, trans- and Ingenious feminists are both celebrated and listened to. Based in critical refusal, the authors of the Manifest-No claim that refusal \u2018can help different feminisms recognise interlocking struggles across domains, across contexts and cultures, and that enables us to work in solidarity to prop up and build resilience with one another\u2014to generate mutually reinforcing refusals\u2019<sup>6</sup>. Therefore, the series of refusals and commitments set out in the Feminist Manifest- No acknowledge the importance of both shared refusal and that \u2018systemic patterns of violence and exploitation produce differential vulnerabilities for communities\u2019<sup>6</sup>. Within the document, there are also commitments to mobilise data by working \u2018with minoritised people in ways that are consensual, reciprocal, and that understand data as always co-constituted\u2019<sup>6</sup>. These practices engage in critical refusal as participation, combat discriminatory and racialised politics of data collection and use, question binaries, and critique existing forms of power.</p>"},{"location":"skills-tracks/dj/dj-101-5/#ensure-transformational-inclusiveness-rather-than-power-preserving-inclusion","title":"Ensure transformational inclusiveness rather than power-preserving inclusion","text":"<p>Incorporating the priority of inclusion into sociotechnical processes of data innovation can be detrimental where existing power hierarchies are sustained or left unaddressed. Where mechanisms of inclusion normalise or support existing power imbalances in ways that could perpetuate data injustices and fortify unequal relationships, these should be critically avoided. Transformational inclusiveness demands participatory parity so that the terms of engagement, modes of involvement, and communicative relationships between the includers and the included are equitable, symmetrical, egalitarian, and reciprocal. </p> <p>Illustrative example: Maiam nayri Wingara Aboriginal and Torres Strait Islander Data Sovereignty Collective, Australia</p> <p>The Maiam nayri Wingara Aboriginal and Torres Strait Islander Data Sovereignty Collective was formed in 2017 in response to the isolation of Indigenous Australians from the language, control, and production of data, as well as the neglection of their knowledge, worldviews, and needs. The Collective seeks to progress Indigenous Data Sovereignty and Indigenous Data Governance through the development of data sovereignty principles, data governance protocols, and the identification of strategic data assets. </p> <p>The Maiam nayri Wingara Data Sovereignty Collective and Australian Indigenous Governance Institute created a Communique as a result of the 2018 Indigenous Data Sovereignty Summit. The Communique aims to advance Indigenous Data Sovereignty through the initiation of Indigenous data governance protocols. The Communique claims that Indigenous communities \u2018maintain the right to not participate in data processes inconsistent with the principles asserted in this Communique\u2019. Actions taken by the collective are founded on the understanding that the exercise of Indigenous data governance will enable an accurate and informed picture of the realities, needs, and aspirations of Indigenous people.</p> <ol> <li> <p>Viljoen, S. (2021). A relational theory of data governance. The Yale Law Journal, 131(2), 573-654. https://www.yalelawjournal.org/pdf/131.2_Viljoen_1n12myx5.pdf \u21a9</p> </li> <li> <p>Sursiendo. (2020, October 20). Registrando Incidentes de Seguridad Digital como Pra\u0301ctica de Mitigacio\u0301n del Riesgo. Sursiendo.https://sursiendo.org/2020/10/registro-y-analisis-de-incidentes-de-seguridad-digital/ \u21a9</p> </li> <li> <p>See Ahmed, S. (2012). On being included. Duke University Press and Ahmed, S. (2018, June 28). Refusal, resignation, and complaint. Feministkilljoys. https://feministkilljoys.com/2018/06/28/refusal-resignation-and-complaint/ \u21a9</p> </li> <li> <p>Benjamin, R. (2019). Race after technology: Abolitionist tools for the new Jim code. Polity Books.\u00a0\u21a9</p> </li> <li> <p>Cifor, M., Garcia, P., Cowan, T.L., Rault, J., Sutherland, T., Chan, A., Rode, J., Hoffmann, A.L., Salehi, N., Nakamura, L. (2019). Feminist data manifest-no. https://www.manifestno.com/ \u21a9</p> </li> <li> <p>Cifor, M., Garcia, P., Cowan, T.L., Rault, J., Sutherland, T., Chan, A., Rode, J., Hoffmann, A.L., Salehi, N., Nakamura, L. (2019). Feminist data manifest-no. https://www.manifestno.com/ \u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"skills-tracks/dj/dj-101-6/","title":"Knowledge","text":"<p> \u2018Knowledge' illustration by Johnny Lighthands, Creative Commons Attribution-ShareAlike 4.0 International.</p> <p>The pillar of knowledge underlines the need to recognise that diverse forms of knowledge and understanding can add valuable insights to the aspirations, purposes, and justifications of data use\u2014including on the local or context-specific impacts of data-intensive innovation. Inclusion of diverse knowledges and ways of being can open unforeseen paths to societal and biospheric benefits and maximise the value and utility of data use across society in ways which take account of the needs, interests, and concerns of all affected communities. </p> <p>The examination of knowledge in this framework considers five core dimensions: embracing the pluralism of knowledges, challenging the unquestioned authority of \u2018expertise\u2019, prioritising interdisciplinarity, pursuing reflexivity, and cultivating intercultural sharing, learning, and wisdom. The following sections explore each of these elements in detail, offering a framework for investigating concerns of knowledge in the context of data and data-intensive technologies.  </p> <p>For a brief overview of the knowledge pillar, take a look at the infographic video below.</p>"},{"location":"skills-tracks/dj/dj-101-6/#embrace-the-pluralism-of-knowledges","title":"Embrace the pluralism of knowledges","text":"<p>Different communities and sociocultural groups possess unique ways of seeing, understanding, and being in the world. This plurality of knowledges and of lived experience should inform and be respected in practices of data collection, processing, and use as well as in the policymaking practices surrounding the governance of data technologies. Embracing the pluralism of knowledges involves recognising that diverse forms of knowledge, and ways of knowing and understanding, can add valuable insights to the aspirations, purposes, and justifications of data use\u2014including on the local or context-specific impacts of data-intensive innovation. Moreover, inclusion of diverse knowledges and ways of being can open unforeseen paths to societal and biospheric benefits and maximise the value and utility of data use across society in ways which take account of the needs, interests, and concerns of all affected communities.  </p> <p>Illustrative example: Coconet, Southeast Asia</p> <p>Since its launch in 2019, Coconet serves a dual function as a both a mechanism for network building and a platform for digital rights. The organisation works in collaboration with EngageMedia and the Association for Progressive Communications among others. Coconet aims to accelerate the digital rights movement in the Asia-Pacific by focusing on the role of the internet in discussion and dissent in the region.</p> <p>Digital and internet spaces have, in recent times, been subject in Southeast Asia to increasing surveillance, censorship, deterioration of freedoms and rights, and incidents of false or misleading information. In response to this, Coconet works to provide tools and strategies to effectively use cyberspace for advocacy and activism while improving equitable access to research and content on related themes of digital hygiene and AI. Coconet publications have focused on the harms and impacts of multifarious legislation within the states of South-East Asia. For instance, they have provided detailed research on Myanmar\u2019s draft Cyber Security Law which will empower the government to enact internet shutdowns and increase military control and oversight of the internet<sup>1</sup>. Another example is the case of Indonesia\u2019s Ministerial Regulation 5 (MR5) which has the potential for \u2018prepublication censorship\u2019. Importantly, Coconet\u2019s community-driven platform is working toward providing multilingual resources for data justice advocacy and activism. </p>"},{"location":"skills-tracks/dj/dj-101-6/#challenge-the-assumed-or-unquestioned-authority-of-technical-professional-or-expert-knowledge-across-scientific-and-political-structures","title":"Challenge the assumed or unquestioned authority of technical, professional, or \u201cexpert\u201d knowledge across scientific and political structures","text":"<p>Processes of knowledge creation in data science and innovation are social processes which require scrutiny and wider public engagement to hold \u201cexpertise\u201d to account and to ensure that data science and innovation progress in ways which align with wider societal values. This means that data technology producers and users have a responsibility to communicate plainly, equitably, and to as wide an audience as possible. Clear and accessible public communication of research and innovation purposes/goals and data analytic and scientific results should enable the public to interrogate the claims and arguments being put forward to justify data-driven decision-making and data innovation agendas. This also means that members of the public have a corollary responsibility to listen to\u2014i.e. to pay attention to, engage with, and critically assess \u2013 the scientifically authoritative knowledge claims and technological systems that impact them.  </p> <p>Illustrative example: Nupef, Brazil</p> <p>The Instituto Nupef is an NPO in Brazil established in 2019 that aims to generate conditions for the effective use of technology to defend democracy and fundamental rights. Nupef produces research and learning activities for different stakeholders about the potentials of ICTs to enhance citizens\u2019 participation and the promotion of digital rights, implements pilot projects for innovation, and advocates for policies and practices that range from privacy to an open spectrum (an approach to radio spectrum management where unlicensed spectrum is available for use by all). </p> <p>To integrate and expand the knowledge about existing practices of innovative uses of ICT that support sustainable development, human rights, and social justice, Instituto Nupef launched \u201cEspectro\u201d (translated as \u201cSpectrum\u201d). Espectro is a collaborative and multieditorial web portal created by Nupef with the aim of sharing information and knowledge on network practices pertaining to new radio technologies for community use, encouraging the innovative use of information and communication technologies to support sustainable development, human rights, social justice, good governance, and democratic values. Content on the Espectro platform includes the monitoring of community experiments and information on regulation and the effective implementation of ICT projects. </p>"},{"location":"skills-tracks/dj/dj-101-6/#prioritise-interdisciplinarity","title":"Prioritise interdisciplinarity","text":"<p>Approach the pursuit of understanding of data innovation environments\u2014and the sociotechnical processes and practices behind them\u2014through a holistically informed plurality of methods. This involves placing a wide range of academic disciplines and specialised knowledges conceptually on par, enabling an appreciation and integration of a wide range of insights, framings, and understandings. Ways of knowing that cannot (or are not willing to) accommodate a disciplinary plurality of knowledgeable voices that may contribute to richer comprehensions of any given problem cease to be knowledgeable per se.  </p> <p>Illustrative example: InternetLab, Brazil</p> <p>InternetLab is a non-profit research centre based in Brazil that conducts interdisciplinary research about internet policy. More specifically, their research agenda includes issues of privacy and surveillance, culture and knowledge, freedom of speech, information and politics, gender, and technology. Its mission is to create and disseminate knowledge about internet policy and to support related initiatives and research projects. For this reason, they engage with other stakeholders across the globe who are working on related fields. </p> <p>Among its projects, InternetLab has evaluated automated content moderation tools by social media platforms. In their research \u201cDrag Queens and Artificial Intelligence\u201d, InternetLab has studied Perspective, the AI technology developed by Jigsaw from Alphabet Inc., that assigns a level of \u201ctoxicity\u201d to text-based content. Results showed \u2018that a significant number of drag queen Twitter accounts were considered to have higher perceived levels of toxicity than those of Donald Trump and white supremacists<sup>2</sup>. Words, such as \u2018gay\u2019, \u2018lesbian\u2019, and \u2018queer\u2019, despite their context, were associated with \u201ctoxic\u201d content. The lack of neutrality in this approach was highlighted by InternetLab as problematic for the exercising of the right to freedom of expression. </p> <p>InternetLab has also participated in a project titled \u201cPolicy Frameworks for digital platforms\u2014moving from openness to inclusion.\u201d This research explores the juridical-institutional arrangements that regulate digital platforms. It also identifies emerging issues including market monopolisation, challenges to development justice, ownership of user data, privatisation of informational commons, and worker exploitation. Based on its unpacking of these issues, the project proposes policies that may address such challenges while considering inclusion and socio-economic development. InternetLab is conducting a case study on the regulation of on- demand videos, investigating the impact of the platform economy on the Brazilian audiovisual market as this pertains to user access, available content, and the financial contributions to the State</p>"},{"location":"skills-tracks/dj/dj-101-6/#pursue-a-reflexive-and-positionally-aware-objectivity-that-amplifies-marginalised-voices","title":"Pursue a reflexive and positionally aware objectivity that amplifies marginalised voices","text":"<p>A robust approach to objectivity demands that knowers have positional self-awareness, which acknowledges the limits of everyone\u2019s personal, historical, and cultural standpoint. It also demands that knowers carry out critical and systematic self-interrogation to better understand these limitations. This launching point in reflexive and positionally aware objectivity can end up leading to more objective and more universalistic understandings than modes of scientific or technical objectivity which stake a claim to unobstructed neutrality and value-free knowledge that evades self-interrogation about the limits of standpoint and positionality. One reason for this has to do with power dynamics. Reflexive and positionally aware objectivity starts from a reflective recognition of how differential relations of power and social domination can skew the objectivity of deliberations by biasing the balance of voices that are represented in those deliberations. It then actively tries to include and amplify marginalised voices in the community of inquiry to transform situations of social disadvantage where important perspectives and insights are muted, silenced, and excluded into situations that are scientifically richer and more advantaged. Such richer and more inclusive ecologies of understanding end up producing more comprehensive knowledge and more just and coherent practical and societal outcomes. Reflexive and positionally aware objectivity amplifies the voices of the marginalised, vulnerable, and oppressed as a way to overcome claims of objectivity, impartiality, and neutrality that mask unquestioned privileges<sup>3</sup> <sup>4</sup>.</p>"},{"location":"skills-tracks/dj/dj-101-6/#cultivate-intercultural-sharing-learning-and-wisdom","title":"Cultivate intercultural sharing, learning, and wisdom","text":"<p>A plurality of insights, learning, and wisdom from a diverse range of communities and sociocultural groups should inform the values, beliefs, and purposes behind data research and innovation agendas and practices. Cultivating intercultural sharing, learning, and wisdom serves this end by bringing a multitude of ideas and perspectives into conversation. This involves setting up and sustaining networks of communication and collaboration between communities and sociocultural groups, so that they can come together to cultivate shared understandings and constructively explore differences. At the same time, cultivating intercultural sharing, learning, and wisdom as a way to advance data justice involves drawing on the principles and priorities of social justice to find commonality and to build solidarity among communities and sociocultural groups.</p> <p> Pluralism of knowledges.</p> <p>Illustrative example: iFreedom, Uganda</p> <p>The weaponising of the internet against LGBTQI+ and Sex Workers (SW) in Uganda was found to have severe consequences for these marginalised communities\u2019 offline and online freedoms and rights, including those to freely associate and express themselves without fear of threats posed by state agencies and hackers<sup>5</sup>. Consequently, five LGBTQI+ and two SW organisations came together to establish the iFreedom network to defend digital rights and freedoms alongside the protection of security and safety in 2012. Since then, the network has grown to include 28 organisations\u2014categorised into LGBTQI+, SW, and other human rights defenders\u2014all of whom share a common goal of promoting digital rights and freedoms through research, advocacy, and digital knowledge capacity-building. </p> <p>Beyond research and advocacy activities, the organisation also provides IT support, web design and hosting services, and computer training programmes. The training programmes are primarily catered towards sex workers regardless of their levels of skill. Beginning with a fundamental computer training phase, participants can then move to more complex digital security training, thereby contributing to the growth of an environment of informed and skilled sexual minorities and sex workers. Even amidst the many instances of violence and persecution of sexual minorities in Uganda, the organisation has become critical for equipping marginalised communities with the skills needed to defend themselves and progress in an increasingly digital world. </p> <p>Harding, S. G. (1995). \u201cStrong objectivity\u201d: A response to the new objectivity question. Synthese, 104(3), 331\u2013349. https://doi.org/10.1007/BF01064504 </p> <p>Harding, S. G. (2008). Sciences from below: Feminisms, postcolonialities, and modernities. Duke University Press. https://doi.org/10.1215/9780822381181 </p> <p>Harding, S. G. (2015). Objectivity and diversity: Another logic of scientific research. The University of Chicago Press. https://doi.org/10.7208/9780226241531 </p> <p>Unwanted Witness. (2015, April 28). For LGBT Ugandans, physical security threats often translate into digital threats and vice versa. Unwanted Witness. https://www.unwantedwitness.org/for-lgbt-ugandans-physical-security-threats-often-translate-into-digital-threats-and-vice-versa/ </p> <ol> <li> <p>Coconet. (2021a, February 24). #WhatsHappeningInMyamnmar: Six risks from Myanmar\u2019s draft Cyber Security Law. Coconet. https://coconet.social/2021/myanmar-cyber-security-law/index.html \u21a9</p> </li> <li> <p>InternetLab. (2019). Drag Queens and Artificial Intelligence. https://internetlab.org.br/en/projetos/drag-queens-and-artificial-intelligence/ \u21a9</p> </li> <li> <p>Haraway, D. (1988). Situated knowledges: The science question in feminism and the privilege of partial perspective. Feminist Studies, 14(3), 575-599. https://www.jstor.org/stable/3178066?seq=1#metadata_info_tab_contents \u21a9</p> </li> <li> <p>Harding, S. G. (1992). Rethinking standpoint epistemology: What is \"strong objectivity?\" The Centennial Review, 36(3), 437\u2013470.\u00a0\u21a9</p> </li> <li> <p>Amnesty International. (2020, May 18). Uganda\u2019s new anti-human rights laws aren\u2019t just punishing LGBTI people. https://www.amnesty.org.uk/uganda-anti-homosexual-act-gay-law-free-speech \u21a9</p> </li> </ol>"},{"location":"skills-tracks/dj/dj-101-index/","title":"Introduction to the Six Pillars of Data Justice","text":"<p> 'Six Pillars of Data Justice' illustrations by Johnny Lighthands, Creative Commons Attribution-ShareAlike 4.0 International.</p>"},{"location":"skills-tracks/dj/dj-101-index/#chapter-outline","title":"Chapter Outline","text":"<ul> <li>Power</li> <li>Equity</li> <li>Access</li> <li>Identity</li> <li>Participation</li> <li>Knowledge</li> </ul> <p>Chapter Summary</p> <p>This chapter introduces the six pillars of data justice: power, equity, access, identity, participation, and knowledge. Stemming from the work undertaken by the Advancing Data Justice Research and Practice (ADJRP) project, these pillars are intended as guiding priorities to guide the critical reflection of data justice through wider ethical considerations, rather than technocentric ones. </p> <p>Each pillar is explored through its own dedicated subchapter and through a set of guiding considerations that can be used as a framework for furthering the data justice movement. </p> <p>Learning Objectives</p> <p>In this chapter you will:</p> <ul> <li>familiarise yourself with the six pillars of data justice; and</li> <li>learn about how unjust data practices and data-intensive technologies can be examined, critiqued, challenged, and reimagined using the scaffolding provided by these six guiding priorities. </li> </ul>"},{"location":"skills-tracks/dj/dj-102-1/","title":"Documentary series","text":"<p> Snapshots from the three-part documentary series.</p> <p>The previous chapters have introduced the data justice movement and have delved into how this effort can be advanced in inclusive and intercultural ways through the critical considerations raised by the Six Pillars of Data Justice. This chapter expands on the theoretical foundations and illustrative examples provided previously by exploring instances of data justice in action through a range of grassroots initiatives that are being undertaken by impacted communities across the world. You will be hearing directly from individuals and communities that have been impacted by unequitable data practices and learn more about how they have been mobilising to challenge these. </p> <p>Keep In Mind</p> <p>As demonstrated by the following initiatives, marginalised communities have been facing and mobilising against data injustices\u2014along with wider, systemic inequalities\u2014long before the term \u2018data justice\u2019 was coined within academic circles. It is important to recognise the longer-term histories of discrimination, inequality, and oppression that continue to inform present day experiences of harm within the context of data-intensive technologies.  </p> <p>As part of the ADJRP project, we collaborated with 12 policy pilot partners to develop a three-part data justice documentary series that closely examines the ongoing activism and advocacy work these civil society organisations are undertaking. These 12 organisations (listed below) have an extensive history of working closely alongside their local communities to expose and challenge ongoing data injustices. Their crucial perspectives, experiences, and approaches are captured through the following three episodes \u2013 \u2018Introducing Data Justice\u2019, \u2018Uncovering Data Injustice\u2019, and \u2018Mobilising for Data Justice\u2019. </p> <ul> <li>AfroLeadership (Yaound\u00e9, Cameroon)\u200b </li> <li>CIPESA (Kampala, Uganda)\u200b </li> <li>CIPIT (Nairobi, Kenya)\u200b </li> <li>Digital Empowerment Foundation (Delhi, India)\u200b </li> <li>Digital Natives Academy (Aotearoa, New Zealand)\u200b </li> <li>Digital Rights Foundation (Lahore, Pakistan)\u200b </li> <li>EngageMedia (Melbourne, Australia)\u200b </li> <li>Gob_Lab UAI (Santiago, Brazil)\u200b </li> <li>Internet Bolivia (La Paz, Bolivia)\u200b </li> <li>ITS Rio (Rio de Janeiro, Brazil)\u200b </li> <li>Open Data China (Shanghai, China)\u200b </li> <li>WOUGNET (Kampala, Uganda) </li> </ul>"},{"location":"skills-tracks/dj/dj-102-2/","title":"Introducing Data Justice","text":"<p>This is the first episode in the data justice documentary series by The Alan Turing Institute, which brings together perspectives from across the globe to discuss how data-driven technologies can be deployed in an equitable manner (19:09 in duration).</p>"},{"location":"skills-tracks/dj/dj-102-3/","title":"Uncovering Data Injustice","text":"<p>This is the second episode in the data justice documentary series by The Alan Turing Institute. It examines the range of data injustices occurring across the world, including the unfair algorithmic practices employed in India\u2019s gig economy (Digital Empowerment Foundation), the perpetuation of online abuse against women (Women of Uganda Network), and the impacts of sensitive data leaks (Internet Bolivia) (21:50 in duration).</p>"},{"location":"skills-tracks/dj/dj-102-4/","title":"Mobilising for Data Justice","text":"<p>The path to data justice has many complex and systemic challenges, yet individuals and communities around the world are paving a hopeful path. In this final episode of the data justice documentary series by The Alan Turing Institute, we hear from our global partners on how they mobilise for data justice through their transformative activism and advocacy. They shed light on the actions we need to take now to disrupt longstanding structures of inequity. This episode features two partner perspectives; Project Adal from Digital Rights Foundation with HOPE and M\u0101ori Data Rangatiratanga from Digital Natives Academy (29:09 in duration). </p>"},{"location":"skills-tracks/dj/dj-102-index/","title":"Data Justice in Action","text":"<p> \u2018Policymakers\u2019 illustration by Johnny Lighthands, Creative Commons Attribution-ShareAlike 4.0 International.</p>"},{"location":"skills-tracks/dj/dj-102-index/#chapter-outline","title":"Chapter Outline","text":"<ul> <li>Documentary series</li> <li>Introducing Data Justice</li> <li>Uncovering Data Injustice</li> <li>Mobilising for Data Justice</li> </ul> <p>Chapter Summary</p> <p>In this chapter we explore instances of data justice in action by diving into grassroots initiatives being undertaken by impacted communities across the world. These stories are told through the Advancing Data Justice Research and Practice (ADJRP) project\u2019s three-part data justice documentary series, offering viewers a chance to hear directly from communities impacted by unequitable data practices and learn about how they have been mobilising to challenge these.  </p> <p>Learning Objectives</p> <p>In this chapter you will:</p> <ul> <li>hear directly from communities impacted by unequitable data practices and learn about how they have been mobilising to challenge these.</li> </ul>"},{"location":"skills-tracks/ped/","title":"About this Course","text":"<p>This course is designed to help you understand the practical and ethical value of public engagement with data science and AI.   The course begins with an introduction to different forms of public engagement, while critically examining the different methods and approaches.   Then, through a series of structured seminars and workshops, you will consider the impact of public engagement upon both practices of research and innovation as well as society more broadly.</p> <p>Following this general introduction, the course pivots to introduce and discuss practical methods of public engagement, including deliberative activities that help build consensus among stakeholder; transparent and explainable methods of data governance to support project activities; methods of data visualisation to support the communication of science and technology; and an awareness of social and psychological biases, which can negatively affect the goals of responsible public engagement.</p>"},{"location":"skills-tracks/ped/#table-of-contents","title":"Table of Contents","text":"<ul> <li> <p> What is Public Engagement?</p> <p>This chapter looks at foundational concepts and topics, including what public engagement is and what the various goals of public engagement are.</p> <p> Go to chapter</p> </li> <li> <p> The Value(s) of Public Engagement</p> <p>This chapter explores and critically examines the ethical and social values that motivate and underpin public engagament.</p> <p> Go to chapter</p> </li> <li> <p> Facilitating Public Engagement</p> <p>This chapter introduces a wide variety of practical methods and processes for designing and facilitating public engagement in data science and AI.</p> <p> Go to chapter</p> </li> <li> <p> Public Communication</p> <p>This chapter addresses some challenges that arise in the context of public communication (e.g., cognitive biases and scientific uncertainty) and considers ways to overcome them.</p> <p> Go to chapter</p> </li> <li> <p> Public Trust and Assurance</p> <p>The concluding chapter looks at the issue of public trust, exploring matters such as mis/disinformation, social media, and the use of social media for science and technology communication.</p> <p> Go to chapter</p> </li> </ul>"},{"location":"skills-tracks/ped/#who-is-this-course-for","title":"Who is this course for?","text":"<p>Primarily, this course is for researchers with an active interest in public engagement, specifically in the context of data science and artificial intelligence. This doesn't mean you have to be a data scientist, or use Python to develop machine learning algorithms. You could also be an ethicist, sociologist, or someone with an interest in law and public policy.</p> <p>This course has practical, and sometimes hands-on activities that are designed to a) encourage critical reflection and b) help you build practical understanding of the processes associated with effective and responsible public engagement in data science and AI. While they can be carried out as part of individual and self-directed learning, they are most suited to group discussion.</p>"},{"location":"skills-tracks/ped/#learning-objectives","title":"Learning Objectives","text":"<p>This guidebook has the following learning objectives:</p> <ul> <li>Critically examine what 'public engagement' is, the goals associated with different types of public engagement, and to identify the associated values.</li> <li>Understand the different stages of public engagement as they apply to the typical activities of a data science or AI research/innovation project.</li> <li>Explore practical methods and activities that can help build more effective forms of public engagement.</li> <li>Identify the elements of public engagement that help build a more trustworthy data and AI ecosystem.</li> </ul>"},{"location":"skills-tracks/ped/chapter1/","title":"What is Public Engagement?","text":"<p>You may think that the answer to this question is so obvious that it is not worth asking the question in the first place. Or, perhaps you would give an answer along these lines:</p> <p>Public engagement is a process of dialogue between researchers and members of the public with the goal of either a) determining the attitudes or beliefs of the public or b) informing them of developments in science and technology.</p> <p>An answer such as this would capture a significant part of what public engagement involves. However, it would also miss some important nuance and detail.</p> <p>The purpose of this section is to develop a more careful and extensive understanding of what public engagement is and entails, which will then serve as the foundation for the remainder of the course. It begins with an introduction to and critical examination of public engagement, before turning to look at what public engagement is and the goals associated with it.</p> <p>The chapter is primarily theoretical in scope, but serves as an important foundation for the more practical chapters later in the guide. Without this foundation, your understanding of some of the concepts in later chapters may be impaired. Therefore, it is only advised for you to skip this chapter if you already have a good understanding of the concept of 'public engagement'.</p>"},{"location":"skills-tracks/ped/chapter1/#chapter-outline","title":"Chapter Outline","text":"<ul> <li>Climbing the Ladder: From Informing to Empowering</li> <li>Goals of Public Engagement</li> </ul> <p>Learning Objectives</p> <p>In this chapter, you will:</p> <ul> <li>Learn what is meant by the term \u2018public engagement\u2019.</li> <li>Identify and explore some of the different approaches to public engagement and the goals associated with them.</li> </ul>"},{"location":"skills-tracks/ped/chapter1/goals/","title":"Goals of Public Engagement","text":"<p>Before we explore some specific goals of public engagement, have a go at answering the following question on your own:</p> <p>Quote</p> <p>What are some goals of public engagement?</p> <p>In answering the above question, you may have come up with a valid goal of public engagement or, alternatively, a different framing for one of the subsequent goals that have been discussed in the relevant literature. Let's look at some of the most significant or influential goals.<sup>1</sup></p>"},{"location":"skills-tracks/ped/chapter1/goals/#improving-public-knowledge-and-awareness-of-science","title":"Improving Public Knowledge and Awareness of Science","text":"<p>This first, epistemic goal is often the most intuitive and familiar to scientific researchers and developers. In many cases this is due to institutional factors that promote uni-directional forms of engagement such as blog posts, news articles, or television and radio interviews. For many members of the public, these types of engagement can be both entertaining and informative when they are well produced.</p> <p>However, as already discussed, it is unclear whether this goal is an intrinsic good. That is, should the goal of improved public knowledge in science be treated as something valuable in and of itself, or is the value of improved scientific knowledge instrumental upon the practical benefits that this knowledge can bring?</p> <p>How we answer this question will affect how we evaluate this goal. On the one hand, if we think that improved knowledge is an instrumental good, then we may be sceptical about specific public education campaigns. For example, few users are likely to leverage technical knowledge about how object recognition algorithms work to deploy a system in their own home that can identify common household objects. But on the other hand, if we see improved knowledge as an intrinsic good, then any efforts to raise public awareness in science and technology should be treated as valuable.</p>"},{"location":"skills-tracks/ped/chapter1/goals/#public-deliberation","title":"Public Deliberation","text":"<p>This second goal is often associated with Jurgen Habermas\u2014a German philosopher and social theorist\u2014who developed the theory of discourse ethics and saw deliberation as</p> <p>Quote</p> <p>a time-consuming process of mutual enlightenment, for the \u2018general interest\u2019 on the basis of which alone a rational agreement between publicly competing opinions could freely be reached [@habermas1989]</p> <p>The general idea is that deliberation aims at consensus, such that the latter emerges from the sharing of public reasons and a drive towards mutual understanding between those engaged in dialogue. Here, consensus could involve a shared belief and agreement about the benefits or harms of some scientific or technological development.</p> <p>In practice, consensus does not have to involve unanimous agreement on a final outcome. Rather, it can also include acceptable domains or preferences and ranges of competing options, the credibility of disputed beliefs, and the legitimacy of competing values.[@dryzek2006] This avoids the misplaced criticism that deliberation aimed at consensus building results in the flattening of the range of options or covers up dissent and legitimate disagreement or difference.</p>"},{"location":"skills-tracks/ped/chapter1/goals/#establishing-trust-legitimacy-and-social-license","title":"Establishing Trust, Legitimacy, and Social License","text":"<p>Closely connected with the goal of consensus building or formation is the goal of establishing trust, legitimacy, and a social license.</p> <p>As a socially-embedded process, science and technology development can often depend on the support of members of the public. This is especially true where science and technology research and development is publicly funded and administered.</p> <p>A clear example of this is the recent case of contact-tracing apps used across the globe in response to the COVID-19 pandemic.[@leslie2020] Many formal healthcare organisations supported contact tracing because of its potential to support epidemiological research. However, the success of the public health programmes depended upon whether members of the public trusted the scientific process, including whether their personal data were handled in a responsible and ethical fashion.</p> <p>The choice of whether to use contact-tracing apps was, therefore, dependant upon a) how trustworthy the app was judged to be, b) whether the operator of the app had perceived legitimacy in the eyes of the public, and c) whether there was broad social license for the app in the first place.</p> <p>Public engagement was seen by many as crucial to establishing trust, legitimacy, and social license.[@aitken2020] Although this may be a valid goal, it is also important to note that like the first goal, it can be treated as an instrumentally valuable goal insofar as it supports subsequent goals, such as improving health outcomes or supporting epidemiological research.</p>"},{"location":"skills-tracks/ped/chapter1/goals/#improving-social-welfare","title":"Improving Social Welfare","text":"<p>Following on from the previous goal, we can also identify the goal of improved social welfare. It is often recognised that science and technology can improve many facets of society and people's lives. Cleaner air and water, improved health, more effective forms of communication, better governance and public policy\u2014all of these social goods can (and have) been improved by science and technology.</p> <p>Where local knowledge is required in order to realise these goals though, it is vital that members of the public (e.g. local communities) are able to participate in the scientific process.</p> <p>However, what constitutes improved 'social welfare' is itself a question that may require public engagement to satisfactorily address. For example, if a research team were exploring whether a drug could improve the health outcomes for a group of patients, they may need to work with these patients in order to understand which side effects or symptoms were most important to them when assessing if the drug had a positive impact on their health or well-being. Universal agreement on whether a medical intervention is a net positive in terms of health and well-being is not something that can easily be assumed.</p>"},{"location":"skills-tracks/ped/chapter1/goals/#safeguarding-and-supporting-human-rights","title":"Safeguarding and Supporting Human Rights","text":"<p>Consider the following text from the United Nation's Universal Declaration of Human Rights:[@nations2022]</p> <p>Article 27</p> <ol> <li>Everyone has the right freely to participate in the cultural life of the community, to enjoy the arts and to share in scientific advancement and its benefits.</li> <li>Everyone has the right to the protection of the moral and material interests resulting from any scientific, literary or artistic production of which he is the author.</li> </ol> <p></p> <p>The scope of human rights, and the obligations or duties that they impose upon various institutions is a lively area of debate among legal scholars and practitioners. However, when compared to other rights, Article 27 has received little attention.</p> <p>Although enshrined in a legal document, the manner in which these various articles translate into either practical safeguards against possible abuses or positive opportunities and capabilities for human flourishing is far from obvious.</p> <p>Citizen science is one way, among many, in which this right may be exercised. The term 'citizen science' is defined by Vayena and Tasioulas as,</p> <p>Quote</p> <p>[...] any form of active non-professional participation in science that goes beyond human subject research conducted by professional researchers.[@vayena2015]</p> <p>This inclusive definition makes room for myriad scientific activities, ranging from astronomy to zoology, or from self-tracking and experimentation through smart devices (e.g. wearables and smartphones) to participation in distributed forms of environment data collection and mapping<sup>2</sup>.</p> <p>The unprecedented technological means available to members of the public has undoubtedly played a significant role in increasing the participation in citizen science projects. However, the fact that access to science and technology remains unequally distributed throughout society suggests that technological means alone are insufficient for universally establishing the aforementioned human right. Therefore, it is worthwhile considering how such a goal could be better realised.</p> <p>In the next chapter we will look at the preconditions for effectively realising these goals, by exploring the values that support, underwrite, and motivate public engagement.</p> <ol> <li> <p>The astute reader will notice that these goals are not mutually exclusive nor exhaustive. For instance, mutual understanding can also be aimed at building trust.\u00a0\u21a9</p> </li> <li> <p>see the Colouring London project as one visually-appealing example of crowdsourced data-science.\u00a0\u21a9</p> </li> </ol>"},{"location":"skills-tracks/ped/chapter1/ladder/","title":"Climbing the Ladder: From Informing to Empowering","text":"<p>In 1969, Sherry Arnstein published 'A Ladder of Citizen Participation' [@arnstein1969]. The eponymous ladder was a typology of different forms of public participation in science and research, which was intended to be both \"provocative\" and also facilitate clearer dialogue on the objectives of different forms of public engagement<sup>1</sup>.</p> <p>Arnstein visualised her typology as a ladder to signify that the higher rungs represent increasing forms of \"citizen power\". In order from lowest to highest, the eight rungs are as follows:</p> <p>Arnstein's Ladder of Public Participation</p> <ol> <li>Manipulation</li> <li>Therapy</li> <li>Informing</li> <li>Consultation</li> <li>Placation</li> <li>Partnership</li> <li>Delegated Power</li> <li>Citizen Control</li> </ol> <p>With each step up the ladder, we move closer to a genuine form of public participation that seeks to empower citizens through distributed forms of knowledge production and enhanced capabilities. For instance, perhaps a local council sets out to engage and upskill residents to enable them to have greater control over how their data are used to improve local services.</p> <p>Such a project could represent any of the top three rungs, depending on how the project was designed and organised. Like the other rungs, this is because the top three rungs form a separate category that further delineates them from the lower levels. The categories are as follows:</p> <ul> <li>Nonparticipation (Manipulation, Therapy)</li> <li>These rungs describe forms of participation that have been \"contrived by some to substitute for genuine participation\". However, the goal of these non-participatory forms of public engagement are often to enable those in power (e.g., researchers) to \"educate\" participants.</li> <li>Tokenism (Informing, Consultation, Placation)</li> <li>These rungs afford participants a voice but only insofar as their views serve the interests of those who hear them. Participants still lack any real sense of power in such forms of engagement, as researchers still make the final decision based on pre-determined goals or values.</li> <li>Citizen Power (Partnership, Delegated Power, Citizen Control)</li> <li>The higher rungs empower participants to an increasing degree. Where members of the public can enter in partnerships with researchers, they will likely be granted autonomy over decisions. However, the extent to which this power is truly delegated or under the control of the participants may be limited at this level. Each step up the ladder from here represents increased power and autonomy over decision-making.</li> </ul> <p>Notice that even the lowest levels would, technically, constitute a form of engagement, while failing to count as genuine forms of participation.</p> <p>In the approximately 50 years since this article's publication many have engaged with Arnstein's original proposal, including critical perspectives that emphasise the model's limitations. The ladder is, after all, a simplification\u2014like all heuristic models. As such, it fails to capture variations across policy or research domains, such as education versus healthcare, where participants may not wish or be able to exercise power or autonomy over decisions but could benefit from other forms of empowerment. In addition, the model is unable to provide insights into what can be done to rebalance, rather than outsource, power in specific contexts.</p> <p>Nevertheless, in spite of these limitations, the model that Arnstein developed has been highly influential. For instance, NHS England have a simplified form of Arnstein's ladder, which is better suited to help structure patient and public involvement or activities (e.g., policy decisions).</p> <p></p> <p>Although we will not use the rungs of the ladder as conceptual reference points in this course, the ethical and social significance of public empowerment will be a recurring theme in this course. However, it is not the only theme that we will consider, and Arnstein's ladder is not the only model that is worth considering.</p>"},{"location":"skills-tracks/ped/chapter1/ladder/#other-models-of-public-engagement","title":"Other Models of Public Engagement","text":"<p>Arnstein's ladder draws our attention to the ethical and social value of public empowerment. This is arguably a social good, but is not an unconditional good. That is, there may be cases where increased citizen control is neither desirable nor appropriate (e.g. when working with vulnerable groups, such as those with severe mental health disabilities).</p> <p>As conscientious and responsible researchers, however, we may still wish to do better than mere tokenism and also avoid complete non-participation. In such cases, it is important to have a clear understanding of what our goal for public engagement ought to be. In an article reviewing different approaches to science and technology communication, Bruce Lewinstein identifies four models, which can help us to better understand the goals of public engagement [@lewenstein2003]:</p> <ul> <li>The Deficit Model</li> <li>The Contextual Model</li> <li>The Lay Expertise Model</li> <li>The Public Participation Model</li> </ul>"},{"location":"skills-tracks/ped/chapter1/ladder/#the-deficit-model","title":"The Deficit Model","text":"<p>The first of these models reflects an assumption that public awareness of and knowledge about science and technology is, in general, poor. Public surveys are often cited in support of such a view, including claims such as the following:</p> <p>Quote</p> <p>[...] only 10 percent of Americans can define \"molecule,\" and that more than half believe that humans and dinosaurs lived on the Earth at the same time.</p> <p>From this starting point, those who adhere to the deficit model presume that attempts to improve public knowledge are invariably a good thing.</p> <p>But are they?</p> <p>Why, for example, do members of the public need to know that the luminosity of the Andromeda galaxy is \\(~2.6\u00d710^{10}\\) \\(L_\u2609\\), or that variational autoencoders are popular types of generative models in artificial intelligence? Moreover, why should the absence of this knowledge be treated as a deficit when it is likely to have no application in the daily lives of members of the public?</p>"},{"location":"skills-tracks/ped/chapter1/ladder/#the-contextual-model","title":"The Contextual Model","text":"<p>The way we respond to information differs depending on the context in which it is presented. Our personality type, for example, may affect how we perceive and evaluate risk. Social and cultural attitudes affect levels of trust in scientific authority. And, the media can play a substantial role in shaping our interest in science and technology\u2014Carl Sagan, for example, was highly renowned for the passion and excitement he was able to generate in often complex or dry scientific topics.</p> <p>The second model Lewinstein describes\u2014the contextual model\u2014therefore, recognises the need for tailoring information to specific audiences. This helps to address the deficit model's own deficit, as describe above. However, we may think that this revised model is just a more sophisticated version of the deficit model\u2014old wine in a new bottle! After all, there is still a presumed gap in understanding, but it needs to be addressed in a specific way.</p> <p>What these two models share is their presumed equivalence of 'public understanding of science and technology' with 'public appreciation of the benefits provided by science and technology to society'. In other words, the more that members of the public understand about science and technology the more they will come to appreciate its benefits.</p> <p>This may be true in some instances. And so the goal of enhanced literacy ought to be recognised as a valid objective for public engagement. However, public engagement under these two models reduces engagement to a uni-directional form of communication in which the goal of the scientist is simply to inform and educate. It is for this reason that such forms of engagement fall under the 'non-participatory' category of Arnstein's ladder.</p>"},{"location":"skills-tracks/ped/chapter1/ladder/#the-lay-expertise-model","title":"The Lay Expertise Model","text":"<p>Turning the spotlight of attention back onto the scientific communities, a further concern with the previous two models that Lewinstein identifies is that they do not adequately address the social or political contexts in which science and technology are developed, including the conflicts of interest that scientific pursuits may have with local communities or expertise (e.g. labour groups).</p> <p>Far from being the sole arbiters of knowledge and expertise, scientists often fall prey to their own biases or limited perspective (i.e. positionality) and overlook diverse forms of knowledge that are rooted in local communities and practices (e.g. agriculture). Such local knowledge, including the data flows interconnected with the embedded processes of local knowledge production, may be as relevant (or even more valuable) than \"technical\" forms when attempting to address (or redefine) some problem.</p> <p>In cases where knowledge resides in local practices and expertise, public engagement may benefit from meaningful partnerships, leading to complementary and mutually enhancing forms of knowledge. A pragmatic goal of such endeavours, therefore, is to solve problems. However, in pursuing such a goal scientists must remain vigilant that their goal and formulation of the problem is not misaligned with the goal of the communities with whom they partner.</p>"},{"location":"skills-tracks/ped/chapter1/ladder/#the-public-participation-model","title":"The Public Participation Model","text":"<p>Lewinstein's final model brings us back to Arnstein's ladder. However, the topics that Lewinstein addresses go beyond empowerment to include additional benefits such as heightened public trust and consensus formation.</p> <p>There are myriad activities that can support these objectives, including citizen juries, consensus building workshops, and deliberative polling. Therefore, this model also seeks to identify means for democratising science and technology in order to wrest control of research and innovation away from elite institutions and politicians.</p> <p>Once we understand these different activities, it becomes clear that public participation is not about outsourcing decision-making authority to the public. In other words, the people that participate are not making decisions on behalf of researchers, but their voices should be listened to insofar as they are impacted by the consequences of a research or innovation project.</p> <p>This is because scientific research and the design, development, and deployment of technology embody particular values that may not be shared by those who are directly or indirectly affected by the consequences of such activities. Obvious examples include military science and technology. Can you think of others?</p> <p>The field of science and technology studies has been critically examining scientific research and practice for several decades, exposing many latent value structures and biases in the process.<sup>2</sup> An awareness of such structures and biases is vital to understanding the goals of public engagement. We will explore this value laden nature of science and technology in relation to public engagement later. However, with this additional understanding of public engagement, let us now take a closer look at the goals of objectives of public engagement.</p> <p>Who (or what) are the \"Public\"?</p> <p>Before going any further, we should also stop to address a conceptual point about what the concept 'the Public' actually references. It is easy to fall into the trap of treating \"the public\" as some monolithic or homogenous entity, because of how its usage as a mass noun affects our judgement. However, as we all know, the public are a wonderfully diverse set of people, communities, and groups, of which we are a part. The more we can keep this in the forefront of our minds going forward, the less likely we will be to fall prey to myopic assumptions about public engagement.</p> <ol> <li> <p>A note on the terms 'participation' and 'engagement': in some cases these terms can be used interchangeably. For instance, public participation is a form of engagement. However, the reverse is not always true, as Arnstein's ladder demonstrates. For instance, informing members of the public about novel scientific results or findings through print communication is best seen as engagement, rather than than participation. This is because the reader is unable to participate in the scientific process, even if they have been engaged in the later stages (i.e. scientific communication). Throughout this guide, we will default to the use of 'engagement' where an inclusive usage is appropriate.\u00a0\u21a9</p> </li> <li> <p>For an overview of key developments in science and technology studies, see this timeline \u21a9</p> </li> </ol>"},{"location":"skills-tracks/ped/chapter2/","title":"The Value(s) of Public Engagement","text":""},{"location":"skills-tracks/ped/chapter2/#introduction","title":"Introduction","text":"<p>In the first chapter we explored and considered answers to the following questions:</p> <p>Questions</p> <ol> <li>What is public engagement?</li> <li>What are the goals of public engagement?</li> </ol> <p>In discussing possible answers we took for granted the underlying values that support and motivate different forms of public engagement. For instance, values like shared understanding that are enacted through activities such as democratic deliberation. These values matter, and so we could also ask two additional questions:</p> <p>Additional Questions</p> <ol> <li>What are the different values that support and motivate public engagement?</li> <li>What does it mean to conduct responsible public engagement for data science and AI?</li> </ol> <p>Addressing these questions will be the main focus of this chapter. We will start with the first question to further our understanding of the social significance and importance of public engagement, as well as to ground the subsequent discussion of what constitutes 'responsible public engagement'.</p>"},{"location":"skills-tracks/ped/chapter2/#chapter-outline","title":"Chapter Outline","text":"<ul> <li>Deliberative Values</li> <li>Responsible Public Engagement in Data Science and AI</li> </ul> <p>Learning Objectives</p> <p>This chapter is designed to help you identify and evaluate the different ethical and social values that are implicated in all public engagement activities. By the end of the chapter you should have a better understanding of why these values matter, and also be able to use the SAFE-D principles to assess whether a particular research or innovation project is responsible.</p>"},{"location":"skills-tracks/ped/chapter2/deliberation/","title":"Deliberative Values","text":"<p>Let's begin with a simple definition of 'deliberation':</p> <p>Quote</p> <p>We define deliberation minimally to mean mutual communication that involves weighing and reflecting on preferences, values, and interests regarding matters of common concern.[@bachtiger2018] (emphasis added)</p> <p>As defined, deliberation is separate to (but consistent with) processes such as decision-making, polling, and voting\u2014processes that are common components of public engagement. However, deliberation is a process that aims at mutual understanding and building consensus, prior to decision-making. As such, deliberative forms of engagement place an emphasis on the communicative value inherent in deliberation as a form of dialogue (or, \"mutual communication\"), instead of the practical value of decision-making. An illustrative example can help explain why this distinction matters:</p> <p>Conflicting Preferences</p> <p>A team of social research scientists working with the local government is undertaking research into public attitudes concerning the environmental impact of potential traffic policies. They have decided to engage a representative group of local residents to explore a range of policies under consideration, including the following:</p> <ol> <li>The local government would like to increase investment in access to electric vehicle charging points. However, because they have limited resources available, they are only able to deploy 100 charging points in five areas with the highest number of electric vehicles, while also creating a central hub used by those visiting the city centre.</li> <li>The local government would like to use automated number plate recognition on residential roads that are commonly used by commuters to bypass traffic on busier routes. This policy is supported by a strong body of empirical evidence that shows it can discourage non-essential journeys and also helps ease congestion in parts of the city, in turn lowering pollution. </li> </ol> <p>Consider what would happen if the research team simply choose to ask the residents to vote on whether these policies should be adopted. It's possible that they would both be approved by a majority who see them as positive investments in renewable energy and the environment. But upon deeper reflection, the policies could be quite divisive.</p> <p>The first policy, for instance, could be seen as problematic because of its impact upon socionecomic inequality. That is, the policy would likely favour already affluent neighbourhoods because of the prioritisation of areas that have the \"highest number of electric vehicles\". In turn, limited resources would be diverted from already disadvantaged communities, further widening socioeconomic inequality.</p> <p>In addition, the second policy could split the group according to whether they rely heavily on roads (e.g. for commuting or work) or prefer quieter streets because they are able to work remotely. Despite the positive environmental impact, there would likely be a significant level of inconvenience and disruption to some groups.</p> <p>Focusing solely on the positive value of environmental impact, therefore, would overlook additional values of fairness and welfare. In both cases, following deliberation, it is unlikely that such policies would receive unanimous approval or disapproval. However, through deliberation, a space could be created to allow all voices to be heard and for conflicting values to emerge. The manner in which these conflicts are handled depends, ultimately, on how the engagement activity is designed.</p> <p>One option would be to forego any voting or polling entirely, in favour of additional research, consultation, and public engagement (e.g. soliciting additional views). We will return to these topics in a later chapter.</p> <p>For the time being, it is sufficient to simply note that public deliberation about the means and ends of scientific and technological innovation, such as the implementation of research and innovation through public policy as above, need not be oriented solely towards practical decision-making. Instead, by focusing on the values inherent to communication and deliberation, we are able to identify and emphasise particular values that ought to guide deliberation in a democratic system.</p>"},{"location":"skills-tracks/ped/chapter2/deliberation/#identifying-values-for-effective-deliberation","title":"Identifying Values for Effective Deliberation","text":"<p>In reviewing the literature on such values, Bachtiger et al.[@bachtiger2018] identify two generations of standards for good deliberation\u2014the latter of which have grown out of and developed upon the first.</p> First Generation Second Generation Respect Unrevised Absence of power Unrevised Equality Inclusion, mutual respect, equal communicative freedoms, equal opportunity for influence Reasons Relevant considerations Aim at consensus Aim at both consensus and clarifying conflict Common good orientation Orentation to both common good and self-interest constrained by fairness Publicity Publicity in many conditions, but not all (e.g., in negotiations when representatives can be trusted) Accountability Accountability to constituents when elected, to other participants and citizens when not elected Sincerity Sincerity in matters of importance; allowable insincerity in greetings, compliments, and other communications intended to increase sociality <p>Rather than reviewing each of these, we'll just consider two with an eye towards understanding the values expressed by such standards.</p> <p>First, 'absence of power' is a standard that serves as a precondition for values such as equality, diversity, and inclusivity (EDI). For example, the imbalance of power that exists between experts and members of the public can create a situation in which the latter feel uncomfortable expressing their honest thoughts or opinions for fear of being wrong. Similarly, deliberation within research teams can be impacted by unequal power relations between senior researchers and graduate students, preventing the emergence and exploration of novel ideas.</p> <p>Second, aiming at both consensus and clarifying conflict reinforces the inherent value of public communication and mutual understanding, offsetting the prioritisation of more instrumental values such as decision-making. In the case of our two policies above, for example, aiming at consensus, even if it cannot be reached unanimously, can at least create a situation in which the different voices in the debate are better able to appreciate what matters to other members of their community. This is an often under-appreciated value by research teams, who are focused on influencing policy or pursuing research objectives. However, deliberation that upholds the standards above can also increase trust and respect between communities (e.g. scientists and the public)\u2014an additional goal that we considered in the previous chapter.</p> <p>Why EDI matters?</p> <p>Equality, diversity, and inclusivity (EDI) as a collection of values has received widespread and growing attention in recent years. There are many reasons why this increased focus is both significant and vital. These reasons are typically contextual, and reflect moral priorities of the domains in which the values emerge (e.g. STEM research).</p> <p>In the context of public engagement, equal, diverse, and inclusive deliberation helps support a public that</p> <ol> <li>reflectively recognises shared needs and interests,</li> <li>produces the best solutions to the problems involved in meeting those needs and interests,</li> <li>brings individual into a close and fulfilling relationship with a community.[@chambers2018] </li> </ol> <p>As with the remaining standards identified by Bachtiger et al.,[@bachtiger2018] the above two express important values that are enacted through effective deliberation and public engagement. However, there are myriad barriers that can prevent such values from being realised. These include:</p> <ul> <li>Unbalanced relationships of power</li> <li>Increased polarisation that impedes consensus building</li> <li>Temporal constraints</li> <li>Cognitive biases</li> </ul> <p>The challenge for any researcher is, therefore, 'how to navigate these barriers and pursue value-based objectives of effective public engagement'. This is particularly challenging when the barriers originate, in part, because of competing research priorities. For instance, the priority of ensuring a sufficient sense of urgency on the completion of research deliverables can disincentivise lengthy deliberative activities.</p> <p>It is worth noting, therefore, that the standards and values identified above are normative ideals. That is, they are motivational and aspirational principles for good deliberation but may not be obtainable in practice {% cite bachtiger2018 %}.</p> <p>Values for Additional Forms of Public Engagement (Activity 2.1)</p> <p>We have considered values associated with the goal of democratic deliberation aimed at consensus building and mutual understanding. However, there are other goals for public engagement, as we saw in the previous chapter:</p> <ul> <li>Improved Public Awareness of Science and Technology</li> <li>Establishing Trust, Legitimacy, and Social License</li> <li>Improving Social Welfare</li> <li>Safeguarding and Supporting Human Rights</li> </ul> <p>What additional values (or standards) do you associate with these additional goals of public engagement?</p>"},{"location":"skills-tracks/ped/chapter2/deliberation/#engagement-in-principle-vs-engagement-in-practice","title":"Engagement in Principle vs. Engagement in Practice","text":"<p>Consider, the principle \"ensure all individuals can participate equally\", which safeguards vital social values of justice and inclusivity. While laudable as a principle, it is, of course, impossible to achieve in practice.</p> <p>There are simply too many cognitive biases or social inequalities that affect our ability to participate on an equal footing. Higher levels of educational ability, natural charisma, and lived experience can all impart advantages that separate participants based on their capacity for participation. Two points can be raised in response to this challenge.</p> <p>First, as researchers, it is always important to ensure that an ideal does not end up as the enemy of the good. Values are often expressed as ethical principles that prescribe what we ought to do under ideal circumstances (e.g. as rational agents with perfect information). While potentially valid as evaluative criteria (i.e. as post hoc means for determining what is right or wrong after the fact), they often fail to translate well as decision procedures. Nevertheless, they can still serve action-guiding roles in steering us towards ethical actions or behaviours.</p> <p>Second, as Bachtiger et al.[@bachtiger2018] acknowledge, there are often good reasons to strive harder in pursuit of some values over others. This allows a team to prioritise values that are particularly relevant for their project (e.g. those that matter to their stakeholder or affected users). For instance, perhaps a team of epidemiologists are concerned with understanding the social and environmental risk factors associated with a specific disease that disproportionately affects those from disadvantaged backgrounds. In this case, the research team would be justified in prioritising values that promote health equity or amplify the voices of marginalised stakeholders, even if pursuing such values is to the neglect of others.  </p> <p>There is, ultimately, no simple procedure or algorithm for choosing between values. We will explore practical steps and procedures that can help with this often challenging process in later chapters, but the point to keep in mind at present is that in designing engagement activities or carrying out scientific or technological research and development you will always be making a choice between certain values. Having an awareness and understanding of these values will at least enable you to do so in an informed and principled manner.</p>"},{"location":"skills-tracks/ped/chapter2/responsible/","title":"Responsible Public Engagement in Data Science and AI","text":"<p>Quote</p> <p>The public have to live with the consequences of science and technology. As such, RRI is above all \"an opportunity for truly collective stewardship of our highly technologized future.\" (Sebastian Pfotenhauer quoted in Pain, 2017)[@pain2017]</p> <p>The above quote helps to motivate a need to consider our responsibility, as researchers, to the societal impact of science and technology. The practical tools and methods we will explore in subsequent chapters will help us meet our various obligations. However, before we shift to the practical considerations, there is one final question to consider:</p> <p>Question</p> <p>What does it mean to conduct responsible public engagement in data science and AI?</p> <p>Responsible research and innovation is increasingly important in science and technology. As a term, 'responsible research and innovation' (RRI) is most strongly associated with the European Commission's Framework Programmes for Research and Technological Development\u2014a set of funding programmes that support research in the European Union. Beginning with the seventh framework programme in 2010, and continuing on through Horizon 2020 (FP8), the term 'responsible research and innovation' became increasingly important for the European Commission's policy.</p> <p>Since then, other national funding bodies have also shown a commitment to RRI. For example, UKRI's Engineering and Physical Sciences Research Council have developed the AREA framework, which sets out four principles for RRI: Anticipate, Reflect, Engage, and Act (AREA).</p> <p>AREA Principles</p> <p>Anticipate - Describe and analyse the impacts, intended or otherwise, that might arise. Do not seek to predict but rather support the exploration of possible impacts (such as economic, social and environmental) and implications that may otherwise remain uncovered and little discussed.</p> <p>Reflect - Reflect on the purposes of, motivations for and potential implications of the research, together with the associated uncertainties, areas of ignorance, assumptions, framings, questions, dilemmas and social transformations these may bring.</p> <p>Engage - Open up such visions, impacts and questioning to broader deliberation, dialogue, engagement and debate in an inclusive way.</p> <p>Act - Use these processes to influence the direction and trajectory of the research and innovation process itself.</p> <p>However, research and innovation in data science and AI is unlike traditional scientific research or technological innovation in many ways. For example, the following considerations are pertinent and relate to the impact upon public engagement specifically:</p> <ul> <li>Effective public communication requires sufficient levels of data and digital literacy (e.g. understanding of statistics, data visualisation)</li> <li>Conceptual knowledge and understanding of relevant concepts such as 'autonomous and adaptive behaviour', 'intelligence', or ' algorithmic system' may vary widely, such that different stakeholders may not be talking about the same thing</li> <li>Participants will hold differing opinions on key topics, such as data privacy and protection, which could give rise to challenging attitudes such as algorithmic aversion.<sup>1</sup></li> <li>Values and norms concerning the impact of novel data-driven technologies are still in flux, as they continue to affect different areas of society. Therefore, public attitudes may also vary, perhaps drastically, over time.</li> </ul> <p>As such, legitimate doubts could be raised about how universally these principles apply, and also whether they extend to considerations about public engagement?</p> <p>In response to these doubts, a justifiable argument could be made that some principles of RRI are more directly relevant to some of the applied sciences than more theoretical disciplines (e.g. fundamental physics).</p> <p>Two comments can be made:</p> <ol> <li>We should exercise caution in assenting to such a belief too rapidly given the many examples in the history of science that show how unintended consequences can arise as a result of advances in some scientific discipline that even experts could not foresee.</li> <li>Although principles of RRI may apply more strongly in some areas of application or research, there are also principles (e.g. FAIR principles) that support responsible public engagement that ought to apply universally (e.g. research integrity, promotion of equality, diversity, and inclusivity among researchers).</li> </ol> <p>Question</p> <p>How else does public engagement in data science and AI differ from public engagement in traditional scientific research or technological innovation?</p>"},{"location":"skills-tracks/ped/chapter2/responsible/#the-burden-of-responsibility","title":"The Burden of Responsibility","text":"<p>Exercising responsibility can be demanding. In the context of public engagement for research this is made more demanding by the existence of several barriers:</p> <ul> <li>High levels of competition for research funding</li> <li>Secrecy among project teams</li> <li>Temporary (fixed-term) contracts for researchers</li> <li>Time pressures</li> </ul> <p>Although some of the tools and methods we will consider in later chapters can help alleviate some of these burdens, it would be naive to pretend that a wider institutional change in attitudes and incentive structures is not also necessary.<sup>2</sup></p> <p>On top of these barriers, public engagement in data science and AI is not always recognised as valuable for the furthering of one's scientific career by some specialists. However, consider the following quotation:</p> <p>Quote</p> <p>The lack of diverse jobs after a PhD, a shrinking funding budget and lack of public support all seem daunting to graduate students. In the face of these major hurdles, many scientists isolate themselves from these issues and focus on their own research. A recent study has shown, however, that increased interaction with reporters and the more Twitter mentions a study receives correlate with a higher h-index of the author, a metric for measuring the scientific impact of a publication. To determine whether media coverage of a published science article is causative of increased citations, a 1991 study looked at journal articles that would have been covered by the New York Times, but due to a writer\u2019s strike in the late 1970s, were not. Researchers found that the journal articles that were not covered had consistently less citations than other research articles covered by the Times. There is clear value for the science research community to publicise new findings to the public.[@pham2016]</p> <p>While this is only one form of engagement (i.e. communication) it serves as a pragmatic reason for pursuing public engagement. However, there are also ethical reasons for pursuing public engagement in data science, in spite of the aforementioned barriers.</p> <p>With these considerations in mind, let us now return to the original question, 'what does it mean to conduct responsible public engagement in data science and AI?' Although some of the principles alluded to above may also apply to responsible engagement in data science and AI (e.g. AREA principles), there are more specific principles that can help us with planning and delivering public engagement activities. We call these the SAFE-D principles.</p>"},{"location":"skills-tracks/ped/chapter2/responsible/#safe-d-principles","title":"SAFE-D Principles","text":"<p>We cover the SAFE-D principles in greater detail in our our course on responsible research and innovation. Here, it will suffice to simply summarise them and explain how they can support responsible public engagement through some illustrative examples. We will return to some of the examples in later activities.</p> <p>In general, the SAFE-D principles are designed to support responsible project governance in data science and AI research and innovation. Insofar as public engagement forms a key part of this research and innovation, the SAFE-D principles serve as useful ethical reflection points to help design responsible public engagement activities.</p>"},{"location":"skills-tracks/ped/chapter2/responsible/#sustainability","title":"Sustainability","text":"<p>Sustainability can mean a couple of things. From a technical perspective, sustainability requires the outputs of a project to be safe, secure, robust, and reliable. For example, if an organisation is developing an autonomous vehicle, it should operate safely in the intended context of use. However, in the context of responsible data science and AI, there is also a social sustainability component. This aspect of sustainability requires a project\u2019s practices and outputs to be informed a) by ongoing consideration of the risks of harm to individuals and society, even after the system has been deployed and the project completed\u2014a long-term (or sustainable) safety\u2014but also b) by the shared values that motivate the pursuit of social goals.</p> <p>The final point in the above summary is significant for public engagement, and connects with the following question:</p> <p>Question</p> <p>Who Designs the Future?</p> <p>As society continues to stagger forward feeling the ongoing effects of the global pandemic, while struggling with the increasing complexity and uncertainty that a modern, data-driven society poses, the ability for particular voices to participate in imagining a collective vision for the future becomes increasingly difficult. This can lead to a growing distrust in vital social institutions as people become alienated from society.</p> <p>In response, organisations like Nesta in the UK are carrying out public engagement activities focused on building \"participatory futures\", which aim to include marginalised or systematically excluded voices in these conversations. Public engagement activities and events such as these can play a vital role in supporting sustainable research and innovation.</p>"},{"location":"skills-tracks/ped/chapter2/responsible/#accountability","title":"Accountability","text":"<p>In contrast to responsibility, accountability is often seen as a backwards-looking process of holding an individual or organisation to account. It is commonly associated with regulation and governance, but in the context of data science or AI research and innovation, it goes beyond these matters.</p> <p>In this context, 'accountability' refers to the transparency of processes and associated outcomes that enable people to understand how a project was conducted (e.g., project documentation), or why a specific decision was reached. But it can also refer to broader processes of responsible project governance that seek to establish clear roles of responsibility where full transparency may be inappropriate (e.g., confidential projects).</p> <p>Without this broader, end-to-end awareness of accountability throughout a project's lifecycle, it is possible that harms could arise such that specific individuals are stripped of their ability to seek redress for their grievances or violations of certain rights.</p> <p>Responsible public engagement, therefore, can serve as a way to ensure that a project and its associated outputs remain sufficiently accountable, by ensuring that all affected stakeholders are able to voice potential concerns. The following quote is a nice reflection point for the values contained within this principle:</p> <p>Quote</p> <p>Sometimes we want everyone\u2019s voice to be heard because we think that will make a better decision as a result, and sometimes we want everyone\u2019s voice to be heard simply because we think that everyone has a right to be heard.[@macgilvray2014]</p>"},{"location":"skills-tracks/ped/chapter2/responsible/#fairness","title":"Fairness","text":"<p>Consider the following statistic:</p> <p>Quote</p> <p>15 per cent of scientists come from working class backgrounds; and in the US, children from the top 1 per cent of richest families (by income) are ten times as likely to have filed for a patent as those from families in the bottom half of the income distribution.[@saunders2018]</p> <p>While much has been done in the past decade to improve access to science and technology, there is, of course, still a lot more that needs to be done to ensure all individuals have an equal opportunity to participate in a core part of society. This is, ultimately, a matter of fairness.</p> <p>Fairness is inseparably connected with legal conceptions of equality and justice. In the context of data science or AI, this often leads to an emphasis on features such as non-discrimination, equitable outcomes of automated decisions, or procedural fairness through bias mitigation. However, these notions serve only a subset of broader ethical considerations pertaining to social justice, socioeconomic capabilities, diversity and inclusivity.</p> <p>As the above quote acknowledges, fairness is also a matter of addressing the socioeconomic barriers that prevent or make it difficult for certain individuals or communities to participate. Responsible public engagement, therefore, can help researchers or developers identify relevant barriers and work with affected people to co-design ways to improve capabilities.</p>"},{"location":"skills-tracks/ped/chapter2/responsible/#explainability","title":"Explainability","text":"<p>Explainability has received widespread attention in recent years from many within the AI community. This is because it is a key requirement for autonomous and informed decision-making in situations where data-driven systems interact with or influence human judgement and choice behaviour. For example, if an autonomous vehicle is involved in an accident or if a algorithmic system fails to recognise a fault in vital energy infrastructure, the reasons for these unexpected and undesirable behaviours needs to be identified.</p> <p>As such, there needs to be a) interpretability of the system itself (e.g., an ability to make sense of the underlying logic by which a system reaches a decision) and b) the ability to translate this information into a variety of explanations that are acceptable to relevant users and stakeholders. On this second point, public engagement can support evaluation and determination of both the accessibility and usability of different explanations, as well as providing input on possible trade-offs between, say, the interpretability and accuracy of possible system types (e.g., decision trees versus neural networks).<sup>3</sup></p>"},{"location":"skills-tracks/ped/chapter2/responsible/#data-quality-integrity-privacy-and-protection","title":"Data Quality, Integrity, Privacy and Protection","text":"<p>Data quality, integrity, protection and privacy must all be established to be confident that a research or innovation project has been designed, developed, and deployed in a responsible manner.</p> <ul> <li>\u2018Data Quality\u2019 captures the static properties of data, such as whether they are a) relevant to and representative of the domain and use context, b) balanced and complete in terms of how well the dataset represents the underlying data generating process, and c) up-to-date and accurate as required by the project.</li> <li>\u2018Data Integrity\u2019 refers to more dynamic properties of data stewardship, such as how a dataset evolves over the course of a project lifecycle. In this manner, data integrity requires a) contemporaneous and attributable records from the start of a project (e.g., process logs; research statements), b) ensuring consistent and verifiable means of data analysis or processing during development, and c) taking steps to establish findable, accessible, interoperable, and reusable records towards the end of a project\u2019s lifecycle.</li> <li>\u2018Data protection and privacy\u2019 reflect ongoing developments and priorities as set out in relevant legislation and regulation of data practices as they pertain to fundamental rights and freedoms, democracy, and the rule of law. For example, the right for data subjects to have inaccurate personal data rectified or erased.</li> </ul> <p>These final properties are, typically, more technical than the others (e.g., requiring legal expertise to determine if a project violates any data protection laws). However, public engagement can help determine whether a project that is legal is also acceptable from a moral perspective.</p> <p>For example, if a research project uses a basis of informed consent to collect and process personal data, are the participants able to opt-out at a later stage and are their processes to make this decision easy and accessible for participants. Whether this is the case will, invariably, be a matter of working with participants to determine.</p> <ol> <li> <p>Algorithmic aversion refers to the reluctance of human agents to incorporate algorithmic tools as part of their decision-making processes due to misaligned expectations of the algorithm\u2019s performance (see Burton et al. 2020).\u00a0\u21a9</p> </li> <li> <p>There are increasing signs of reform in the funding landscape, however, with more funding bodies favouring applications that demonstrate commitments to responsible research and innovation or include early career researchers as co-investigators.\u00a0\u21a9</p> </li> <li> <p>For more on the explainability/accuracy trade-off see our module on explainability.\u00a0\u21a9</p> </li> </ol>"},{"location":"skills-tracks/ped/chapter3/","title":"Facilitating Public Engagement for Data Science and AI","text":""},{"location":"skills-tracks/ped/chapter3/#introduction","title":"Introduction","text":"<p>In the previous chapters we discussed the following topics:</p> <p>Summary</p> <ol> <li>What are the goals and values of public engagement?</li> <li>What does it mean to conduct responsible public engagement for data science and AI?</li> </ol> <p>With the conceptual foundations laid, we now turn towards the more practical side of public engagement.</p> <p>This chapter begins by reviewing two frameworks that can help us locate activities and methods for public engagement within a broader project lifecycle for data science and AI. The objective is to use these models to support informed answers to the following questions:</p> <ul> <li>When should you engage?</li> <li>How should you engage?</li> </ul>"},{"location":"skills-tracks/ped/chapter3/#chapter-outline","title":"Chapter Outline","text":"<ul> <li>When should you engage?</li> <li>How should you engage?</li> </ul> <p>Learning Objectives</p> <p>By the end of this chapter you should have a strong comprehension of a) a stakeholder analysis process, b) a model of a typical data science or AI project lifecycle, and c) a variety of methods for public engagement. Collectively, these will enable you to scaffold your own project planning activities to help answer</p> <ul> <li>How should you engage?</li> <li>When should you engage? </li> </ul>"},{"location":"skills-tracks/ped/chapter3/how/","title":"How should you engage?","text":"<p>There are many different methods for engagement in data science and AI. If you recall the discussion from chapter 1, these range from the one-sided processes of 'informing' to the more empowering types of engagement that strengthen the capabilities of the public and enable them to participate in and contribute to more diverse forms of science and technology.</p> <p>The following table summarises a wide range of salient methods:</p> Mode of Engagement Description Degree of Engagement Practical Strengths Practical Weaknesses Newsletters (email) Regular emails (e.g.: fortnightly or monthly) that contain updates, relevant news, and calls to action in an inviting format. <code>INFORM</code> Can reach many people; can contain large amount of relevant information; can be made accessible and visually engaging. Might not reach certain portions of the population; can be demanding to design and produce with some periodicity; easily forwarded to spam/junk folders without project team knowing (leading to overinflated readership stats). Letters (post) Regular letters (e.g.: monthly) that contain the latest updates, relevant news and calls to action. <code>INFORM</code> Can reach parts of the population with no internet or digital access; can contain large amount of relevant information; can be made accessible and visually engaging. Might not engage certain portions of the population; Slow delivery and interaction times hampers the effective flow of information and the organisation of further engagement. App notifications Projects can rely on the design of apps that are pitched to stakeholders who are notified on their phone with relevant updates. <code>INFORM</code> Easy and cost-effective to distribute information to large numbers of people; Rapid information flows bolster the provision of relevant and timely news and updates. More significant initial investment in developing an app; will not be available to people without smartphones. Community fora Events in which panels of experts share their knowledge on issues and then stakeholders can ask questions. <code>INFORM</code> Can inform people with more relevant information by providing them with the opportunity to ask questions; brings community together in a shared space of public communication. More time-consuming and resource intensive to organise; might attract smaller numbers of people and self-selecting groups rather than representative subsets of the population; effectiveness is constrained by forum capacity. Online Surveys Survey sent via email, embedded in a website, shared via social media, etc. <code>CONSULT</code> Cost-effective; simple mass- distribution. Risk of pre-emptive evaluative framework when designing questions; Does not reach those without internet connection or computer/smartphone access. Phone Interviews Structured or semi-structured interviews held over the phone. <code>CONSULT</code> <code>PARTNER</code> Opportunity for stakeholders to voice concerns more openly. Risk of pre-emptive evaluative framework when designing questions; Might exclude portions of the populations without phone access or with habits of infrequent phone use. Door-to-door interviews Structured or semi-structured interviews held in-person at people\u2019s houses. <code>CONSULT</code> <code>PARTNER</code> Opportunity for stakeholders to voice concerns more openly; can allow participants the opportunity to form connections through empathy and face-to- face communication. Potential for limited interest to engage with interviewers; time-consuming; can be seen by interviewees as intrusive or burdensome. :fontawesome-solid-people-arrows-left-right: In-person interviews Short interviews conducted in- person in public spaces. <code>CONSULT</code> <code>PARTNER</code> Can reach many people and a representative subset of the population if stakeholders are appropriately defined and sortition is used. Less targeted; pertinent stakeholders must be identified by area; little time/interest to engage with interviewer; can be viewed by interviewees as time- consuming and burdensome. Focus Groups A group of stakeholders brought together and asked their opinions on a particular issue. Can be more or less formally structured. <code>CONSULT</code> <code>PARTNER</code> Can gather in-depth information; Can lead to new insights and directions that were not anticipated by the project team. Subject to hazards of group think or peer pressure; complex to facilitate; can be steered by dynamics of differential power among participants. Online Workshops Workshops using digital tools such as collaborative platforms. <code>CONSULT</code> Opportunity to reach stakeholders across regions, increased accessibility depending on digital access. Potential barriers to accessing tools required for participation, potential for disengagement. Crowdsourcing (Online) Well-designed tasks that can be undertaken by a distributed collective, with individuals working on separate components. <code>CONSULT</code> <code>PARTNER</code> Opportunity to engage stakeholders across regions, in an asynchronous manner, with increased accessibility depending on digital access. Supports increased potential for diverse forms of expertise and experience. Can be misused as a method for outsourcing cheap labour; potential barriers to accessing tools required for participation; potential for disengagement; difficult to ensure accuracy and validity of input from participants. Distributed Project Collaboration (Online) Online digital platforms, such as GitHub, enable new forms of citizen science and collaborative development on diverse projects (e.g., open source software, open science). <code>CONSULT</code> <code>PARTNER</code> <code>EMPOWER</code> Opportunity to engage stakeholders across regions, in an asynchronous manner, with increased accessibility depending on digital access; increased potential for diverse forms of expertise and experience; empowers new communities to actively participate in shaping and building tools that have real value for their communities. Potential barriers to accessing digital tools required for participation, including high levels of digital literacy. Citizen panel or assembly Large groups of people (dozens or even thousands) who are representative of a town/region. <code>INFORM</code> <code>CONSULT</code> <code>PARTNER</code> <code>EMPOWER</code> Provides an opportunity for co-production of outputs; can produce insights and directions that were not anticipated by the project team; can provide an information base for conducting further outreach (surveys, interviews, focus groups, etc.); can be broadly representative; can bolster a community\u2019s sense of democratic agency and solidarity. Participant rolls must be continuously updated to ensure panels or assemblies remains representative of the population throughout their lifespan; resource-intensive for establishment and maintenance; subject to hazards of group think or peer pressure; complex to facilitate; can be steered by dynamics of differential power among participants. Citizen jury A small group of people (between 12 and 24), representative of the demographics of a given area, come together to deliberate on an issue (generally one clearly framed set of questions), over the period of 2 to 7 days (Involve.org.uk. <code>INFORM</code> <code>CONSULT</code> <code>PARTNER</code> <code>EMPOWER</code> Can gather in-depth information; can produce insights and directions that were not anticipated by the project team; can bolster participants\u2019 sense of democratic agency and solidarity. Subject to hazards of group think; complex to facilitate; risk of pre-emptive evaluative framework; small sample of citizens involved risks low representativeness of wider range of public opinions and beliefs. <p>As with all forms of engagement, deciding on the best method requires awareness of your audience. Consider the following cases:</p>  Policymakers General Public Researchers <p>A research team has released results from an economics study that could have a positive impact on public policy. They decide to share these results with policymakers. The goal is to directly influence policy. Therefore, the results need to be clearly communicated and also connected to the policy goal. This connection is important to help ensure that policy-makers are able to evaluate the wider implications of the scientific findings. </p> <p>Communication Goal: to demonstrate how scientific findings can support evidence-based policy impact</p> <p>As part of an education outreach campaign to improve digital literacy among adolescents, a mental health charity are running workshops with secondary school students. They wish to communicate recent evidence about the impact of over-using social media on mental health. Rather than communicating complicated statistical information about the methods used in their study, the team develop a more accessible form of their findings and link these findings to practical steps that the students can take to protect themselves online.</p> <p>Communication Goal: to build awareness of possible risks associated with excessive social media usage and support behavioural change strategies</p> <p>A PhD student working in a Physics department has results from a recent study that developed and tested a new method for the large-scale data mining of astronomical data. The PhD student wish to present this new method and the validation study at an upcoming international conference for data science. The audience will be technically literate, but will not have specialist expertise in astronomy. Therefore, the PhD student describes the method in the context of its original study buy also emphasises the generalisability for other sciences (e.g. genomics).</p> <p>Communication Goal: to advance academic career by gaining experience of presenting conference papers and also generating interest in a novel data science method</p> <p>A common need with all of the above cases is the tailoring of the communication strategy to the engagement goal. And, in all cases, a clear message is vital. This topic (and others) will be the focus of the next section.</p>"},{"location":"skills-tracks/ped/chapter3/how/#education-and-outreach-getting-creative","title":"Education and Outreach: Getting Creative","text":"<p>The table at the start of this section outlines several methods for <code>informing</code>, but these are not the only ones that are relevant in the context of science and technology education and outreach. Other examples include,</p> <p>Education and Outreach</p> <ul> <li>Websites and social media</li> <li>Presentations</li> <li>Posters and displays</li> <li>Exhibitions</li> <li>Theatre, Film, and Documentaries</li> <li>Festivals</li> </ul> <p>And, some forms of education and outreach can be very creative. Consider the following two examples:</p> DeadinburghBillenium (Future Places Toolkit) <p></p> <p>\"An unknown pathogen ravages Scotland\u2019s capital, turning the unlucky souls into bloodthirsty ambling beasts. You are one of the last uninfected citizens in a city under martial law, cut off from the rest of the UK. Now, with help from real scientists, you have only hours to decide how to save Edinburgh, and perhaps the world. The Enlightenment Caf\u00e9: Deadinburgh, produced by LAStheatre, introduces the audience to the worlds of epidemiology and biomedical science through a night of immersive theatre. In a theatrical world, with actors playing the infected hordes and besieged soldiers, the audience meet genuine scientists using real science to solve a fictitious disease. In the end the audience must decide whether to destroy the city, cull the infected, or search for a cure; the fate of the city is in their hands. Through the outbreak of a zombie epidemic Deadinburgh asks \u2018what does it really mean to be human\u2019 whilst offering parallels with real life science and procedures for managing disease outbreaks.\"</p> <p>From the National Co-ordinating Centre for Public Engagement</p> <p></p> <p>Billennium was a mobile augmented reality (AR) project that took place in Bristol, UK in 2018. Participants used mobile devices to be guided on an AR tour, not of their city in the present day, but of possible architectural futures. The tour was designed to help promote members of the public engage in discussion and futures thinking about how their city could be design together.</p> <p>(Clarke, 2021)[@clarke2021] </p>"},{"location":"skills-tracks/ped/chapter3/when/","title":"When should you engage?","text":"<p>A complete answer to the question of when you should engage will, ultimately, depend on the specific details of your project. For instance, if your project involves the collection of experimental data from research subjects, then a significant part of the engagement will likely happen early on in your projects lifecycle. In contrast, if your project only requires communication of results and findings (e.g., for the goal of increased public awareness), then engagement may happen towards the end. The goal in this chapter is not to provide an exhaustive list or flowchart that can help you answer the question for any project\u2014this would not be feasible. Instead, what we will be exploring are two frameworks that can help you answer this question through a series of deliberative processes.</p> <p>The first of these frameworks is based on a model of a typical ML/AI project lifecycle. Although most relevant in the context of innovation projects, it can also play a valuable role in research projects, such as those that use ML to support data science.</p>"},{"location":"skills-tracks/ped/chapter3/when/#the-mlai-project-lifecycle","title":"The ML/AI Project Lifecycle","text":"<p>A significant portion of modern AI methods are data-driven and rely on various types of machine learning to identify patterns in the data and construct predictive models that can classify, forecast, or optimise goals (among other things).<sup>1</sup> As such, data science projects are increasingly turning to machine learning algorithms to help generate knowledge or support scientific processes of discovery (e.g., drug discovery).</p> <p>Although a wide range of differences exist between research and innovation projects, the following model is a useful abstraction for reflecting on the typical activities associated with the lifecycle of a project that involves some form of ML or AI.</p> <p></p> <p>As the above figure demonstrates, we can split a typical project into three over-arching stages:</p> <ol> <li>Project Design</li> <li>Model Development</li> <li>System Deployment</li> </ol> <p>'Project Design' comprises those activities that set the foundation for the practical development of the model (e.g., desk-based research, experimental design, data cleaning, exploratory data analysis).</p> <p>'Model Development' captures the technical processes of building a predictive model, as well as the important process of documenting how it was developed. This is not just important for the reproducibility of science, but is also vital for innovation (e.g. for regulatory compliance).</p> <p>Finally, 'System Deployment' captures the implementation and use of a model within a sociotechnical system. In the context of a scientific research project, this may include the application of the model to additional datasets to generate insights or knowledge. Whereas in the context of a commercial organisation, this could refer to a wide range of use cases (e.g., recommendation system for online bookings, predictive model for diagnoses in healthcare).</p> <p>This macroscopic perspective is helpful as a heuristic, but the main benefits of this model for facilitating pubic engagement comes when we look at the lower-level stages.</p>"},{"location":"skills-tracks/ped/chapter3/when/#project-design-stages","title":"Project Design Stages","text":"Stage Description Public Engagement Relevance Project Planning Preliminary activities designed to help scope out the aims, objectives, and processes involved with the project, including potential risks and benefits. May include impact assessment activities designed to identify possible risks to affected users (e.g., safety risks, data privacy violations). Problem Formulation The formulation of a clear statement about the over-arching problem the system or project addresses (e.g., a research statement or system specification) and a lower-level description of the computational procedure that instantiates it. Clarity on the problem or task being pursued can help a project team whether the system is likely to be socially acceptable prior to engagement (e.g. a facial recognition system deployed in a public space to monitor frequent visitors) Data Extraction or Procurement The design of an experimental method or decisions about data gathering and collection, based on the planning and problem formulation from the previous steps. Members of the public may have differing views on what data (and the methods by which they are collected) are acceptable to collect. Data Analysis Stages of exploratory and confirmatory data analysis designed to help researchers or developers identify relevant associations between input variables and target variables. Having an understanding of your data prior to model development can help identify whether groups of stakeholders have been systematically overlooked (i.e. sources of missing data)."},{"location":"skills-tracks/ped/chapter3/when/#model-development-stages","title":"Model Development Stages","text":"Stage Description Public Engagement Relevance Preprocessing and Feature Engineering A process of cleaning, normalising, and refactoring data into the features that will be used in model training and testing, as well as the features that may be used in the final system. Choices made at this stage can affect the overall interpretability of a model, which can affect how easily users or stakeholders will be able to engage with the final system. Model Selection and Training The selection of a particular algorithm (or multiple algorithms) for training the model. Choices made at this stage can affect the overall interpretability of a model, which can affect how easily users or stakeholders will be able to engage with the final system. Model Testing and Validation Testing the model against a variety of metrics, which may include those that assess how accurate a model is for different sub-groups of a population. This is important where issues of fairness or equality may arise. If a model is more performant for one group than another, this should be discussed with the affected and impacted stakeholders during engagement. Model Documentation A process of documenting both the formal and non-formal properties of both the model and the processes by which it was developed (e.g., source of data, algorithms used, evaluation metrics). Access to documented evidence can be useful for a wide variety of public engagement goals, including general communication of results to evidence to support deliberative exercises."},{"location":"skills-tracks/ped/chapter3/when/#system-deployment-stages","title":"System Deployment Stages","text":"Stage Description Public Engagement Relevance Model Productionalisation The process of putting a model into production. That is, implementing a model within a system that enables and structures interaction with the model (e.g. a recommender system that converts a user's existing movie ratings into recommendations for future watches. Engaging affected stakeholders can help identify challenges or barriers to productionalisation that have not been anticipated (e.g., approval processes, software compatibility, user experience concerns). User Training Training for those individuals or groups who are either required to operate a data-driven system (perhaps in a safety critical context) or who are likely to use the system (e.g. consumers). Where a research or innovation project results in a change to existing processes, it is important that the users are engaged in order to ensure that the system is used correctly. Otherwise, its overall efficacy can be significantly impaired. System Use and Monitoring Ongoing monitoring and feedback from the system, either automated or probed, to ensure that issues such as model drift have not affected performance or resulted in harms to individuals or groups. Ongoing engagement with users and stakeholders can help identify whether the original project's objectives have been met, or if the model/system are still performing as intended. Model Updating or Deprovisioning An algorithmic model that that adapts its behaviour over time or context may require updating or deprovisioning (i.e. removing from the production environment). If the updating or deprovisioning results in a loss of service\u2014possibly to a business critical system\u2014then users and stakeholders will likely need to be consulted, either to ensure no harms arise or to identify the steps needed to bring a new or revised project to fruition. <p>As we will see shortly, the above framework helps establish a common reference point that can anchor reflection and deliberative activities related to public engagement. The following illustrative example helps to demonstrate this point.</p> <p>Shaping Attitudes to Public Policy</p> <p>A group of researchers working for a news organisation are carrying out a project to explore how the political attitudes of members of the public may be influenced by algorithmically generated articles. One of their goals is to determine whether the emotional valence of an article affects the likelihood of the reader agreeing with or disagreeing with public policies that are being discussed by the news articles.</p> <p>Using the AI project lifecycle model, they identify three initial stages and activities during their preliminary project planning:</p> <ul> <li>Stakeholder identification and engagement to support experimental design (Project Planning)</li> <li>Participant data gathering and informed consent (Data Extraction or Procurement)</li> <li>Experiment and participant feedback (Model Testing and Validation)</li> </ul> <p>However, when they begin the initial stakeholder identification and engagement they hit a roadblock. During discussion and deliberation with potential users of the news platform, the research team ask the participants to share their attitudes towards algorithmically generated content. A majority of the participants have a critical attitude towards the use of AI to generate news contents, and express strong disapproval of the proposed idea of influencing attitudes towards public policies through what they see as \"emotional manipulation\".</p> <p>On the basis of this feedback, the research team decide to change their research plan by co-designing an alternate system with the participants. The revised system still uses generative AI, but as a decision support for a human journalist. The team also add a further engagement activity during the <code>model documentation</code> stage to help mitigate any further aversion towards the system by ensuring sufficient public understanding and awareness of how the system operates.</p> <p>We will reference this model and its constituent stages repeatedly as we continue through the remainder of the course.</p>"},{"location":"skills-tracks/ped/chapter3/when/#stakeholder-analysis","title":"Stakeholder Analysis","text":"<p>Regardless of the type of engagement, a core part of any public engagement is knowing your audience.</p> <p>Important</p> <p>Ask \"who\" before \"how\"!</p> <p>This section will explore a three-step process, which begins with 'Project Planning' and is designed to help you identify and understand your audience (or, stakeholders), summarised in the following graphic.</p> <p></p> <p>Principle of Proportionality</p> <p>The following steps are designed to be fairly exhaustive. Therefore, it is important to recognise that they represent an ideal, which will need to be tailored to the specifics of a research and innovation project. In short, some questions or forms of engagement/evidence gathering may not be required for particular research or innovation projects. For example, a simple research project focusing on improving data literacy will not require the same types of extensive impact assessments as an innovation project developing a new safety critical AI system for use in healthcare.</p>"},{"location":"skills-tracks/ped/chapter3/when/#1-preliminary-project-scoping-and-stakeholder-analysis","title":"1 Preliminary Project Scoping and Stakeholder Analysis","text":"<p>The first step occurs within the project team and involves a number of questions or self-assessments that help lay the groundwork for the remainder of the project:</p> <ul> <li> outline key project components, including</li> <li> a high-level description of the ML/AI system being employed</li> <li> the domain or context of use</li> <li> the data to be used in the project (e.g. for model training)</li> <li> identify individuals or groups who may be affected by your project</li> <li> identify individuals or groups who may affect your project</li> <li> scope potential stakeholder impacts</li> <li> evaluate the salience and contextual characteristics of identified stakeholders</li> </ul> <p>The answers and evidence accumulated from answering these questions will help your team with subsequent activities.</p>"},{"location":"skills-tracks/ped/chapter3/when/#2-positionality-reflection","title":"2 Positionality Reflection","text":"<p>All individual human beings come from unique places, experiences, and life contexts that have shaped their thinking and perspectives. Reflecting on this variation can help us understand how our viewpoints might differ from those around us, and from those who have diverging cultural and socioeconomic backgrounds and life experiences.</p> <p>Social scientists have long referred to this kind of self-locating reflection as \u201cpositionality.\u201d When team members take their own positionalities into account, and make this explicit, they can better grasp how the influence of their respective social and cultural positions may affect the engagement process. The following 'positionality matrix' is designed to help with this process:</p> <p></p> <p>An example of how an individual's or group's characteristics may affect a research or innovation project is if a project team fail to identify how their own educational backgrounds enable them to understand and comprehend technical or complex concepts with comparative ease, or how the lack of cultural diversity in the team prevent them from recognising the significance of alternative values (e.g., prioritising technological policies that support social cohesion over individual autonomy). </p>"},{"location":"skills-tracks/ped/chapter3/when/#3-stakeholder-engagement-objectives-and-methods","title":"3 Stakeholder Engagement Objectives and Methods","text":"<p>The final step is to establish engagement objectives that enable the appropriate degree of stakeholder engagement and co-production in project evaluation, and methods that support the achievement of defined objectives. We will look at the latter of these in detail in the next section, so let's just address the former here.</p> <p>If we think back to the first chapter, and the different models of participation (e.g., Arnstein's ladder of engagement)[@arnstein1969], then we can identify the following four goals that can help us specify our project's objectives:</p> <p></p> <p>The use of these goals to support the identification of engagement objectives should also be informed by a) the following variations on participation, and also b) the methods of participation available to you and those who you are engaging (see next chapter)</p> Degree of Participation Description Means of Participation Level of Agency INFORM Stakeholders are made aware of decisions and developments. External input is not sought out. Information flows in one direction. This an be done through newsletters, the post, app notifications or community forums. <code>LOW</code> Stakeholders are considered information subjects rather than active agents. CONSULT Stakeholders can voice their views on pre-determined areas of focus, which are considered in decision-making. Engagement occurs through online surveys or short phone interviews, door-to- door or in public spaces. Broader listening events can support consultations. <code>LOW</code> Stakeholders are included as sources of information input under narrow, highly controlled conditions of participation. PARTNER Stakeholders and teams share agency over the determination of areas of focus and decision making. External input is sought out for collaboration and co- production. Stakeholders are collaborators in projects. They are engaged trough focus groups. <code>MODERATE</code>  Stakeholders exercise a moderate level of agency in helping to set agendas through collaborative decision making. EMPOWER Stakeholders are engaged with as decision-makers and are expected to gather pertinent information and be proactive in co-operation. Co-production exercises occur through citizens\u2019 juries, citizens\u2019 assemblies, and participatory co-design. Teams provide support for stakeholders\u2019 decision making. <code>HIGH</code> Stakeholders exercise a high level of agency and control over agenda-setting and decision making. <ol> <li> <p>This is in contrast to symbolic or rules-based methods where a human is required to program the underlying logic that determines the machine's behaviour.\u00a0\u21a9</p> </li> </ol>"},{"location":"skills-tracks/ped/chapter4/","title":"Practical Guidance","text":""},{"location":"skills-tracks/ped/chapter4/#introduction","title":"Introduction","text":"<p>This chapter builds on the guidance for planning and facilitating different types of public engagement by delving deeper into some of the practical tools and methods.</p> <p>In the first section we look at the communication of data science and AI as a process of storytelling. This includes a brief look at data visualisation tools; the need to consider the limits of your audience's attention; and the importance of narrative structure. </p> <p>In the second section we consider the challenge of communicating uncertainty. Science and technology communication often requires the communication of probabilistic information, statistics, and risky or uncertain outcomes. There are a range of challenges associated with this form of communication when it comes to public engagement. Therefore, we take some time to identify what these challenges are, and what we can do to overcome them. </p>"},{"location":"skills-tracks/ped/chapter4/#chapter-outline","title":"Chapter Outline","text":"<ul> <li>Storytelling with Data</li> <li>Communicating Uncertainty</li> </ul> <p>Learning Objectives</p> <p>By the end of this chapter you should a) have a good appreciation of how the perspective of storytelling can help you develop more effective forms of communication and engagement, and b) understand some of the challenges that arise when attempting to communicate probabilistic or statistical information to members of the public.</p>"},{"location":"skills-tracks/ped/chapter4/storytelling/","title":"Storytelling with Data","text":"<p>\"Data tells a story!\"</p> <p>This phrase, which gets thrown around a lot, does contain a kernel of truth. But telling a story with data is not always straightforward, and data do not tell stories on their own. It takes careful analysis and informed deliberation to make sure that data tell the right story, and that the story is engaging to those who want or need to hear the story. Consider the following graph, for instance:</p> <p></p> <p>Now, ask yourself the following questions:</p> <p>Questions</p> <ol> <li>What story do you think this graph is trying to tell?</li> <li>Which part of the graph caught your attention initially?</li> </ol> <p>Cole Nussbaumer Knaflic's excellent book, Storytelling with Data is full of examples such as this one, in which badly designed or ordinary graphs are transformed into engaging and insightful visualisations that help tell a story and realise various goals.[@knaflic2015] The previous graph, for example, is altered into the following one, where a) more supportive messaging helps explain how a new program improved children's interest and engagement in science, and b) better use of colours help to direct the audience's attention to the values of significance (i.e., the increased percentages of children who were either 'interested' or 'excited' in science following the intervention).</p> <p></p> <p>In this section, we are going to look at three lessons (based on Knaflic's book)[@knaflic2015] to help tell more engaging stories with data:</p> <ol> <li>Choose the right tool for the job</li> <li>Understand the limits of attention</li> <li>Construct a clear narrative</li> </ol>"},{"location":"skills-tracks/ped/chapter4/storytelling/#choosing-the-right-tools","title":"Choosing the right tools","text":"<p>You would not choose a hammer to drill in a screw. Sure, it may get the screw into the wall, but it's certainly not the most effective tool for the job. In the context of data visualisation, there are no shortages of graphs (tools) and libraries (toolkits) to help you create a multitude of different graphs. For example, the popular Seaborn data visualization library for Python:</p> <p>But, similar to the hammer analogy, it's easy to end up with choice paralysis in situations like this, especially when there are so many options available. </p> <p>Example</p> <p>Although far from being an exhaustive list, the following options are a varied set of examples for data visualisation tools:</p> <ol> <li>Tableau: powerful tool for data analysis, BI, and visualisation</li> <li>Canva: an online graphic design platform with a simple interface</li> <li>D3.js: a javascript library well suited to web-based visuals</li> <li>Google Charts: another free option for embedding (many different) charts into webpages</li> <li>Datawrapper: a popular option among cvommunicators (e.g. journalists) with a good range of interactive and responsive charts</li> <li>Bokeh: an Python library for creating interactive visualisations with a variant for R</li> <li>Flourish: easily turn your data into animated charts, maps, and interactive stories.</li> </ol> <p>So, how do you choose the right tool? How do you select the right visualisation?</p> <p>Well, simply put, you have a clear answer to the questions from the preceding chapters:</p> <ul> <li>Who is your audience?</li> <li>What matters to them (or, what do they value?</li> <li>Why are you engaging them?</li> <li>What is your goal?</li> </ul> <p>When you can satisfactorily answer these questions you will probably know which tool is the right one top use. However, there are still a couple of hazards that could get in your way.</p>"},{"location":"skills-tracks/ped/chapter4/storytelling/#attention","title":"ATTENTION!!!","text":"<p>You're about to take part in a short experiment. Watch the following video:</p> <p>Did you spot the change?</p> <p>As the video emphasises, our limited cognitive capacity forces us to pay attention to the details that we expect to be most salient to the current tasks we are engaged in. When you are communicating with or engaging an audience, they will also be in a similar predicament\u2014forced to \"make (sub-conscious) choices\" about what to pay attention to. Therefore, it is important for you to have an understanding of how you can reduce cognitive load and make it easier for your audience to pay attention to what matters.</p> <p>Preattentive attributes are one way to achieve this.</p> <p>Knaflic's book, again, has useful guidance here about the role of preattentive attributes\u2014elements of visual design that are rapidly processed by the lowest levels of our visual systems, signifying something of salience to the individual. The following 12 preattentive attributes are well-known among designers, and often exploited by those in advertising!</p> <p></p> Graph with no preattentive attributesGraph with colour addedGraph using colour to build hierarchy <p></p> <p></p> <p></p>"},{"location":"skills-tracks/ped/chapter4/storytelling/#constructing-narrative-structure","title":"Constructing Narrative Structure","text":"<p>The Hero's journey is a well-known template in literature and story-telling more generally. In brief, it identifies several stages for a protagonist's journey, starting with a departure (or, call to adventure) that serves as a hook, which leads into a series of trials, initiations, or challenges for the protagonist to overcome, before returning home as a changed or transformed individual.</p> <p></p> <p>This template can be seen in many areas of popular culture, ranging from science-fiction films (e.g. Star Wars) to classic works of literature (e.g. Jane Eyre). There are, of course, many challenges to the validity of this template and its application to specific stories\u2014in both academia and geek culture! However, as a scaffold for both constructing and interpreting meaning the template serves a purpose.</p> <p>Unfortunately, there is no comparable template that we can point to for building narrative in the context of public engagement. But there are principles that can help steer us in the right direction, which include some we have already explored.</p> <p>Consider the following visual communication from the Education Endowment Foundation\u2014an independent charity with the goal of improving educational attainment of the poorest pupils in English schools.</p> <p></p> <p>The purpose of this graphic is to clearly summarise the evidence base for different education interventions, by focusing on three properties:</p> <ul> <li>Cost</li> <li>Strength of Evidence</li> <li>Impact</li> </ul> <p>Each of these three elements helps to build and communicate a clear narrative:</p> <ol> <li>Our research has uncovered a variety of interventions and policy options for improving education.</li> <li>These options differ in three ways:</li> <li>Their cost</li> <li>The strength of their supporting evidence</li> <li>The potential impact from implementing the chosen policy</li> <li>The information we have gathered and organised can support evidence-based policy making</li> </ol> <p>Obviously there is a lot more behind these three properties, but they serve as an accessible overview to help direct the attention of the audience. This is achieved through the use of clear visuals, clear messaging (and narrative structure), and a clear understanding of the audience and goals for the project\u2014in the above case, policy-makers. Moreover, the message is linked to an implied causal process\u2014a key component in narrative. Specifically, doing A (implementing a policy) is likely to cause B (the associated impact).</p> <p>This latter point may strike data scientists as problematic, as the idea that correlation does not imply causation is drilled into all scientists early on in their career. As such, suggesting evidence of causation is typically seen as an instance of overstating research findings. But when it comes to good storytelling and narrative, an underlying causal process is often implied or assumed by the audience. This is not necessarily a problem, as long as you are clear to differentiate between any causal processes referred to in your research, and the causal account assumed in your engagement's narrative.</p> <p>Although not a replacement for the Hero's journey, the following set of principles from Northeastern University serve as both a good summary of what we have learned so far, and also reinforce the importance of clear narrative structure (see principles in bold):</p> <p>10 Principles for How to Communicate Effectively</p> <ol> <li>Know your audience</li> <li>Identify the goals of communication</li> <li>Start with the most important information</li> <li>Avoid jargon</li> <li>Be relatable</li> <li>Provide visuals</li> <li>Stick to three points</li> <li>Talk about the scientific process</li> <li>Focus on the bigger impact</li> <li>Develop an elevator pitch</li> </ol>"},{"location":"skills-tracks/ped/chapter4/uncertainty-example/","title":"Visualising Uncertainty","text":"<p>Temporary Page</p> <p>Due to a broken plugin that prevents us from rendering Jupyter notebooks, this page is currently incomplete. Our original document can be downloaded here.</p>"},{"location":"skills-tracks/ped/chapter4/uncertainty/","title":"Communicating Uncertainty","text":"<p>There are many forms of uncertainty that we have to consider when planning and facilitating forms of public engagement.</p> <ul> <li>There is the conceptual uncertainty involved with key terms that are necessary for shared knowledge and understanding (e.g. what does 'intelligence' mean in the context of AI?).</li> <li>There is the normative uncertainty that is implicated when deliberating about or choosing on the right or best course of action (e.g. citizen juries).</li> <li>And, there is the scientific uncertainty associated with data analysis, experiments, and scientific knowledge.<sup>1</sup></li> </ul> <p>In this section, we are just going to look at the final option, but it's good to be aware of the other two.</p>"},{"location":"skills-tracks/ped/chapter4/uncertainty/#the-challenges-of-scientific-uncertainty","title":"The Challenges of Scientific Uncertainty","text":"<p>Roll up! Roll up! Come and see a true scientific mystery: the marvelous, mystifying marmokreb!</p> <p></p> <p>Alright, it's not much to look at, but it is a true scientific mystery in one respect.</p> <p>As Michael Blastland[@blastland2020] recounts in The Hidden Half, the marmorkrebs caused quite the stir in the late 20<sup>th</sup> Century when it was discovered that lone females were able to lay eggs without fertilisation. In short, the marmorkrebs did not need to mate, such that offspring were natural clones of their mothers\u2014a process known as parthogenesis. However, it was not the parthogenic property\u2014unique among the ~15,000 species of decapod\u2014that baffled the scientific establishment. Rather, the mystery lay in what the marbled marmokrebs meant<sup>2</sup> for the nature-nurture debate.</p> <p>Because they were natural clones, the marmokrebs were ideal subjects for an experiment that raised two lineages in identical test conditions, designed to treat the environment as a control condition and investigate how and which genes contribute to observed behaviour or physiological traits. As Blastland puts it,</p> <p>The aim as far as humanly possible was to eliminate every variation that anyone could think of. They were born into the most boring uniformity humans could contrive.</p> <p>And yet, the following figure shows three marmorkrebs from the same lineage, who were raised in the exact same conditions.</p> <p> </p> A figure of three marmorkrebs displaying significant variation (Reprinted from Vogt et al. 2008) <p>Although the variation in size is striking enough, what is not shown in this image are the many other traits that differed throughout the population, such as distinct marbling or lifespan. And so it was that these seemingly simple crayfish undermined one of the key assumptions about phenotypic variation: if it isn't genes it's environment; if it isn't the environment it's genes.</p> <p>At this point, you may be considering possible explanations for the cause of the variation: is it epigenetics, or uncontrolled differences in the marmorkrebs environment that were overlooked by the researchers? We won't go into each possible causal explanation here, though I do suggest you read Blastland's book in case you find yourself over-confident that you know the answer! The purpose of mentioning this case of the confounding crayfish is to emphasise that there is a lot in science that we simply do not know. Sometimes, this uncertainty can be accounted for by following the scientific method and conducting ongoing experimentation. At other times, the observable phenomena place too much stress on the dominant scientific paradigm, causing the scientific community to rethink core assumptions\u2014a sociological process brought to light by the philosopher of science, Thomas Kuhn.[@kuhn1970]</p> <p>As researchers and developers trained in the scientific method, you are likely familiar with a variety of methods and tools for navigating scientific uncertainty. But when it comes to communicating this uncertainty to different stakeholders, the knowledge and understanding of these methods cannot be assumed. And this can lead to a breakdown in communication and trust. This is where the systematic framework from van der Bles et al. (2019)[@vanderbles2019] comes in handy.</p>"},{"location":"skills-tracks/ped/chapter4/uncertainty/#a-framework-for-communicating-uncertainty","title":"A Framework for Communicating Uncertainty","text":"<p>The framework presented in (van der Bles et al., 2019)[@vanderbles2019] is grounded in a review of the, admittedly \"scattered\" empirical evidence pertaining to the psychological and behaviour effects of communicating uncertainty. Their framework comprises five stages that are summarised as follows:</p> <ol> <li>Who communicates</li> <li>what</li> <li>in what form</li> <li>to whom, and</li> <li>to what effect</li> </ol> <p></p> <p>Let's (briefly) look at each of these in turn.<sup>3</sup></p>"},{"location":"skills-tracks/ped/chapter4/uncertainty/#who","title":"Who","text":"<p>The first stage of their framework directs us to consider who is a) assessing the uncertainty, and b) who is responsible for communicating the uncertainty. As we will see, this is important for identifying the format of communication when communicating to different audiences (e.g. lay people versus those with technical expertise) but also for assessing credibility of the source reporting the information (e.g. media, scientists, commercial organisations).</p>"},{"location":"skills-tracks/ped/chapter4/uncertainty/#what","title":"What","text":"<p>A particularly valuable contribution from their framework is the categorisation of what the uncertainty pertains to into the following taxonomy:</p> <ul> <li>Object of uncertainty<ul> <li>Facts (e.g. is the uncertainty about a specific event, such as whether there has been a data leak)</li> <li>Numbers (e.g. is the uncertainty about specific quantities or parameters of a model)</li> <li>Hypotheses (e.g. is the uncertainty about an underlying assumption in a scientific theory)</li> </ul> </li> <li>Source of uncertainty<ul> <li>Variability in population being sampled</li> <li>Measurement error</li> <li>Limited knowledge</li> <li>Expert disagreement</li> </ul> </li> <li>Level of uncertainty<ul> <li>Direct (i.e. is the uncertaity about the object itself)</li> <li>Indirect (i.e. is the uncertainty related to the quality of the evidence)</li> </ul> </li> <li>Magnitude of uncertainty</li> </ul>"},{"location":"skills-tracks/ped/chapter4/uncertainty/#in-what-form","title":"In what form","text":"<p>The format used to communicate the uncertainty is likely to affect the audience in a variety of ways (see 'to what effect'). As the following figure indicates, this form is associated with varying levels of precision.</p> <p>.</p> <p>Decreased precision is not always undesirable. For instance, sometimes it represents a worthwhile trade-off between precision and accessibility of communication to a non-technical or time-stretched audience. However, as the diagram shows, it can sometimes spill over into forms of explicit denial.</p>"},{"location":"skills-tracks/ped/chapter4/uncertainty/#to-whom","title":"To whom","text":"<p>In chapter 2 we saw how the relationship between researchers and the public can embody a variety of communicative values, but also prevent obstacles when imbalances of power arise. Similar characteristics are present in the framework from van der Bles et al. (2019)[@vanderbles2019].</p> <ul> <li>characteristics of the audience (e.g. data literacy, relevant demographic variables)</li> <li>relationship of the audience to what is being communicated (e.g. skeptical attitude)</li> <li>relationship of the audience to the people doing the communication (e.g. trustworthy or credible relationship)</li> </ul> <p>As an example, van der Bles et al. (2019)[@vanderbles2019] cite a study from Dieckmann et al. (2017)[@dieckmann2017] that found evidence of varying responses to ambiguous information about average global surface temperatures. For audiences, who were more accepting of climate change, they took the information to be evidence of a normal distribution where higher values of surface temperature were likely. Whereas, for audiences who were less accepting of climate change, the same information was taken to reflect a uniform distribution where lower temperatures were more likely.</p>"},{"location":"skills-tracks/ped/chapter4/uncertainty/#to-what-effect","title":"To what effect","text":"<p>The final stage of the framework considers the effect of not knowing, and identifies the following categories based on the extant empirical literature:</p> <ul> <li>Effect on cognition (e.g. are consistent inferences or judgements elicited from verbal statements of uncertainty)</li> <li>Effect on emotion (e.g. do uncertainty ranges elicit higher levels of anxiety in specific audience, rather than point estimates)</li> <li>Effect on trust (e.g. is perceived over-confidence in the communication of uncertainty likely to build confidence or undermine trust)</li> <li>Effect on decision-making (e.g. if uncertainty is communicated, will decision-making be delayed)</li> </ul> <p>The above framework helps to systematise our understanding of uncertainty, which can be very valuable when trying to consider how best to communicate with specific audiences or stakeholders. However, it also raises the following question:</p> <p>Question</p> <p>Given the wide range of factors involved, and the scattered nature of the empirical evidence, how should researchers choose the best course of action when communicating uncertainty?</p> <p>There is, again, and unfortunately, no simple answer to this question. Deciding on the right course of action is likely to be a team effort, and one which considers all of the topics we have covered so far (e.g. goals, values, audience, available methods, time and resources). There are, however, some interesting tools that link back to the first section in this chapter, which deserve a mention.</p> Plotting Hypothetical OutcomesInteractive Heat MapsUncertainty.io <p></p> <p>One method is to use packages built into software like Python and R to generate visual animations that explicitly depict alternate or hypothetical outcomes alongside, say, the most likely model of the data generation process. See this page for more details.</p> <p></p> <p>Heat maps are commonly used for communicating probabilistic information. The Climate Impact Lab take the traditional heat map a step further though by adding interactivity to allow users to alter variables and see the possible impact that emissions, for example, could have across the globe.</p> <p></p> <p>Still in need of some creative inspiration? Uncertainty.io is a queryable interface to references of more than 400 works of fine art that \"have a unique ability to convey uncertainty using a range of approaches and techniques.\"</p> <ol> <li> <p>We can split this into both epistemic and aleatory uncertainty, where the former relates to our uncertain knowledge of the world, and the latter refers to fundamental indeterminacy of the world itself. In this section we will only consider the former (i.e. epistemic uncertainty).\u00a0\u21a9</p> </li> <li> <p>An optional activity is to see who is the fastest at saying this phrase five times \ud83d\ude09.\u00a0\u21a9</p> </li> <li> <p>See the original article for further details and a review of the empirical evidence.\u00a0\u21a9</p> </li> </ol>"},{"location":"skills-tracks/ped/chapter5/","title":"Public Trust and Assurance","text":""},{"location":"skills-tracks/ped/chapter5/#introduction","title":"Introduction","text":"<p>To conclude, we turn to look at some of the outstanding issues raised in the previous chapter. Specifically, we look at some of the explanatory factors for why members of the public may be skeptical of public engagement or even distrust science and technology.</p> <p>These factors are important to understand, even if there are no easy answers or solutions. We explore why public trust matters in the context of public engagement, and also what can be done to provide assurance that you and your team have conducted your project in a responsible and trustworthy manner.</p>"},{"location":"skills-tracks/ped/chapter5/#chapter-outline","title":"Chapter Outline","text":"<ul> <li>Public Trust in Science and Technology</li> </ul> <p>Learning Objectives</p> <p>By the end of this chapter you will understand several reasons why members of the public may be distrustful of science and technology. This recognition, alongside the lessons learned over the course, will help you identify means or methods in your own work that could help build trust, rather than undermine it.</p>"},{"location":"skills-tracks/ped/chapter5/trust/","title":"Public Trust in Science and Technology","text":"<p>Going back to the beginning of the course, establishing social trust in science is one of the goals of public engagement. Not only does a lack of trust diminish science's legitimacy in the public eye, it also makes the more practical goals of science much harder to achieve . If laypeople distrust science and are incredulous or suspicious of its claims, it will be quite difficult to put scientific results into wide use.[@grasswick2010]</p> <p>Vaccine hesitancy is a clear example of this. If a relevant portion of the public distrusts the science behind vaccine efficacy and safety, they will be less likely to get vaccinated no matter how readily available the vaccine is. Not only does this leave the unvaccinated with a higher risk of falling ill themselves, it also negatively impacts the overall level of immunity in the population.</p> <p>Another example from the Covid-19 pandemic comes from contact-tracing apps which were designed to track the spread of the virus. These apps enjoyed widespread support from healthcare organisations as they had the potential to develop and strengthen epidemiological research. However, the reception from the general population was mixed. In particular, a relevant portion of the public was concerned with whether their data was being handled responsibly, and this sometimes led to poor adoption rates from the general population, once again highlighting the difficulty of implementing scientific research and processes when people do not trust science.</p> <p>As important as citizen trust in science is, there are many challenges that can make science less trustworthy to individuals and communities. As scientists and researchers, it is important to become aware of them and reflect on how they might be mitigated.</p>"},{"location":"skills-tracks/ped/chapter5/trust/#10-challenges-to-public-trust-in-science","title":"10 challenges to public trust in science","text":"<p>Over the next sections we will look at different challenges or barriers to public trust in science and technology. Although we will go through each issue separately, in practice there is a lot of overlap and interrelations between them, and some of them feed into or amplify each other.</p>"},{"location":"skills-tracks/ped/chapter5/trust/#1-understanding-of-science","title":"1. Understanding of science","text":"<p>The first challenge to fostering public trust in science we will look at has to do with science education. In particular, with the general public's poor understanding of what science is and how it works, which can lead them to misinterpret the normal workings of science as its failings.</p> <p>On the first day we looked at the Deficit Model[@lewenstein2003] which assumed scientific literacy amounted to knowledge of scientific findings. As we saw, there are problems with this view since arguably members of the public have no reason to know the detailed workings of generative models in machine learning or complicated equations in theoretical physics.</p> <p>There is however, another way to think about scientific literacy; not in terms of how much scientific knowledge the public has, but instead on whether they grasp the nature of the scientific process. As Douglas (2012) argues, what should be at the core of scientific education is not science as a set of facts about areas of knowledge, but instead a thorough understanding of what science is as an epistemic endeavour.[@douglas2017]</p> <p>The most important thing to understand about the scientific process is that science is jointly critical and inductive in nature (ibid). Science seeks to build an empirical understanding of the world through proposing explanatory theories and then testing them in the best way possible (ibid). Therefore, science must always rely on induction to reach its conclusions. As such, there is no  irrevocable proving of facts in science. Instead, one can only falsify (or fail to falsify) its theories. </p> <p>A classical example is that of the black swan. The phrase dates back to the Roman poet Juvenal who used it to describe a rare bird presumed to be non-existent. The phrase was popular in 16<sup>th</sup> century London to refer to impossible events or inexisting ojects. Because no one had ever seen a black swan (at least among Europeans), it was assumed that black swans did not exist.</p> <p>Such an analysis relies on induction: it goes from particular instances of (a lack) of sightings of black swans to the general conclusion that black swans do not exist. However, in 1697 Dutch explorers became the first Europeans to see a black swan in Western Australia and their non-existence was thus disproven. </p> <p>The black swan story helps illustrate the limits of induction: no matter how many sightings of (only) white swans, one cannot irrevocably prove that black swans do not exist. To do so, one would need to examine all swans in existence, which is not possible. And science always works like this: we can only generalise findings from particular instances. This means a definite and irrevocable proof is unattainable. </p> <p>Given its inductive nature, science must also be continually  open to criticism: it must always allow for the testing of its claims under the light of new evidence. The very reason for the advancement of scientific research is the fact that the status quo is continually challenged and tested as opposed to being dogmatic and definitive in its conclusions.</p> <p>However, the argument goes, the problem is that many people do not understand science as an ever-evolving process which produces only provisional results. Instead they perceive science to be a set of facts about the world (the speed of light is 300,000,000 m/s, humans cells have 23 pairs of chromosomes, etc.). This leads to public confusion about how science works, and it can make people interpret the normal (even crucial) workings of science as evidence of scientific failure. </p> <p>A good example is expert disagremeent and debate. As we just saw, this is a key component of a healthy scientific community. Only if scientists are always open to criticism and willing to change their minds, can science continually improve its understanding of the world. </p> <p>As Douglas notes, \"[e]xperts changing their minds is also evidence of science functioning properly, not evidence of experts being fickle of weak-minded\".[@douglas2017] However, expert disagreement can be met with frustration from the general public if they are under the impression that science should provide definite answers, and when scientific 'facts' change (as some inevitably will) this may be interpreted as evidence of science's failings, slowly corroding the public's trust in it.</p>"},{"location":"skills-tracks/ped/chapter5/trust/#2-spread-of-doubt-and-confusion","title":"2. Spread of doubt and confusion","text":"<p>Another phenomenon which can erode the public's trust in science is documented in Naomi Oreskes and Eric M. Conways's book 'Merchants of Doubt: How a Handful of Scientists Obscured the Truth on Issues from Tobacco Smoke to Global Warming'.[@oreskes2011] The authors' show how a group of scientists in the United States used the inductive nature of science to discredit scientific findings that went against the financial interests of the industries that employed them. </p> <p>Two of the most-well known cases are those of the tobacco and oil industries. The tobacco industry famously denied the link between smoking and lung cancer long after science had provided robust evidence for it. The same can be said of the oil industry; first they sowed doubt on whether climate change was real, and when that became increasingly difficult to deny, they advanced the possibility that perhaps it was not man-made (again, going against what the evidence was consistently showing). </p> <p>In both cases, the scientists (who had ties to these industries) used the very nature of the scientific method as a way to undermine science's findings. As we know, because science is at its core inductive, nothing can ever be definitely and finally proven. There is always the possibility, however remote, that current scientific understanding is wrong. </p> <p>This does not mean of course, that there cannot be overall consensus in science. In the case of climate change for instance, Oreskes (2004, 2007)[@oreskes2004]<sup>-</sup>[@oreskes2007] documents that in a review of approximately 900 scientific articles on climate change, none of them refuted the idea \"global climate change is ocurring, and human activities are at least part of the reason why.\"[@almassi2012] However, she also notes that a 2006 ABC News poll in the US found that 64% of Americans perceived there to be a lot of disagreeemnt amongst scientists on the reality of global warming (ibid).</p> <p>This is the strategy of the 'merchants of doubt': spreading confusion among the public about the level of scientific consensus on certain topics in an effort to delay public criticism and therefore regulation. If, as we have noted, the public expects science to provide with a set of unchanging facts, this strategy will have a doubly negative effect: not only will it undermine trust in science as scientists are supposed to have the answers, but it will also create the false perception that there is no scientific consensus when there actually seems to be one. Clearly, this can increase distrust in science, especially if the strategy is used to hide harms to the public in an attempt to maintain or increase the profits of those funding the scientists (as is blatantly the case in the examples from the tobacco and oil industries).</p>"},{"location":"skills-tracks/ped/chapter5/trust/#3-vested-interests","title":"3. Vested interests","text":"<p>Related to the 'merchants of doubt' strategy is the general problem of scientists having incentives other than those motivated by the disinterested attitude of someone trying to understand how the world works. This is of course, always true: scientists can be driven by multiple motivations, such as ego or the quest for status, among many others. Yet when scientists' incentives are aligned with protecting the interests of the companies who employ them, things become particularly problematic. </p> <p>Cases where this happens are well-document in medicine, where the pharmaceutical industry has been known to use diverse strategies to advance their interests: from aggressive marketing directed at physicians to increase the prescription of their drugs,[@keefe2021] to funding scientists who are supposedly objective and neutral, but who in reality are under the pharmaceutical companies' payroll.[@ritchie2020]</p> <p>An example from the world of tech comes from Facebook (now Meta). In documents known as 'the Facebook files', the Wall Street Journal published various articles documenting the company's clear awareness of the many problems and failings of it products. However, it did surprisingly little to fix them, all the while minisiming the extent of the problem or feigning ignorance to the general public.</p> <p>In particular, leaked company documents show that internal research inside Meta concluded that Instagram was detrimental for teen mental health.[@wells2021] The documents not only detail the evidence Meta had amassed linking poor teen mental health outcomes to Instagram use, but they also documented how this information was relayed to Mark Zuckerberg, the company's CEO. Additionally, it shows that worries about users interacting less with the platform played into the company's decision to fix (or more accurately, to do nothing about) some of Instagram's problems (ibid). When research like this is exposed, it is not hard to see how it could erode people's trust in researchers. Although in this case  we are looking at Meta employees payed to carry our research and not at independent researchers, 'the Facebook files' highlight the negative consequences for the public that occur when research is ultimately serving the interests' of private companies rather than the public good.</p>"},{"location":"skills-tracks/ped/chapter5/trust/#4-fraud","title":"4. Fraud","text":"<p>Even more extreme cases occur when scientists are caught in outright fraud, claiming to have achieved feats which are simply not true, or inventing data and publishing it as real. Many examples come from the world of medicine and technology. </p> <p>Take the case of Paolo Macchiarini, a surgeon who claimed to have solved the rejection problem in trachea transplants (whereby the body rejects the transplanted organ). Not only that, he managed to convince the scientific establishment of his successes and for a while became a rockstar in his field.[@ritchie2020] But the reality was far from a success story: Macchiarini had exaggerated or outright lied about the effectiveness of his treatments, and tragically, many of the patients to which the procedure was performed to died in the following months or years due to complications from the surgery. </p> <p>One of the most worrying parts of stories like this is how long it took to catch Macchiarini in his wrongdoings. It would be one thing if had he been exposed during the peer-review process or shortly after publishing, but this was not what happened. Macchiarini managed to publish his research in the most prestigious journals in his field, operate on multiple patients, and even land a job at the world-renowed Karolinksa Institute before being exposed.</p> <p>Fraud to the extent commited by people like Macchiarini serves to highlight just \"[...]how much science, despite its built-in organised scepticism, comes down to trust\" (ibid).[@ritchie2020] As a general rule, scientists operate with a certain level of trust towards other scientists: trust that researchers are telling the truth, that they actually conducted the experiments they claim they did, trust that the statistical analysis have been reported correctly, etc.</p> <p>Why do scientists lie? Clearly there can be many reasons. As we saw in the last section, scientists (and human beings in general) might have many different motivations such as the search for status or fame in their fields, among others. But the so-called \"publish or perish\" culture prevalent in academia today certainly does not help. Because it might feel like their whole career is at stake, scientists can feel extremely pressured to get something, anything, published (even if that means resorting to the worse means to make that possible). Of course, this is not to excuse fraud at any level, and there are plenty of other reasons scientists will resort to lying, but it does highlight a structural problem in the social functioning of the science today. We will delve deeper into the problem in the next section.</p> <p>Cheating and fraud will never be completely eradicated from any human endeavour, and cases where fraudsters are  exposed show us that at least not all lies can stay buried in science. However, there remains the question of how much fraud is not being exposed. As Ritchie notes, the image of objectivity and honesty that the scientific community prides itself in, might be exactly what prevents it from spotting fraudsters like Macchiarini in a timely fashion. If the implicit trust level between scientists is 'too high', this might stop them from scrutinising the data and results in enough depth. </p> <p>Another extreme case is that of Elizabeth Holmes, founder of Theranos, the now-infamous company which claimed to have developed breakthrough health technology to automate and miniaturise blood tests. This technology  supposedly enabled hundreds of tests to be done with just a single drop of blood. A few years after raising millions of dollars for Theranos, (which even earned Holmes the title of youngest self-made female billionaire), she was exposed as a complete fake. Her technology did not work at all. The company had given customers innacurate test results which in many cases compromised their health. Once again, the fact that it took years and millions of dollars in investements to realise the extent of her fraud, unsurprisingly may cast doubt on the public perception of science as honest and legitimate.</p>"},{"location":"skills-tracks/ped/chapter5/trust/#5-bias-negligence-and-hype","title":"5. Bias, negligence, and hype","text":"<p>Scientists do not have to commit outright fraud to skew their results in ways which can, in the long run, diminish public trust in science.</p> <p>There are other ways in which scientific results can be presented to make them seem more robust than they really are. As we saw, the incentives in the \"publish or perish\" culture are such that scientists are driven to put the possibility of publication over every other consideration, which can lead to biases in their research as well as sloppiness.<sup>1</sup></p> <p>The drive is not only to publish, but to publish \"attention-grabbing, unequivocal, statistically significant results\" (ibid, emphasis added), and it makes for one of the biggest sources of bias and skewed results in science.<sup>2</sup> Researchers know that certain kinds of studies are very unlikely to get published.  Studies which do not find any new and surprising effect, studies which (only) replicate previous findings, or studies with no statistically significant results have very slim chances of getting published in peer-reviewed journals regardless of how rigorous the methodology is. Obviously, this is a big problem. </p> <p>In a perfect world, studies would be published based almost solely on their methodological virtues, paying no attention to how new or surprising the effect found is (or to whether an effect is found at all). If a study is designed properly, its results should be of interest to the scientific community whether the result is positive, negative or null.[@ritchie2020]</p> <p>Instead what we get is scientists not publishing (and sometimes not even writing up) research which did not find any statistically significant results (sometimes refered to as the 'file-drawer effect'). If only studies with statistically significant effects are published while others which show a smaller effect or no effect at all never see the light of day, the whole literature in the area will overstate the effect(s). </p> <p>In fact, this is partly what is the driving the so-called replication crisis in academia, where famous studies which established relevant effects cannot be replicated by other researchers. The problem is widespread and well-documented, and it seems to be the most prevalent in social sciences like psychology as well as biological or medical sciences.[@ioannidis2005] In fact, in a study published in Nature in 2016, over 70% of the 1,500 researchers who filled out a questionnaire, declared that they had tried and failed to replicate other scientists experiments and over half of them failed to replicate their own experiments.[@baker2016] Not only that, but over 60% of respondents claimed that two factors, pressure to publish and selective reporting, were driving the problems in replicability.</p> <p>Does this mean the original scientists were lying? No, not at all. More likely, they just 'got lucky' and their data showed bigger effects than what the 'real' effect is (or what one would expect to get on average if one runs the experiment many times). In any case, if only the 'lucky' studies get published, the overall effect in question will most likely be inflated.</p> <p>But there are also other reasons which contribute to the inflation of research results. An all-too prevalent practice in academia, known as p-hacking is one of them. P-hacking refers to a set of practices where scientists slightly nudge (or hack) their p-values until something reaches the almost holy grail status of statistic significance. They can re-run almost identical versions of their regressions until they get a statistically significant result, drop certain data points, change the statistical tests used, or even take a data set with no particular hypothesis in question and just see what sorts of effects are statistically significant. The pressure to publish makes this kind of behaviour way too common in current scientific practice, to the point where many researchers might not even think they are doing anything wrong.</p> <p>One particularly revealing case is that of Professor Brian Wansink, for a long time one of the most important voices on food psychology (the famous studies which shows that people who use bigger plates tend to eat more food comes from his lab). Professor Wansinsk inadvertently outed himself when he wrote a blog post detailing how he adviced a student whose original hypothesis had \"failed\". Wansink encouraged his student to keep mining the data until something was salvaged. The blog post got other scientists revising Wansink's work, which led to the retraction of many of his studies, and with his resignation from Cornell University where he had been head of the Food and Brand Lab.[@ritchie2020]</p> <p>As bad as Wansinks case is, he is hardly alone. A 2012 poll of over 2,000 psychologists asked whether they had ever engaged in p-hacking[@john2012]: 65% admitted to collecting data on several different outcomes but reporting on only some of them, 40% claimed to have excluded particular datapoints after looking at the results, and 57% said they decided to collect further data after running their analyses.[@ritchie2020] </p> <p>Scientists might just also be negligent when checking their findings which may skew their overall results. As Ritchie rightfully points out, when researchers get 'good' results, that is, results which they think are likely to get published, they will probably feel excited (and perhaps relieved), and then move on. Conversely, if the results are 'bad' (unlikely to get published), they might scrutinise them in detail to make absolutely sure that such a dissapointing result is correct. If this kind of uneven behaviour is consistent, then all flawed null results will get corrected, but most statistically significant which are flawed will not, further inflating the statistically significant effects found.[@ritchie2020]</p> <p>Finally, there is also the over-hype of scientific results. There is no shortage of examples where scientists use words like 'unique', 'robust' and 'unprecedented' to describe their work, and the most prestigious journals pride themselves in publishing studies of \"exceptional importance\" (Proceedings of the National Academy of Sciences) and papers which have \"great potential impact\" in their fields[@ritchie2020]. </p> <p>As we saw, it is highly unlikely that the most methodologically rigorous studies will all find unique and unprecedented findings. However, there is pressure to present them this way since researchers might feel like it's language that appeals to readers and to reviewers and editors of prestigious journals.</p> <p>All of these issues end up heavily skewing data towards mostly positive results which many times are inflated due to a combination of the effects just described. The replication crisis in academia and the (correct) perception that there are relevant problems in the way the incentives are currently set up for scientits, provides us with good reasons to be at least somewhat weary of scientific findings.</p>"},{"location":"skills-tracks/ped/chapter5/trust/#6-lack-of-control-over-the-message","title":"6. Lack of control over the message","text":"<p>No matter how much care researchers take when communicating and engaging with the public, the truth is, no one is entirely in control of the message they put out. Scientists might be misquoted or misinterpreted in the media by unscrupulous journalists or simply by errors of miscommunication. And scientific findings may be spun in ways which are designed to grab people's attention while not necessarily communicating in the most truthful way.</p> <p>Going back to pandemic examples, since the Covid-19 vaccine has been rolled out, there have been numerous claims that more vaccinated people are dying of Covid than unvaccinated. While technically this may have been true in some cases, (that is, the number of deaths of vaccinated may have been higher than that of the unvaccinated). However, these numbers failed to take into account that as the vaccine uptake increased, the proportion of the unvaccinated grew smaller and smaller. So, although the total number of deaths among the unvaccinated may have been small, if you take into account the total number of unvaccinated people, the proportion of deaths among the unvaccinated was a lot higher than that of the unvaccinated.[@spiegelhalter2021] Examples like this show the importance of science communication: it is not enough to get the numbers 'right' (in the sense that they are not adulterated or fabricated), one needs to be be able to read them properly.</p> <p>Even in cases when researchers are successfully able to convey their message across, it is impossible to control how this message might be then further distorted or changed in social media or otherwise.</p> <p>As we saw in sections 9 and 10, messages which cause strong emotions (such as anger or moral outrage) spread much more quickly through social media than unemotional or nuanced ones. Again, this is in great part due to the workings of the content-filtering algorithms which are programmed to show us content which is most likely to grab our attention and/or get users to share it. </p> <p>Click-bait headlines and misleading quotes are therefore easily propagated through social media and scientists engaging with the media and general public should be well aware of that. One cannot control headlines and quotes, but it is important that researchers are mindful of these issues, as well as be willing to engage with journalists when they feel they are being misquoted. It is important to remember that engagement does not end after giving an interview, it is important to follow up and clarify one's message when needed.</p>"},{"location":"skills-tracks/ped/chapter5/trust/#7-mistreatment-and-discrimination-of-marginalised-communities","title":"7. Mistreatment and discrimination of marginalised communities","text":"<p>Science history is riddled with cases of sexism and racism being passed off as 'objective science', with some  particularly gruesome episodes such as the Tuskegee studies in the United States during the 20<sup>th</sup> century. There is no shortage of examples where science was used to justify discriminatory practices and worldviews.</p> <p>It is unsurprising that the long history of science's racist and sexist practices can make people from historically oppresed communities suspicious or distrustful of science, evidenced for example in African American women's distrust of the birth control pill when it first emerged,[@grasswick2010] or in the higher rates of vaccine hesitancy of marginalised groups both for vaccines in general as well as  and response to the Covid-19 vaccine.[@nguyen2022] </p> <p>Given its history, how does trust in science fare when in comes to data science and AI? When these technologies were first used, and as their use became widespread, it was thought that they would eradicate (or at least greatly diminish) biases and discrimination (such as racism or sexism) from scientific practice. The reason for this was that data and AI were broadly perceived as 'neutral and objetive'. It was humans, not algorithms, who were full of biases. </p> <p>We now know this way of thinking is grossly mistaken. If anything, algorithms can amplify the already existing human biases, regardless of whether these biases are conscious or not. Sadly, there are far too many examples  in recent years. A 2019 paper published in Science showed how an algorithm used in US healthcare to predict patients' needs was producing racist results.[@obermeyer2019] The bias was introduced because the algorithm used past health costs as a proxy for health needs, which inadvertently favoured White patients. Less money was spent on Black patients with the same level of need as their White counterparts, and the algorithm thus falsely concluded that Black patients are healthier than equally sick White patients.</p> <p>Another famous example is Amazon's hiring algorithm which turned out to be biased against women. In an attempt to automate their hiring practices, Amazon developed an experimental hiring tool which used artificial intelligence to give job candidates scores ranging from one to five stars.[@dastin2022] The algorithm quickly taught itself to discriminate against women candidates, penalising resumes which included the word women, (in 'women's chess club' for instance), as well as downgrading resumes which came from all-women colleges (ibid). Although the algorithm is not used by the company, it was actually taken down precisely because of concerns about its sexism,[@dastin2018] it serves as a powerful example of how AI can perpetuate and amplify historical biases (such as learning from the fact that traditionally Amazon has not hired many women, and extrapolating that to mean that women are not good employees).</p> <p>Because discrimination is often embedded into technology, people from marginalised groups have a rational reason to distrust it, all the more if these technologies are untruthfully portayed as just the opposite: neutral and imparcial. Therefore, it is crucial to be aware of the potential biases of algorithms, reminding ourselves that no technology is ever truly neutral.</p>"},{"location":"skills-tracks/ped/chapter5/trust/#8-misuse-of-data","title":"8. Misuse of data","text":"<p>Often scientists and researchers can abuse their power and the trust members of the public have given them. The world of data and AI is full of opportunities to do so, especially if we take into account the huge asymmetry of information between researchers and the public in terms of how data is collected, used, and how the algorithms work.</p> <p>A famous example is the so-called Facebook emotional contagion study.[@kramer2014] A group of researchers at Facebook and Cornell University studied how emotional contagion spread across the social network. To do this, they manipulated the News Feed of Facebook users, either to reduce positive messages (thus amplifying negative ones), reduce negative messages (thus amplifying positive ones), or reduce messages at random (control condition). </p> <p>The level of emotional contagion was measured by the proportion of positive and negative messages the manipulated users themselves then posted. The study found that when positive expressions of emotion were reduced, people produced fewer positive posts and more negative posts; when negative expressions were reduced, the opposite pattern occurred. The article was published in the prestigious Proceedings of the National Academy of Sciences (PNAS).</p> <p>However, it was met with outrage from the public.[@meyer2014]<sup>-</sup>[@chambers2014] The company claimed it had not violated Facebook's terms and conditions (they explicitly invoked them to argue that users' acceptance of these T&amp;C constituted informed consent for the research). However, the public did not agree. None of the users knew they were part of this research, much less what the research amounted to in terms of manipulation of their News Feed. The realisation of how easily Facebook could manipulate its users (for research or other purposes), angered, scared, and increased people's distrust in technology. It is interesting to note that one of the company's defenses actually had to do with how much and how often they already tweak and manipulate the algorithm.[@meyer2014] As we will see in the next section, this is no cause for celebration.</p> <p>This episode also highlights how unaware researchers might be of the public's concerns. These researchers decided to publish their research in one of the most prestigious journals in science where it would receive a lot of attention. Presumably, it did not occur to them that other researchers and the general public would be outraged at their privacy policies and would find the study was unethical. This reminds us of the importance of public deliberation, and having conversations which can allow trust to flourish between scientists and the public.</p>"},{"location":"skills-tracks/ped/chapter5/trust/#9-online-misinformation-and-disinformation","title":"9. Online misinformation and disinformation","text":"<p>We now turn to misinformation and disinformation. Misinformation refers to information which is incorrect or inaccurate whereas disinformation has been \"[...]used to denote a specific type of misinformation that is intentionally false\".[@scheufele2019]<sup>-</sup><sup>3</sup> Unsurprisingly, they both can cause important damage to the relationship of trust between the public and the world of science and research. When exposed to misinformation people might become confused about what the scientists are saying and start to distrust scientists' motivations, which can then lead to overall distorted narratives about the state of scientific evidence for any given topic.</p> <p>Misinformation is certainly not a new phenomena, but it seems to have become increasingly more prevalent over the last years. It is now well-documented that fake news is more likely to be retweeted and spread online than real news,[@vosoughi2018] and the Internet and social media can sometimes seem to be infested by it.</p> <p>The link with algorithms and technology is direct (although possibly unintended as such). As we previously stated, no piece of technology is neutral. It is designed by humans with particular aims in mind and it can perpetuate and amplify human biases.</p> <p>In the case of social media, the algorithm which determines the users' News Feed is maximising for one thing: time spent on, and interacting with, the platform. Because of the business model of social media companies, they are competing for users' attention (sometimes referred to as 'the attention-economy').[@zuboff2019]<sup>-</sup>[@center]</p> <p>Therefore, content-filtering algorithms are designed to show us content which is most likely to grab our attention and thus keep us on the platform. Sadly, it seems that emotional content which angers or outrages us is an easy way to do so. Studies have shown that emotional, and particularly angry messages spread much faster in social media (one of them is the infamous Facebook emotional contagion study from the last section).[@kramer2014]<sup>-</sup>[@chen2017]<sup>-</sup>[@crockett2017]<sup>-</sup>[@brady2017] If the message being spread is fake or otherwise distorted, it is easier to make it as outrageous as required. Even though the algorithm was designed with the aim of maximising attention and engagement in mind, we can see how this can inadvertently also end up promoting fake news and misinformation.</p> <p>Widespread misinformation is detrimental to public trust in science for at least two reasons. First of all, if misinformation is rampant in social media, misinformation about science and scientists will not be an exception. The amount of misinformation and outright fake news about the Covid-19 pandemic and the response to it has sadly given us (too) many examples of this over the last two years. </p> <p>Additionally, an environment rife with misinformation promotes an overall worldview that maybe there is no 'real' information (sometimes known as post-truth), and that instead we are just confronted with people using supposed information to push their own interests forward in the public stage. As we will see in the next section, when this is combined with siloed communities which distrust anyone outside it, it can lead to very problematic epistemic outcomes.</p>"},{"location":"skills-tracks/ped/chapter5/trust/#10-filter-bubbles-and-echo-chambers","title":"10. Filter bubbles and echo chambers","text":"<p>This leads us to epistemic filter bubbles and, even more epistemically pernicious, echo chambers. Again, these are not problems exclusive to social media platforms and the algorithms that fuel them, but the latter certainly play a role in making them more ubiquituous as well as amplifying their contents.</p> <p>C.Thi Nguyen (2020) proposes a useful disintion between  epistemic bubbles and echo chambers. He defines an epistemic bubble as a \"social epistemic structure which has inadequate coverage through a process of exclusion by omission\".[@nguyen2020] That is, it is a filter bubble which omits certain views and positions. The key here is that this inadequate coverage occurs through omission. There is no need for ill-intent in the creation of epistemic bubbles, they can arise \"[..]through the ordinary processes of social selection and community formation\" (ibid).</p> <p>In fact, epistemic bubbles can be quite common in everyday life. We might find ourselves in one if we only buy newspapers of a certain political leaning, or only speak to friends who hold similar worldviews to us. Social media News Feeds can in many occasions become epistemic bubbles as people mostly interact with others who are similar to them.</p> <p>The good news about epistemic filter bubbles is that they can be burst through sufficient exposure to information from outside of the bubble.[@nguyen2020] In this case, someone's warped view of the world is mainly due to lack of exposure to a variety of views on certain issues. Therefore, the solution is relatively easy: in order to burst the bubble, people should be exposed to many varied worldviews and opinions.</p> <p>An echo chamber however, is another story. Unlike an epistemic bubble, a lack of diversity is not the main reason people become polarised and entrenched in their views. Instead, Nguyen defines an echo chamber as a \"social epistemic structure in which other relevant voices [those outside of it] have been discredited\" (ibid), which implies at least a certain level of intentionality in the discrediting of those not part of the echo chamber.</p> <p>In fact, the crucial element of an echo chamber as an epistemic community is that there is a \"[...]significant disparity in trust between members and non members\".[@nguyen2020] Members of the echo chambers are given almost infinite credence when they voice their opinions and views, while beliefs of those outside it are completely discredited. It is a process similar to cult indoctrination, and it is very easy to see why it is so pernicious.</p> <p>By preemptively dismissing the opinions of those who do not share the beliefs of those inside the echo chamber, it is easy to epistemically insulate oneself to the point that even evidence which contradicts your views and should give you reason to reevaluate, instead ends up confirming your original views even further.</p> <p>An echo chamber might be the most dangerous challenge to trust in science: once someone is inside one where scientists are considered outsiders and discredited, it is almost impossible to get them to reconsider their beliefs, especially when the suggestion to do so comes from those outside the echo chamber. Once again it is important to remember that although echo chambers are not completely online phenomena, the way algorithms are employed in social media do seem to aid in their formation.</p> <ol> <li> <p>For a detailed explanation of these phenomena, see Stuart Ritchie's book, Science Fictions (2020).\u00a0\u21a9</p> </li> <li> <p>Even the crucial concept of 'statistically significant' has led to a lot of confusion, as the word significant seems to allude to an effect which is big and important in some way, where it actually means that the effect found is sufficiently different from what we would expect to see if there was no effect (Ritchie, 2020, 133).[@ritchie2020] The related concept of p-value has also lent itself for gross misunderstanding. In fact, a study found that 89% of Introduction to Psychology textbooks got the definition wrong.[@ritchie2020]<sup>-</sup>[@cassidy2019]\u00a0\u21a9</p> </li> <li> <p>In order to be concise, I will use the term misinformation to refer to both misinformation and disinformation unless explicitly stated.\u00a0\u21a9</p> </li> </ol>"},{"location":"skills-tracks/rri/","title":"About this Skills Track","text":"Your browser does not support the video tag.  <p>Skills Track Information</p> <ul> <li>Title: Responsible Research and Innovation</li> <li>Authors: Dr Christopher Burr and Claudia Fischer</li> <li>Last updated: March 2023</li> <li>Status: In Development</li> <li>Citation Information:  <pre><code>Burr, C., Fischer, C., and Rincon, C. (2023) Responsible Research and Innovation (Turing Commons Skills Track). Alan Turing Institute. [10.5281/zenodo.7755693](10.5281/zenodo.7755693).\n</code></pre></li> </ul> <p>Responsible scientific research and technological innovation (RRI) is a vital component of a flourishing and fair society. As an area of study and mode of enquiry, RRI plays a central role within academic, public, private, and third-sector organisations. This skills track will explore what it means to take (individual and collective) responsibility for (and over) the processes and outcomes of research and innovation in data science and AI. The notion of 'responsibility' employed throughout this skills track will be grounded in an understanding of the moral relationship between science, technology, and society, exploring both historical and contemporary examples of RRI practices.</p> <p>As well as looking at the theoretical basis of RRI, this skills track will also take a hands-on approach by exploring a variety of tools and procedures that can help operationalise and implement a robust notion of responsibility within research and innovation practices.</p> <p>The skills track is organised around core and optional modules. It starts with two core modules on What is responsible research and innovation? followed by a module on The project lifecycle model - a heuristic model used to represent the different stages of an ML or AI project.</p> <p>It then offers five optional modules (of which at least one must be completed), based on the SAFE-D principles: Sustainability, Accountability, Fairness, Explainability, and Data stewardship. These overarching ethical principles must then be put into practice through a series of tools and methods covered in the modules. Finally, the skills track ends with a core module on Responsible Communication and Open Science, focusing on the importance of making research open and reproducible, as well as communicating results in an accessible manner.</p>"},{"location":"skills-tracks/rri/#learning-objectives","title":"Learning Objectives","text":"<p>This skills track has the following learning objectives:</p> <ul> <li>Understand what is meant by the term \u2018responsible research and innovation\u2019, including the motivation and historical context for its increasing relevance.</li> <li>Identify and evaluate the ethical issues associated with the key stages of a typical data science or AI project lifecycle: (project) design, (model) development, (system) deployment.</li> <li>Explore practical tools and mechanisms for operationalising the several ethical principles, which have been designed to guide the responsible design of data science and AI projects.</li> <li>Understand the importance of responsible communication in the design, development, and deployment of data science and AI projects, and explore ways to exercise this responsibility.</li> </ul>"},{"location":"skills-tracks/rri/#table-of-contents","title":"Table of Contents","text":"<ul> <li> <p> What is Responsible Research and Innovation?</p> <p>This module looks at foundational concepts and topics associated with responsible research and innovation (RRI).</p> <p> Go to module</p> </li> <li> <p> The Project Lifecycle</p> <p>This module introduces the model and framework of the ML/AI project lifecycle, and explores its constituent stages.</p> <p> Go to module</p> </li> <li> <p> The SAFE-D Principles</p> <p>A set of optional modules that explore the SAFE-D principles\u2014a set of guiding principles for the responsible design of data science and AI projects.</p> <p> Go to module</p> </li> <li> <p> Responsible Communication</p> <p>This module explores and critically examines what it means to act responsibly when communicating the processes by which a project is governed. (Coming Soon!)</p> </li> </ul>"},{"location":"skills-tracks/rri/further-resources/","title":"Further Resources (Responsible Research and Innovation)","text":"<p>Coming soon!</p>"},{"location":"skills-tracks/rri/rri-100-1/","title":"Understanding Responsibility","text":"<p>Novel technologies invariably create new and sometimes unexpected means to transgress moral and social norms.</p> <p>For example, it is considered rude in many cultures to not pay attention when someone is speaking with you. However, the shift to online communication technologies, spurred on by the COVID-19 pandemic, has resulted in changing expectations about when it is deemed \"rude\" to not look at someone when they are speaking on Zoom. New questions arise with the adoption of new technologies.</p> <p></p> <p>Questions</p> <ul> <li>Is it rude to use the chat function when in a group meeting?</li> <li>Does it enable additional means for communicating that bring more people into the conversation, or does it just create distracting back channels akin to passing notes at the back of a classroom?</li> </ul> <p>Addressing such questions and challenges can be difficult for myriad reasons:</p> <p>Some more questions</p> <ul> <li>How can we be confident that we have reached a satisfactory, acceptable, or morally permissible conclusion?</li> <li>Which processes can we follow to help us justify and be confident in our conclusions?</li> <li>Who should be a part of the deliberative process, and are there barriers that prevent some groups from participating?</li> </ul> <p>These questions are challenging and they may be answered in more than one way. Therefore, having a framework to help address them, would be incredibly useful. And this is exactly what Responsible Research and Innovation provides us with.</p> <p>Responsible Research and Innovation</p> <p>Responsible Research and Innovation is a framework for reflecting on, anticipating, and deliberating about the ethical, social, and legal questions that arise in the research and development of scientific and technological tools, practices, and systems.</p> <p>If we could capture the essence of the motivation for RRI as a framework in a single sentence, we would be hard pressed to improve upon the following quotation:</p> <p>Technology is neither good nor bad; nor is it neutral. -Melvin Kranzberg</p> <p>Evaluating whether technology is likely to create harms versus opportunities, and identifying how such harms and opportunities are distributed throughout society is complicated. However, if we cannot address these challenges in a reasonable and socially acceptable manner, we will be unable to take responsibility over the research and development of disruptive data-driven technologies.</p> <p>Before picking apart the concept of responsibility and its different varieties, let us first delineate the difference between two concepts often muddled together: responsibility and accountability.</p>"},{"location":"skills-tracks/rri/rri-100-1/#responsibility-versus-accountability","title":"Responsibility Versus Accountability","text":"<p>The concepts 'responsibility' and 'accountability' are often treated as synonyms in everyday conversation.  However, in this module they will be treated as distinct technical terms with precise meanings.  Their respective meanings will be made clear over the course of this module.  For the time being though, the following graphic helps clarify an important difference.</p> <p> </p> Figure 1. Differentiating the forward-looking type of responsibility that is important for RRI from backwards-looking 'accountability'. <p>Whereas 'accountability' is something that is applied retroactively (e.g. someone is held accountable for the consequences of their actions), 'responsibility' has a forward-looking character.  That is, responsibility is an attitude, duty, or moral virtue that we take up and carry with us as we interact with others in the world.</p> <p>This directionality is important.  Many of the practical methods and tools discussed in this skills track have an anticipatory property associated with them.  In this sense they reflect the forward-looking property of responsibility.</p> <p>With a sketch of this distinction in mind, we can now start looking at the concept of 'responsibility' in more detail through two questions that will help us pick the concept apart.</p> <p></p>"},{"location":"skills-tracks/rri/rri-100-1/#two-questions-about-responsibility-in-science-and-technology","title":"Two Questions about Responsibility in Science and Technology","text":""},{"location":"skills-tracks/rri/rri-100-1/#1-the-manhattan-project","title":"1) The Manhattan Project","text":"<p>In April 1945, Michael Polanyi\u2014a chemist and sociologist of science\u2014and Bertrand Russell\u2014a philosopher and logician\u2014were speaking on a radio programme about the practical implications of the famous formula, \\(E=mc^2\\).</p> <p>They were asked whether the formula had any practical applications for society, but neither could provide an answer.  Three months later the Manhattan project dropped the first of their three atomic bombs!</p> <p>Question</p> <p>Did Einstein have any responsibility for the consequences of the Manhattan Project?</p>"},{"location":"skills-tracks/rri/rri-100-1/#2-the-harmless-torturer","title":"2) The Harmless Torturer","text":"<p>The philosopher Derek Parfit (1984) famously offered a series of thought experiments concerning so-called \u201charmless torturers\u201d.<sup>1</sup></p> <p>In the first scenario, you enter a room and see an individual strapped to a chair, connected to various pads and wires that deliver an electric current to the victim.</p> <p>In front of you there is a dial with numbers ranging from 1 to 1000 that controls the electric current.  You turn the dial by a single increment, increasing the electrical current so slightly that the victim is completely unable to perceive any difference in intensity.</p> <p>While certainly not a morally praiseworthy action, in this first scenario you could not be plausibly held responsible for causing any harm.</p> <p>However, we now run the thought experiment for a second time.  In this version you turn the dial by the same small, and imperceptible, increment.  But, at the same time as you do this, 999 other people turn similarly connected dials, all by one increment each.</p> <p>The net result of this collective action is an intensely painful electric shock that ends up killing the restrained victim.</p> <p>Questions</p> <ul> <li>What is your level of responsibility for killing the individual in the second scenario?</li> <li>Are there factors that may increase or mitigate your level of responsibility?</li> </ul> <p>Now, lets's update Parfit's original thought experiment.</p> <p>In this modern version, you are waiting for a bus, tired from a long day at work.  You are mindlessly scrolling through a list of possible videos that have been presented to you by a recommendation system that powers your video streaming app.</p> <p>You select a video of a fiery argument between two political pundits, in which one of them \u201cdestroys\u201d their interlocutor.  Ordinarily you would avoid selecting such a video, knowing that it is likely to be needlessly polarising and sensationalist.  However, you\u2019re tired and occasionally enjoy a spectacle as much as everyone else.</p> <p>Unfortunately, at a similar time, 999 other individuals, with similar viewing histories to yourself also click on the same recommended video.  The effect is that the recommendation system, which powers the social media platform, ends up learning that users similar to yourself and the 999 other individuals are likely to click on videos of this nature towards the end of the day.  As such, in the future it will be more likely to recommend similarly low-quality, politically polarising videos to other users.</p> <p>Question</p> <p>Where does responsibility for the spread of content that has the potential for causing political polarisation within society lie in this example?</p>"},{"location":"skills-tracks/rri/rri-100-1/#types-of-responsibility","title":"Types of Responsibility","text":"<p>In addition to differentiating 'responsibility' from 'accountability', we should also consider several types of responsibility.</p> <p>Types of responsibility</p> <ul> <li>Moral Responsibility: all moral agents (i.e. those with the capacity for ethical decision-making) are capable of receiving praise or blame for their actions (cf. causal responsibility), especially where they are obligated to act because of some moral duty.</li> <li>Legal Responsibility: people have an obligation to act in accordance with the law.  The law can compel people to act in a specific way (e.g. public sector duties) or to refrain from acting in a particular manner (e.g. civil or criminal offences).</li> <li>Professional Responsibility: the roles or responsibilities that are expected of a person in their capacity as a member of a team or organisation (e.g. to carry out research in accordance with institutional norms and best practices).</li> <li>Societal Responsibility: expectations on individuals and businesses (i.e. corporate social responsibility) to act with awareness and appreciation of social, cultural, economic, and environmental issues.  Societal responsibility is often seen as demanding more from businesses than of individuals, as the former may have overriding responsibilities towards their families or local communities.</li> </ul>"},{"location":"skills-tracks/rri/rri-100-1/#morality-and-the-law","title":"Morality and the Law","text":"<p>It is also worth drawing further distinctions between moral and legal responsibility and duties.  This is because there are many legal responsibilities that data controllers and processors may have, even if these topics are outside of the scope of this module.<sup>2</sup></p> <p>Both morality and the law impose duties (or obligations) on individuals and organisations. These duties can be either positive (i.e. requiring action) or negative (i.e. requiring inaction).</p> <p>However, the law tends to impose more negative duties upon individuals than positive duties, whereas morality is often more demanding.  The following quotation captures this point nicely:</p> <p>\"Ethics is knowing the difference between what you have a right to do and what is right to do.\" -Potter Stewart, Associate Justice of the Supreme Court of the United States of America</p> <p>Moral philosophers refer to duties that compel action beyond what is simply permissible as 'supererogatory' duties.<sup>3</sup>  In the context of morality, one may receive praise for going above and beyond (e.g. for donating a significant portion of one's income to charity), but receive neither praise nor blame for simply acting in line with minimal obligations.</p> <p>In contrast, the law tends to set the minimal threshold for acceptable behaviour while remaining silent on what constitutes morally praiseworthy behaviour.  This is especially true in the context of Human Rights Law, where rights are taken to be universal in nature and, therefore, must be accepted by many different people and cultures.</p> <p>NB: Duties on public sector organisations are typical exceptions to this rule.  For instance, the Public Sector Equality Duty in the UK requires public sector organisations to \"Advance equality of opportunity between people who share a protected characteristic and those who do not\" (positive duty), as well as avoiding discrimination (negative duty).</p> <ol> <li> <p>Parfit, D. (1984). Reasons and persons. Oxford University Press.\u00a0\u21a9</p> </li> <li> <p>See our module on Data Stewardship for further information and resources. This module is coming soon!\u00a0\u21a9</p> </li> <li> <p>Heyd, D. (2019). Supererogation In E. N. Zalta (Ed.), The Stanford encyclopedia of philosophy.\u00a0\u21a9</p> </li> </ol>"},{"location":"skills-tracks/rri/rri-100-2/","title":"Collective and Distributed Responsibility","text":""},{"location":"skills-tracks/rri/rri-100-2/#the-problem-of-many-hands","title":"The Problem of Many Hands","text":"<p>Question</p> <p>How should we assign moral responsibility when large groups of people, organized or unorganized, wrongfully cause some harm?</p> <p>This is not just an abstract challenge for moral philosophers. It is known as the 'Problem of Many Hands'<sup>1</sup> and affects areas including:</p> <ul> <li>Anthropogenic Climate Change</li> <li>Medical Negligence</li> <li>Public Policy and Governance</li> </ul>"},{"location":"skills-tracks/rri/rri-100-2/#collective-responsibility","title":"Collective Responsibility","text":"<p>One response to the problem of many hands is to ascribe \"collective responsibility\" to groups of people or organisations.</p> <p>Collective responsibility</p> <p>Collective responsibility, here, is not the same as mechanisms of legal accountability that are used to fine organisations (e.g. limited liability companies).</p> <p>Important</p> <p>Whether we can ascribe collective responsibility is independent from any practical consequence (e.g. fines, sanctions). Here, we are just interested in whether it makes sense, conceptually, to blame a collective.</p> <p></p>"},{"location":"skills-tracks/rri/rri-100-2/#arguments-for-collective-responsibility","title":"Arguments For Collective Responsibility","text":"<p>In large-scale data science and AI projects, the distributed structure of roles and responsibilities is necessary for various reasons:</p> <ul> <li>Individuals have specific forms of expertise, and the delivery of a fully functional AI system requires a plurality of skills. </li> <li>Reuse of datasets is common, and the collection and curation of the original dataset may not have been performed by the software engineer responsible for training a model. </li> <li>Pre-trained models and cloud computing services are increasingly common (e.g. AI as a service, APIs), such that project teams are reliant on third-party infrastructure.</li> </ul> <p>As such, when we ask where responsibility for the behaviour of an AI system lies (e.g. responsibility for biased or discriminatory inferences), it is incredibly difficult to locate it at an individualistic level of abstraction because of the distributed nature of the system's development.  Rather, it seems best located at the level of a project team or organisation (i.e. collective and distributed responsibility).</p> <p>Question</p> <p>If moral praise and blame is only directed at moral agents, does this make companies or organisations moral agents?</p> <p>Holding a group collectively responsible can have pragmatic benefits (e.g. punishing a group for the misbehaviour of an individual creates motivating incentives for team members to hold each other responsible to avoid future punishment).<sup>2</sup> However, our intuitions may still lead us to think that only one person was truly responsible.</p>"},{"location":"skills-tracks/rri/rri-100-2/#arguments-against-collective-responsibility","title":"Arguments Against Collective Responsibility","text":"<p>Moving beyond intuitions, there are different arguments to oppose the notion of collective responsibility. </p> <p>Some of the arguments go against the very idea of collective responsibility making sense as a moral concept, while others focus on more pragmatic considerations on the difficulties of ascribing collective responsibility in practice.</p> <p>Does it make sense conceptually to hold a group itself (as opposed to its individual members) morally responsible?</p> <p>Some people have argued that it not possible for groups themselves to cause harm in the way that is required in order to ascribe them moral responsibility. The argument can get quite technical, but they general idea behind it is that groups (as opposed to individuals) are simply not the kinds of agents who can be held responsible for their actions.<sup>3</sup></p> <p>The following quote by philosopher H. D. Lewis sums up a version of this view quite clearly:</p> <p>\"Value belongs to the individual and it is the individual who is the sole bearer of moral responsibility. No one is morally guilty except in relation to some conduct which he himself considered to be wrong... Collective responsibility is... barbarous.\"<sup>4</sup></p> <p>On the other hand, arguments against collective responsibility can be based on pragmatic considerations. In particular, some argue that holding groups responsible can have negative consequences in how we place blame on others in ways that can be unfair.</p> <p>For example, it can be argued that collective responsibility lets individuals off the hook for personal responsibility.  For instance, consider the phenomena of the 'bystander effect', explained in the following video:</p> <p>The idea is that when we hold a group responsible for an action, this inevitable 'waters-down' any individual responsibility for said action.</p> <p>Those who wish to argue against collective notions of responsibility, therefore, may wish to appeal to intuitions or point to such empirical matter. However, defendants of collective responsibility could respond that the failure is a conceptual issue.  That is, an inability to adequately and precisely specify particular types of responsibility. </p> <p>For instance, in the context of a data science or AI project, while we may not be able to locate responsibility for the overall behaviour, we can identify specific forms of individual responsibility:</p> <ul> <li>Responsibility for transparent documentation of data provenance</li> <li>Responsibility for due diligence when reusing existing artefacts (e.g. datasets, pre-trained models)</li> <li>Responsibility for carrying out thorough risk assessments</li> <li>Responsibility for organising meaningful forms or stakeholder engagement</li> </ul> <p>Some reflections</p> <ul> <li>What do you think? </li> <li>Is collective responsibility a coherent concept? Or should we make do with structured forms of individual responsibility?</li> </ul> <p></p>"},{"location":"skills-tracks/rri/rri-100-2/#one-more-thing-the-replicability-crisis","title":"One More Thing: The Replicability Crisis","text":"<p>Philosopher of Science, Sabina Leonelli, has argued that the replicability crisis is evidence of the challenge of locating moral responsibility and professional accountability within large-scale research projects:</p> <p>The recent \u2018replicability crisis\u2019 in psychology and biomedicine, which many perceive as evidencing an overwhelming lack of research integrity and a failure of peer review, could also be interpreted as illustrating the difficulties in making individuals accountable for their data processing actions within large research networks\u2014which in turn generates problems when attempting to reconstruct, describe and evaluate the methods and assumptions made in any one piece of research.<sup>5</sup></p> <ol> <li> <p>Thompson, D. (1980). Moral responsibility and public officials: The problem of many hands. American Political Science Review,74(4), 905\u2013916. https://doi.org/10.2307/1954312 \u21a9</p> </li> <li> <p>Admittedly, this is a very punitive way of thinking about the reasons people have for acting responsibly and holding each other responsible. There are many other reasons, such as those grounded in values, that motivate individuals to act responsibly.\u00a0\u21a9</p> </li> <li> <p>Smiley, M. (2022). Collective responsibility. In E. N. Zalta &amp; U. Nodelman (Eds.), The Stanford encyclopedia of philosophy. https://plato.stanford.edu/archives/win2022/entries/collective-responsibility/&gt;.\u00a0\u21a9</p> </li> <li> <p>Lewis, H. (1948). Collective responsibility. Philosophy, 23(84), 3-18.\u00a0\u21a9</p> </li> <li> <p>Leonelli, S. (2016). Locating ethics in data science: Responsibility and accountability in global and distributed knowledge production systems. Philosophical transactions of the Royal society A: Mathematical, physical and engineering sciences, 374(2083).\u00a0\u21a9</p> </li> </ol>"},{"location":"skills-tracks/rri/rri-100-3/","title":"Defining Responsible Research and Innovation","text":""},{"location":"skills-tracks/rri/rri-100-3/#a-definition-of-responsible-of-research-and-innovation","title":"A Definition of Responsible of Research and Innovation","text":"<p>While this module does not advocate for a single definition of 'responsible research and innovation', the following definition is a good starting point for understanding the term:</p> <p>Responsible Research and Innovation is a transparent, interactive process by which societal actors and innovators become mutually responsive to each other with a view on the (ethical) acceptability, sustainability and societal desirability of the innovation process and its marketable products (in order to allow a proper embedding of scientific and technological advances in our society). - Rene Von Schomberg<sup>1</sup></p> <p>Let's break this definition down.</p> <p>First, we have the following phrase:</p> <p>\"transparent, interactive process by which societal actors and innovators become mutually responsive to each other\".</p> <p>This is straightforward enough. Without transparency and interaction between innovators, researchers, and society, the outcomes or consequences of research and innovation cannot be meaningfully scrutinised or challenged, and possible harms or unintended consequences may go unnoticed.</p> Illustrative example <p>For example, if a medical research team failed to interact with and explain to their study participants the risks of a novel drug they are testing, the lack of transparency may prevent the participants from giving their meaningful and informed consent to the possible risks involved with their participation.</p> <p>Second, we have the goal to which the transparent, interactive process is directed:</p> <p>\"with a view on the (ethical) acceptability, sustainability and societal desirability of the innovation process\".</p> <p>A process of responsible research and innovation should be dialogical and inclusive to make space for the unavoidable plurality of values that are implicated within the research and innovation lifecycle.</p> <p></p> <p>For instance, many would agree that technological advances in renewable energy are desirable because of their contribution to the creation of a more sustainable future. However, there may be disagreement about the specifics of a particular project, such as the development of a hydroelectric power plant that displaces downstream communities by disrupting the local ecology. Such a tension is one between competing values and interests, which can only be observed and resolved through a transparent and interactive process with affected and impacted stakeholders.<sup>2</sup></p> <p>Finally, von Schomberg's definition draws our attention to how RRI facilitates a</p> <p>\"proper embedding of scientific and technological advances in our society\".</p> <p>This is another way of saying that scientific research and technological innovation do not operate in a vacuum. This part of the definition draws our attention to the fact that the practices and processes or research and innovation occur at a specific place and time, and also to an awareness of their consequences, which can vary in scope and impact. That is, they are \"embedded\" in society.</p> <p>Some obvious examples of research and innovation with wide-reaching impacts include the development of the atomic bomb, the discovery of penicillin, or the invention of the internet. But all research and innovation has the potential to reshape societal practices and social norms or expectations in unexpected ways.</p> <p>A less obviously important example</p> <p>An example of this is the recommendation algorithm used by YouTube.  The algorithm optimises for engagement, i.e. how long people watch the videos recommended to them, and therefore recommends videos deemed most-likely to keep users engaged in the platform for the maximum of time possible. At first flance, the high-reaching implications of this might not be obvious. But as has become increasingly clear over the last few years, YouTube recommendations quickly steer people into conspiracy theories, misinformation, or unhealthy information. From flat-earth conspiracy theories to extreme diets and pro-anorexia videos, YouTube recommendation systems seem drive users towards extreme content at a much higher rate that search algorithms do.<sup>3</sup></p> <p> </p> Depiction of how recommender systems can lead people down (increasingly extreme) rabbit-holes. In this example, a woman starts by searching for video workouts, and quickly ends up being recommended dieting and fasting video. <p>RRI takes this inextricable relationship between science, technology, and society as its starting point, taking seriously the moral duties and obligations that such a relationship inculcates.</p>"},{"location":"skills-tracks/rri/rri-100-3/#an-incomplete-history-of-the-term-responsible-research-and-innovation","title":"An (Incomplete) History of the term 'Responsible Research and Innovation'","text":"<p>The term 'responsible research and innovation' is strongly associated with the European Commission's Framework Programmes for Research and Technological Development\u2014a set of funding programmes that support research in the European Union.</p> <p>Beginning with the seventh framework programme in 2010, and continuing on through Horizon 2020 (FP8), the term 'responsible research and innovation' became increasingly important for the European Commission's policy.</p> <p>Since the European Commission's initiative, other national funding bodies have also shown a commitment to RRI. For example, in the United Kingdom, the UKRI's Engineering and Physical Sciences Research Council have developed the AREA framework, which sets out four principles for RRI: Anticipate, Reflect, Engage, and Act (AREA).</p>"},{"location":"skills-tracks/rri/rri-100-3/#the-area-framework","title":"The AREA Framework","text":"<p>The AREA framework is a principles-based framework.  That is, the framework comprises four principles that serve to operationalise the term 'responsible research and innovation', while also providing high-level guidance on practical decision-making.</p> Operationalise <p>The term 'operationalise' (or, operationalisation), in the context of principles, means to further refine or specify the principle in actionable or measurable terms so that it can be implemented and evaluated in practice (e.g. during a project). This typically involves creating a set of rules, procedures, or guidelines for how the principle should be applied in a particular context.</p> <p>For example, in a data science project involving sensitive information, the principle 'protect user data and safeguard privacy' (summarised as 'data privacy') may be operationalised through adherence to the following procedures:</p> <ul> <li>Identify what data is considered sensitive and requires extra protection</li> <li>Obtain explicit consent from individuals before collecting or using their sensitive data</li> <li>Regularly review and update the data protection measures in place</li> <li>Provide individuals with the means to access, correct or delete their personal data</li> <li>Establish procedures for reporting and investigating any data breaches</li> <li>Provide training to employees on data protection and privacy practices.</li> </ul> <p>This list could be extended as required by the specific context of the project.</p> <p>While different from the definitional approach discussed previously, the four principles of the AREA framework have a lot in common with Von Schomberg's definition. Let's take a look at each of them.</p>"},{"location":"skills-tracks/rri/rri-100-3/#anticipate","title":"Anticipate","text":"<p>Describe and analyse the impacts, intended or otherwise, that might arise.  Do not seek to predict but rather support the exploration of possible impacts (such as economic, social and environmental) and implications that may otherwise remain uncovered and little discussed. Here we can see the following values and commitments (among others) being expressed or alluded to:</p> <ul> <li>Importance of the interlocking values of 'transparency' and 'understanding' emphasised through the requirement to 'describe and analyse' impacts in a creative and exploratory manner (e.g. through inclusive dialogue with stakeholders)</li> <li>The significance of anticipatory deliberation at the start of a project, to avoid issues such as technological lock-in or technical debt.<sup>4</sup></li> <li>The recognition that \"predicting\" all possible harms or opportunities is unlikely, and that procedures should be established to ensure future harms caused can be suitably remediated.</li> </ul>"},{"location":"skills-tracks/rri/rri-100-3/#reflect","title":"Reflect","text":"<p>Reflect on the purposes of, motivations for and potential implications of the research, together with the associated uncertainties, areas of ignorance, assumptions, framings, questions, dilemmas and social transformations these may bring. Here we can see the following values and commitments (among others) being expressed or alluded to:</p> <ul> <li>The need to clearly identify the underlying (and sometimes competing) values behind a project (e.g. improving patient welfare, generating profit for shareholders)</li> <li>Epistemic humility about what is known and what is uncertain, and a commitment to communicating this uncertainty clearly to stakeholders and affected people.</li> </ul>"},{"location":"skills-tracks/rri/rri-100-3/#engage","title":"Engage","text":"<p>Open up such visions, impacts and questioning to broader deliberation, dialogue, engagement and debate in an inclusive way. Here we can see the following values and commitments (among others) being expressed or alluded to:</p> <ul> <li>The importance of ensuring diverse groups have a meaningful opportunity to participate in these reflections. And, where the project is likely to affect a group of marginalised or vulnerable individuals, that priority is given to ensuring they are empowered to contribute to discussion around dilemmas, transformations, etc.</li> <li>The value of multi-disciplinary collaboration to enhance and widen the scope of possible futures that are envisioned (e.g. how large language models, such as Chat-GPT could continue to alter and shape societal practices).</li> </ul>"},{"location":"skills-tracks/rri/rri-100-3/#act","title":"Act","text":"<p>Use these processes to influence the direction and trajectory of the research and innovation process itself. The final principle draws upon and synthesises the previous principles to embody and enact an ethical approach to responsible action in research and innovation projects.</p> <p>This principle, while less substantive than the previous, should also be understood as a process-based commitment. That is, responsible action is unlikely to be characterised by a one-off decision or action. Rather, it requires consideration of how the various, interlocking actions across a project's lifecycle come together to create a form of collective responsibility, as was discussed in the previous section.</p> <p>CARE &amp; ACT</p> <p>A similar set of principles, called CARE &amp; ACT principles, were developed by Ethics and Responsible Innovation team at the Turing.  If you would like to know more about them, you can go to our course on AI Ethics and Governance. </p> <p>Note: An updated version of the AI Ethics and Governance skills track is coming soon!.</p>"},{"location":"skills-tracks/rri/rri-100-3/#ethics-as-compliance","title":"Ethics as Compliance","text":"<p>One pitfall to be mindful of is treating the principles of RRI as a form of compliance. This perspective reduces a vital and significant process to a mindless tick-box exercise. As Leonelli argues:</p> <p>\"A case in point is that of clinical trials, where top-down effort to provide general guidelines for best practice that has undoubtedly led to real and substantial improvements in overall compliance with the underlying ethical principles over the last two decades\u2014and yet has also made ethical compliance into a \u2018tick-box\u2019 exercise, which researchers often view as a drag on their research time, and which has provided an excuse to delegate away any potential concerns with the ethical implications of research work.\" (Leonelli, 2016)<sup>5</sup></p> <p>This is why anticipation, reflection, and engagement come before action and not at the end of a project to satisfy some pre-existing and external criteria. It is also why our 'project lifecycle model'\u2014introduced in the next module\u2014seeks to embed and operationalise ethical principles throughout a project's lifecycle. In doing so, the requirement of responsible action is not external to the day-to-day research and innovation activities, but embedded within a project's activities. Acting in a responsible manner, therefore, means remaining attentive to the contextual risks and opportunities posed throughout a research or innovation project.</p> <p></p>"},{"location":"skills-tracks/rri/rri-100-3/#science-technology-and-society","title":"Science, Technology, and Society","text":"<p>This summary of 'responsible research and innovation' barely scratches the surface of the relevant literature.<sup>6</sup> However, it is sufficient for present purposes to draw attention to two motivating drivers behind the majority of approaches and framework:</p> <ol> <li>RRI requires a critical awareness of and reflection on the impact that science and technology can have on society</li> <li>RRI involves the appreciation of and continuous commitment to the meaningful participation of members of the public in a dialogue about how science and technology should shape society.</li> </ol> <p>In a later set of modules, we will introduce a new set of principles\u2014the SAFE-D principles\u2014that have been specifically designed to meet the unique needs and challenges of responsible research and innovation in data science and AI. As such, we will not use any of the previous definitions or frameworks. However, it is worth knowing about them to be able to appreciate how our own framework is both influenced by and builds upon complementary approaches.</p> <ol> <li> <p>Von Schomberg, R. (2011). Towards responsible research and innovation in the information and communication technologies and security technologies fields. Publications Office of the European Union.\u00a0\u21a9</p> </li> <li> <p>Our skills track on 'Public Engagement of Data Science and AI' goes into more detail about these topics. Note: An updated version of the Public Engagement of Data Science and AI skills track is coming soon!\u00a0\u21a9</p> </li> <li> <p>You can read more on how this happens in this blogpost by Guillaume Chaslot, one of the computer scientists who helped build YouTube's recommendation algorithm, and play around with the open-source YouTube recommendation explorer he built.\u00a0\u21a9</p> </li> <li> <p>'Lock-in' refers to the situation where early (and foundational) design choices made about, say, the use of a specific software architecture, can lead to a situation where it becomes very challenging to alter the behaviour or function of a system without massive upheaval and expense.  The related term 'technical debt' refers to the cost of the additional rework caused by choosing an easy solution now instead of a better approach that would take longer but avoid the lock-in.\u00a0\u21a9</p> </li> <li> <p>Leonelli, S. (2016). Locating ethics in data science: Responsibility and accountability in global and distributed knowledge production systems. Philosophical transactions of the Royal society A: Mathematical, physical and engineering sciences, 374(2083).\u00a0\u21a9</p> </li> <li> <p>More information can be found in our further resources section. Note: Our further resources section is coming soon!\u00a0\u21a9</p> </li> </ol>"},{"location":"skills-tracks/rri/rri-100-4/","title":"The Scope and Horizon of Responsibility","text":""},{"location":"skills-tracks/rri/rri-100-4/#the-boundaries-of-responsibility","title":"The Boundaries of Responsibility","text":"<p>Consider the following scenario.</p> <p>A research team have developed a predictive model, which is well-validated in a particular context (e.g. for diagnosing an illness in a well-defined cohort of patients; for predicting house prices in a specific locale). They publish their data (withholding any sensitive information), details of their methods, and their model, in accordance with some widely agreed upon protocol or set of practices (e.g. for reproducible, replicable, and open science). They also carefully set out the limitations of their model in a journal article, including the limits of their model's generalisability that apply without further re-training and validation. Finally, they include a permissive license on reuse of their model, but set out specific restrictions for a list of \"non-permitted\" uses of their model, determined based on their own risk assessment activities.</p> <p></p> <p>Now, consider the following questions:</p> <p>What is the team responsible for?</p> <ol> <li>Are the team responsible for any harms caused by a company who try to commercialise their model by embedding it into a novel system that falls under one of the teams \"non-permitted\" uses?</li> <li>Are the team responsible for any harms caused by another research team who deploy the original model in a new context but fail to carry out any of the suggested re-training and validation of the model in the new context (e.g. with updated and representative data)?</li> <li>Are the team responsible for any data leaks from their model that arise because of the methods they chose to use when training the model?<sup>1</sup></li> <li>Are the team responsible for the accidental publication of sensitive data, which occurs as a result of human error when one of their teams uploads the project repository online for the purpose of reproducibility and replicability?</li> </ol> <p>These questions all pertain to the scope or limit of the team's responsibility. Although individual answers to these questions may vary, they are likely to follow this trend:</p> What is the team responsible for? <ol> <li>Team very unlikely to be deemed responsible</li> <li>Team unlikely to be deemed responsible</li> <li>Team may be deemed responsible</li> <li>Team likely to be deemed responsible</li> </ol> <p>In the first case, it is quite obvious that the company are using the model in ways that the original team had taken steps to prevent (i.e. license restrictions on reuse). And, in the second case, the project team had also set out careful limitations on their model by following well-established protocols.</p> <p>But, the third case is where we transition into a gray area.</p> <p></p> <p>Whether you believe the project team to be responsible, whether fully or partially, for the leakage in the third case will depend on several factors. For example, in this third case the issue seems to arise from a design choice made during model training, which could have occurred because of a lack of knowledge within the team about the vulnerabilities of certain training methods. Given the fast-changing nature of data science and AI, knowledge of vulnerabilities such as a model leakage can remain unknown to many teams, especially those with fewer resources and capacity. How sympathetic you are to the challenge of keeping abreast of this fast-paced field will undoubtedly influence your decision about how and whether to attribute responsibility.</p> <p>But, in the fourth case, many are likely to ascribe responsibility to one or more members of the team. For instance, perhaps the project manager should have had greater oversight of the team's data management practices and implemented adequate training? Maybe there should have been safeguards to prevent accidents such as this one from ever happening (e.g. secure research environments that prevent egress of sensitive data)? Or, perhaps the individual who uploaded the sensitive data should have just been paying closer attention and was fully responsible!?</p> <p>The specific answer you give does not matter here. What is important is to acknowledge that the demands of responsibility have boundaries that are often vaguely drawn. Our attempts to reduce this vagueness and better delineate these boundaries requires careful reflection and inclusive deliberation.</p> <p>In the following sections we will explore these boundaries further.</p> <p>Ought Implies Can</p> <p>A widely agreed upon moral standard, attributable to the philosopher Immanuel Kant, which is relevant here is the precept, \"Ought Implies Can\". In short, you are only morally responsible or obligated to perform some action if you are capable of doing so.</p> <p>For example, you would not be responsible for saving a dying person if you lacked the required medical training needed to save them. And, to flip this example on its head, you would be acting irresponsibly if you tried, say, to carry out surgery on someone without having undergone the necessary surgical training, no matter how beneficial your intentions.</p> <p>Rules and principles such as 'ought implies can' also serve to set boundaries on our moral duties, responsibilities, and obligations.</p>"},{"location":"skills-tracks/rri/rri-100-4/#to-whom-are-we-responsible","title":"To whom are we responsible?","text":"<p>Let's start with a thought experiment from the moral philosopher, Peter Singer, which addresses the spatial (or geographic) dimension of moral responsibility.<sup>2</sup> Consider the following scenario.</p> <p>On your way to work each day you walk along a river. One morning, you spot a child that has fallen in and appears to be drowning. However, saving the child would result in you ruining your clothes and being late to work.</p> <p>Singer asks, 'do you have any obligation to jump in and rescue the child?'</p> <p>When asked this question, almost all of us would answer with a resounding \"yes\", except where there are overriding factors (e.g. being unable to swim yourself\u2014another example of the 'ought implies can' precept).</p> <p>Singer continues, 'does the cost of your clothes affect your decision, or would it make a difference that there are other people walking past the pond who would equally be able to rescue the child but are not doing so?'</p> <p>Again, most of us would say, 'no, the cost of clothes should not be valued beyond the life of a child, and it does not matter that others are not acting as they ought to do'.  Another strike against the diffusion of responsibility.</p> <p>And then we come to the crux of the thought experiment: what if the child were far away in another country, and although they are no longer drowning, we could save their life at virtually no cost to ourselves? If proximity or distance make no difference to our moral consideration, as Singer would indeed argue, then as he explains,</p> <p>'we are all in that situation of the person passing the shallow pond: we can all save lives of people, both children and adults, who would otherwise die, and we can do so at a very small cost to us'</p> <p>Whether for the cost of a coffee, a mobile app, or another subscription service, global aid charities can help us save the lives of those in need. Although many of us do not donate to global aid charities, this fact is separate from whether we ought to.</p> <p>Singer is arguing in this famous thought experiment that when asking ourselves 'to whom are we responsible?', geographical considerations should not (at least in principle) play a part in our answer.  That is, Singer argues physical proximity should play no role in determining our duties to help others (as long as one can indeed help someone remotely).</p> <p>While not as forceful as the above thought experiment, many projects involving data-driven technologies give rise to similar challenges about the scope of our moral obligations. For example, the following scenarios all emphasise salient factors related to our capacity for moral concern and the effect of spatial separation:</p> <ul> <li>A data analyst who is physically separated from the subjects in her dataset, viewing them merely as numerical representations on a screen, may be less likely to extend them the same moral consideration for privacy or informed consent as a scientist gathering data during an in-person experiment.</li> <li>A software engineer developing a biometric identity system for a national agency may be unable to fully appreciate the impact that such a system will have on people from poorer backgrounds who have lower levels of digital literacy or access to technology.</li> <li>A climate scientist deciding where to place environmental monitoring systems to record pollution levels may be unable to access demographic information that would reveal how their choices, which are guided only by considerations of maximising coverage, nevertheless fail to monitor neighbourhoods that are overwhelmingly populated by minority groups.  As a result, the benefits of their systems accrue to already affluent areas simply because of the ability to detect, say, air pollution more accurately in the areas where the sensors are located.</li> </ul> <p>Understanding the impact that data-driven technologies can have on society, especially those with international scope (e.g. social media platforms), requires us to consider individuals and communities beyond our immediate sphere of concern. And, in doing so, we often find ourselves drawing an implicit boundary around those we consider and those we do not.</p> <p>Drawing Boundaries</p> <p>This section will not attempt to provide any general guidance on drawing boundaries in practice\u2014beyond the simple precepts such as 'ought implies can'. To do so without first considering the context-specific factors of a particular project would either a) take us too far afield into moral philosophy, or b) simply cause confusion and tangential discussion points. It is sufficient for our purpose to just draw attention to the issue.</p> <p>Our skills track on AI Ethics and Governance, however, goes into more detail on relevant topics.</p> <p>Note: An updated version of the AI Ethics and Governance skills track is coming soon!.</p>"},{"location":"skills-tracks/rri/rri-100-4/#responsibility-for-future-generations","title":"Responsibility for Future Generations","text":"<p>We have explored the spatial dimension of our sphere of moral concern, but what about the temporal dimension?</p> <p>Question</p> <p>How far into the future should or does our responsibility extend?</p> <p>One way to think about this question, popular among decision theorists, is to adopt a function that represents your level of responsibility as dependant on time and increasing uncertainty. For instance, you may think that because it is increasingly difficult to be certain of the consequences of your actions as time increases (recall the AREA framework), responsibility should therefore decline in a way that is proportional to the increase in uncertainty. But what is the shape or properties of such a function?</p> <p>Two possible options include the functions shown in graph A, which represents your level of responsibility as a function that decreases linearly over time, and graph B, which represents it as a function that decreases non-linearly over time.</p> <p> </p> Figure 1: Two graphs depicting different ways responsibility can decline as time passes. <p>Unlike function A, function B would do a better job at capturing the increased uncertainty inherent in decisions made about events that are likely to occur farther into the future. That is, in general, we can have more certainty about events that will happen tomorrow than events that will happen in 1, 10, or 100 years. After all, who knows what society will be like in 100 years? Function B, therefore, suggests responsibility should reduce more rapidly than function A to capture the effects of exponentially increasing uncertainty.</p> <p>These are just two options among many, and omit many questions and details:</p> <ul> <li>Once we have selected a function, how should we then operationalise the decrease in responsibility and, say, construct a measure to guide decision-making (e.g. an increasing reduction in the time spent deliberating about possible impacts for longer-term consequences)?</li> <li>How would such a function interact with the spatial dimension?  Should groups of people that are separated from us both spatially and temporally receive less consideration (e.g. unborn people across the globe)?</li> <li>Do organisations with access to greater resources have a commitment to consider a wider and deeper range of impacts (e.g. social media companies or multi-national technological companies that create globally impactful systems)?  To what extent does this commitment exceed the commitment of a small, national research team?</li> </ul> <p> </p>"},{"location":"skills-tracks/rri/rri-100-4/#next-steps","title":"Next Steps","text":"<p>In this module, we have identified a series of challenges but have not considered how to address them. This is intentional. Our goal so far has been to understand the challenges at a conceptual level.  We have not looked at how we can address the challenges in practical terms.</p> <p>As such, you have encountered many questions without any answers. Fear not, as we move into the later sections, you will begin to encounter practical tools and procedures that can help you address similar challenges when they arise in your own projects.</p> <ol> <li> <p>This article from the PAIR team at Google provides a simple illustration of this phenomenon.\u00a0\u21a9</p> </li> <li> <p>Singer, P. (1997). The drowning child and the expanding circle. New Internationalist. https://newint.org/features/1997/04/05/peter-singer-drowning-child-new-internationalist \u21a9</p> </li> </ol>"},{"location":"skills-tracks/rri/rri-100-index/","title":"About this Module - What is Responsible Research and Innovation","text":"<p>This module sets a foundation for the rest of the skills track. It attempts to understand what responsibility is, focusing on both individual and collective responsibility, and how these ideas impact the way responsibility is distributed across a data-driven or AI project.</p> <p>The module delves into the history of the term responsible research and innovation and why it has become increasingly relevant, as well as questions around the scope of responsibility. What are the boundaries of responsibility? Are we responsible for those far away geographically? For people who have not yet been born?</p> <p>Overall, the module plays a motivating role for the rest of the skills track by highlighting the interconnected nature of science, technology, and society.</p>"},{"location":"skills-tracks/rri/rri-100-index/#learning-objectives","title":"Learning Objectives","text":"<p>This module has the following learning objectives:</p> <ul> <li>Understand what is meant by the term \u2018responsible research and innovation\u2019, including the motivation and historical context for its increasing relevance.</li> <li>Understand the difference between accountability and responsibility.</li> <li>Identify and distinguish between different types of responsibility.</li> <li>Explore the notions of individual, collective, and distributed responsibility, and well as the tension that can arise between them, especially in their application to AI or data-driven projects.</li> <li>Explore and critically evaluate the different frameworks used for defining and operationalising responsible research and innovation.</li> <li>Learn to think critically about the scope of responsibilities, what it's (sometimes vague) boundaries are, and to whom we are responsible.</li> </ul>"},{"location":"skills-tracks/rri/rri-100-index/#table-of-contents","title":"Table of Contents","text":"<ul> <li> <p> Understanding Responsibility</p> <p>This section goes what responsibility is as a concept, how it differs from accountability, and introduces different kinds of responsibilities.</p> <p> Go to module</p> </li> <li> <p> Collective and Distributed Responsibility</p> <p>This section delves into the idea of collective responsibility, what the arguments for and against it are, and why it is relevant for data-driven and AI projects.</p> <p> Go to module</p> </li> <li> <p> Defining Responsible Research and Innovation</p> <p>This section goes over different definitions for responsible research and innovation, why the term has become increasingly relevant, and its connection to STS.</p> <p> Go to module</p> </li> <li> <p> The Scope and Horizon of Responsibility</p> <p>This section focuses on the scope of responsibility. Where do the boundaries of responsibility lie for a specific AI or data-driven project? Are we responsible for spatially distant people? For people who will live in the future?</p> <p> Go to module</p> </li> </ul>"},{"location":"skills-tracks/rri/rri-101-1/","title":"What is the Project Lifecycle?","text":"<p>The proof is in the pudding, so let's start with the model itself.</p> <p> </p> Figure 1: An illustrated version of the sociotechnical project lifecycle model. <p>This model shows the typical stages of a project, which involves the design, development, and deployment of some data-driven technology, such as a ML algorithm or an AI system.</p> <p>This module will explore each of the stages in detail. But let's start with some caveats about how the model has been designed, and how it functions.</p>"},{"location":"skills-tracks/rri/rri-101-1/#a-heuristic-model","title":"A Heuristic Model","text":"<p>The Project Lifecycle is a heuristic model that serves as a \"cognitive scaffold\" to support the collective reflection, deliberation, and decision-making of teams and organisations throughout the various stages of a project's lifecycle. Why do we call it a \"heuristic model\"?</p> <p>All models are wrong, but some are useful. -- George Box</p> <p>This famous statement from the British statistician, George Box, highlights a universal truth about models: they all make various assumptions that affect their representational validity. Our model is no different. It makes assumptions about how a typical project, involving some data-driven technology, is structured (e.g. which stages follow from their predecessors). The reality will differ within and between project teams and organisations. Different projects carried out by the same team could differ, and small organisations are likely to govern projects very different to distributed, multi-national projects.</p> <p>Question</p> <p>The question then is whether and how the project lifecycle model is useful?</p>"},{"location":"skills-tracks/rri/rri-101-1/#scaffolding-reflection-deliberation-and-decision-making","title":"Scaffolding Reflection, Deliberation, and Decision-Making","text":"<p>The model can be useful for several processes or outcomes, including:</p> <ul> <li>initial reflection about the tasks or actions that should be undertaken at the respective stages,</li> <li>deliberation about how the tasks and actions may undermine or promote relevant project goals and objectives (e.g. developing a fair classifier), and</li> <li>ongoing decision-making as the project unfolds and actions are documented.</li> </ul> <p>We will demonstrate and justify these claims over the course of this module.</p>"},{"location":"skills-tracks/rri/rri-101-1/#the-over-arching-stages","title":"The Over-Arching Stages","text":"<p>There are two layers to the model:</p> <ul> <li>The three over-arching stages</li> <li>The twelve lower-level tasks</li> </ul> <p>Let's start with the over-arching stages. </p> <p>Three over-arching stages in the Project Lifecycle model</p> Project DesignModel DevelopmentSystem Deployment <p>The preliminary tasks and activities that set the foundations for the development of the model and system (e.g. impact assessments, data extraction and analysis).</p> <p>The technical and computational tasks associated with machine learning, such as training, testing, validation, and documentation, which are necessary to ensure the model is appropriate for its intended use with the target system.</p> <p>The tasks that ensure the safe and effective deployment and use of the system (and underlying model) within the target environment by the intended users. This stage includes ongoing monitoring, as well as tasks associated with updating or deprovisioning.</p>"},{"location":"skills-tracks/rri/rri-101-1/#properties-of-the-model","title":"Properties of the Model","text":"<p>In addition to its general structure, there are also some noteworthy properties of the model:</p> <ul> <li>The model has a high-degree of representational accuracy for typical <code>research</code> and <code>development</code> projects that involve some form of data science or artificial intelligence (e.g. machine learning algorithms). That is, while it is \"wrong\" in the manner expressed above by George Box, it is less wrong for projects involving data-driven technologies.</li> <li>Although the model is presented as a uni-directional process for simplicity, it is expected that actual research and development practices will involve iteration between stages as well as multiple and simultaneous streams of work (e.g. Agile projects).</li> <li>The model is circular to acknowledge that the deprovisioning of a system may require a new project to commence (e.g. to meet ongoing business needs, or to prevent lock-in due to technical debt).  This is an important part of responsible research and innovation\u2014taking responsibility for future outcomes, and not just \"washing your hands\" when a model or system is deployed.</li> </ul> <p></p>"},{"location":"skills-tracks/rri/rri-101-1/#who-is-the-model-for","title":"Who is the model for?","text":"<p>The model is primarily for anyone who is directly involved with the project in question, and who has some role within or responsibility over one or more of the stages. For example, the model is as much for the data analyst or systems engineer, as it is for the project manager who has oversight over the project itself.</p> <p>The reason for this is two-fold:</p> <ol> <li> <p>The model is intended to create a common conceptual vocabulary when communicating with those who are indirectly involved in the project (e.g. organisations engaged for procurement of data or services, wider stakeholders and affected users with whom the project team need to communicate).  Recall the principle of 'engage' from the AREA framework in the previous module\u2014having a shared or common vocabulary is important in engagement and communication.</p> </li> <li> <p>Although some team members may have oversight over the project as a whole, this often establishes a more narrow form of accountability or liability, not responsibility.  Having a model that can also show how roles and responsibilities become entangled in attempts to remain morally responsible is, therefore, also necessary.</p> </li> </ol> Illustrative roles <p>Some illustrative roles include the following:</p> <ul> <li>Researchers (e.g. user researchers, data scientists, social data scientists)</li> <li>Developers (e.g. ML architect, software and systems engineers)</li> <li>Project governance (e.g. quality assurance, system auditing, data custodian, data protection impact officer, project manager, product owner)</li> </ul> Entangled Ethical Decisions <p>We will explore concrete examples of how ethical decision-making is entangled (or embedded) in the various roles and responsibilities of a project team over the course of this module. For now, if this second reason is unclear, try to recall the discussion in the first module about collective responsibility.</p>"},{"location":"skills-tracks/rri/rri-101-1/#why-and-how-should-you-use-the-model","title":"Why and how should you use the model?","text":"<p>There are many ways in which this model can be used and we will explore some uses in detail over the course of this module.</p> <p>Pragmatic uses of the lifecycle model</p> <p>For now, the following illustrative examples may help understand the pragmatic aspects of the model:</p> <ul> <li>Prior to the start of a project, the model can be used as a checklist reflect-list of tasks and activities that are likely to have ethical significance (e.g. bias mitigation assessment during project planning).</li> <li>The scaffolding (or structure) provided by the model can support the continuous deliberation and documentation of actions and decisions taken as a project evolves (i.e. development of a living document). For example, as tasks are handed over to others, documentation about previous and upcoming decisions or actions can be built upon.<sup>1</sup></li> <li>Towards and at the end of a project, the repository of documented evidence can then serve as an accessible and transparent record of the project's activities and governance.</li> </ul> <ol> <li> <p>Here, additional use of version control technologies (e.g. Git and GitHub) could also enable an open and transparent form of project governance and documentation, when stored alongside data or code in a public (or shared) repository. For instance, keeping track of ethical decision-making recorded against the activities of the project as it unfolds.\u00a0\u21a9</p> </li> </ol>"},{"location":"skills-tracks/rri/rri-101-2/","title":"Stage 1: Project Design","text":"<p>In this section we will begin by looking at the first four tasks of the project lifecycle model, which are concerned with the project design stage. For each task, a description is given as well as information about the importance of the typical activities associated with the task, including issues that have an ethical significance.</p> <p></p>"},{"location":"skills-tracks/rri/rri-101-2/#project-planning","title":"Project Planning","text":""},{"location":"skills-tracks/rri/rri-101-2/#task-description","title":"Task Description","text":"<p>The <code>project planning</code> task encompasses the preliminary activities that are intended to help determine the aims, objectives, scope, and processes associated with the project, including an assessment of the potential risks and benefits.</p> <p>The following activities are illustrative but not exhaustive:</p> <ul> <li>Resource and responsibilities evaluation and allocation: this can help a project team identify gaps in the team's skills or organisation's resources, which may require support from an external partner (e.g. procurement of services)</li> <li>Stakeholder engagement planning: identification and analysis of relevant stakeholders and affected users, with an emphasis on the inclusion of diverse or otherwise marginalised voices</li> <li>Risk and impact assessments: there are many forms of risk and impact assessment that may need to be carried out, including data protection, safety, and equality impact assessments.</li> </ul> <p>These initial assessments can help determine the proportional level of any risk or impact mitigation activities.</p> <ul> <li>Project plan documentation: the development and reporting of an actual project plan, which can be used to track progress and identify any issues that may arise. This can be a formal document or a more informal plan that is used to guide the project team's activities.</li> </ul>"},{"location":"skills-tracks/rri/rri-101-2/#importance-of-task","title":"Importance of task","text":"<ul> <li>Creates a space for anticipatory and reflective activities (see AREA framework) which are necessary for a stable foundation for the project.</li> <li>Offers an opportunity for the team to agree on any \"red lines\" (e.g. contexts or domains in which a system should not be used, data types that are not permissible to collect or use).</li> <li>Allows project team to set milestones and objectives that can be used throughout the project to determine if their original goals have been achieved.</li> </ul>"},{"location":"skills-tracks/rri/rri-101-2/#illustrative-examples","title":"Illustrative Examples","text":"Health careEnvironmental Sciences <p>Illustrative examples coming soon!</p> <p>Illustrative examples coming soon!</p>"},{"location":"skills-tracks/rri/rri-101-2/#problem-formulation","title":"Problem Formulation","text":""},{"location":"skills-tracks/rri/rri-101-2/#task-description_1","title":"Task Description","text":"<p>This task involves the formulation of a clear statement about the overarching problem the target system or project seeks to address (e.g. a research statement or system specification) and a lower level description of the computational procedure that instantiates it (e.g. a functional mapping from input to output variables and explanation about why it is appropriate).</p>"},{"location":"skills-tracks/rri/rri-101-2/#importance-of-task_1","title":"Importance of Task","text":"<p>The importance of this stage is split across the two interlocking understandings of the term \"problem\":</p> <p>1) As a statement about a well-defined computational process (or a higher-level abstraction of the process), this task helps identify the validity and legitimacy of the project.  For example, an algorithmic system that attempts to predict a candidate's 'employability' (the target variable) on the basis of a model trained on biased data from historical hiring practices will be perceived as unjust.</p> <p>2) As a statement about how the system attempts to address a wider practical, social, or policy issue, this task helps the project team determine if their goal is valid and if the target system is sufficient to achieve their goal.  It can also support stakeholder engagement and project communication activities.</p>"},{"location":"skills-tracks/rri/rri-101-2/#illustrative-example","title":"Illustrative Example","text":"Health careEnvironmental Sciences <p>Illustrative examples coming soon!</p> <p>Illustrative examples coming soon!</p>"},{"location":"skills-tracks/rri/rri-101-2/#data-extraction-or-procurement","title":"Data Extraction (or Procurement)","text":""},{"location":"skills-tracks/rri/rri-101-2/#task-description_2","title":"Task Description","text":"<p>By <code>data extraction</code> we refer to a) the design of an experimental method or decisions about data gathering and collection, based on the planning and problem formulation from the previous steps, and/or b) the actual extraction and storage of novel data or the procurement of existing data.</p> <p>Research Data Management</p> <p>This description is situated at a high level of abstraction.  As such, it encompasses many more fine-grained activities that are typically associated with the development and governance of a data pipeline (e.g the Turing Way's guide on research data management).</p>"},{"location":"skills-tracks/rri/rri-101-2/#importance-of-task_2","title":"Importance of Task","text":"<p>The well-known principle of 'garbage-in, garbage-out' summarises the importance of this task nicely.</p> <p>As data-driven technologies, ML algorithms or AI systems depend on the data fed into them.  However, due diligence at this stage is important for reasons other than statistical validity.  Responsible data extraction is, among other reasons, vital for the design of accountable and trustworthy services, the development of safe, fair, and explainable algorithms, and the deployment of sustainable and privacy-preserving systems.</p>"},{"location":"skills-tracks/rri/rri-101-2/#illustrative-example_1","title":"Illustrative Example","text":"Health careEnvironmental Sciences <p>Illustrative examples coming soon!</p> <p>Illustrative examples coming soon!</p>"},{"location":"skills-tracks/rri/rri-101-2/#data-analysis","title":"Data Analysis","text":""},{"location":"skills-tracks/rri/rri-101-2/#task-description_3","title":"Task Description","text":"<p>Data analysis is typically split into two types: <code>exploratory</code> and <code>confirmatory</code> analysis:</p> <ul> <li>Exploratory data analysis allows analysts to better understand the structure and content of the dataset, and identify possible associations between data types and variables.</li> <li>Confirmatory data analysis is where initial hypotheses that are developed in the previous stage are evaluated using a variety of statistical methods (e.g. significance testing).</li> </ul>"},{"location":"skills-tracks/rri/rri-101-2/#importance-of-task_3","title":"Importance of Task","text":"<p>In the context of responsible research and innovation, data analysis is vital to the assessment of myriad biases that can negatively impact a project, many of which are most obvious at this stage.</p> <p>Identifying and dealing with missing data is particularly important during this task.  Although upstream stakeholder engagement activities can help mitigate the impact of this bias, identifying the scope of its impact and determining how effectively it can be addressed (e.g. using various methods imputation, collecting additional data), will largely depend on the quality of the data analysis task.</p>"},{"location":"skills-tracks/rri/rri-101-2/#illustrative-example_2","title":"Illustrative Example","text":"Health careEnvironmental Sciences <p>Illustrative examples coming soon!</p> <p>Illustrative examples coming soon! </p>"},{"location":"skills-tracks/rri/rri-101-3/","title":"Stage 2 : Model Development","text":"<p>In this section we will begin by looking at the second set of tasks of the project lifecycle model, which are concerned with the model development stage. For each task, a description is given as well as information about the importance of the typical activities associated with the task, including issues that have an ethical significance.</p> <p></p>"},{"location":"skills-tracks/rri/rri-101-3/#preprocessing-and-feature-engineering","title":"Preprocessing and Feature Engineering","text":""},{"location":"skills-tracks/rri/rri-101-3/#task-description","title":"Task Description","text":"<p>Data analysis can give rise to valuable insights (e.g. business intelligence), but not all the data types that have been collected will be appropriate to train ML algorithms. Therefore, <code>preprocessing and feature engineering</code> involves transforming the data into a form that is suitable for the next stage of the project lifecycle. This typically involves the cleaning, normalising, or otherwise refactoring of data into the features that will be used in model training and testing, as well as the features that may be used in the final system.</p> <p>Features, therefore, may not be the same as the raw data that are collected in the prior stages.  Rather, they may represent a combination of multiple data types, and as such may not always be interpretable to the end user.</p>"},{"location":"skills-tracks/rri/rri-101-3/#importance-of-task","title":"Importance of Task","text":"<p>Features are dependant upon, but separate from, the raw data that are collected in the prior stages.  They can be engineered by hand or through the use of algorithmic techniques to improve the performance of subsequent ML processes.</p> <p>However, the features that are used in the process of <code>model training</code>, for instance, do not only affect the model's accuracy or predictive power, they also impact the ethical consequences of the project (e.g. reducing the explanatory potential of system, creating discriminatory outcomes).  Therefore, selecting the best features is a vital, albeit often time-consuming and complicated task that can involve trade-offs about which parameter to optimise for (e.g. predictive power versus interpretability).</p>"},{"location":"skills-tracks/rri/rri-101-3/#illustrative-example","title":"Illustrative Example","text":"Health careEnvironmental Sciences <p>Illustrative examples coming soon!</p> <p>Illustrative examples coming soon!</p>"},{"location":"skills-tracks/rri/rri-101-3/#model-selection-and-training","title":"Model Selection and Training","text":""},{"location":"skills-tracks/rri/rri-101-3/#task-description_1","title":"Task Description","text":"<p>This task involves the selection of a particular process (or algorithm) for training a model, and the training of the model itself.</p> <p>There are many factors that feed into the decision of which model to select, including (but not limited to):</p> <ul> <li>Access to computational resources (some learning algorithms require vast levels of computational power)</li> <li>Predictive performance of model (as compared to other models)</li> <li>Properties of underlying data (e.g. is the size of the dataset sufficient)</li> </ul> <p><code>Model training</code> is the process of fitting a statistical model to some training data. The process of training is typically iterative and proceeds by optimising the model's parameters (e.g. weights) to increasingly minimise the error between the model's predictions and the true values of the training data. The <code>problem formulation</code> task is important here because the target variable that was previously determined will guide the choice of model and the training process.</p> <p>In addition, this task requires the project team to split their data into a training and testing set to prepare for the next task.</p>"},{"location":"skills-tracks/rri/rri-101-3/#importance-of-task_1","title":"Importance of Task","text":"<p>There are, of course, many technical and logistical reasons for the responsible selecting and training of a model (e.g. ensuring parsimony, optimising performance).</p> <p>However, a key concept in the responsible development of a model is the inherent interpretability and post hoc explainability of the model and the behaviour of the system into which it is implemented. Although there are nuances and exceptions, it is generally the case that more complex models are harder to interpret and explain (e.g. linear regression versus convolutional neural networks).  Selecting the right technique, therefore, depends on the ultimate use case of the model and system.</p> <p>In addition, the training process can be computationally intensive and may require the use of specialised hardware (e.g. GPUs). This gives rise to issues of sustainability and fairness, as the training process may be inaccessible to some individuals or groups.</p>"},{"location":"skills-tracks/rri/rri-101-3/#illustrative-example_1","title":"Illustrative Example","text":"Health careEnvironmental Sciences <p>Illustrative examples coming soon!</p> <p>Illustrative examples coming soon!</p>"},{"location":"skills-tracks/rri/rri-101-3/#model-testing-and-validation","title":"Model Testing and Validation","text":""},{"location":"skills-tracks/rri/rri-101-3/#task-description_2","title":"Task Description","text":"<p><code>Model testing and validation</code> involves using the testing set from the previous task to evaluate the performance of the trained model. The evaluation of the model can be carried out against a variety of metrics, but typically includes the evaluation of the model's accuracy as applied to novel data (held out from the original training data).</p> <p>This form of testing is sometimes known as internal validation, as it is carried out using a subset of the dataset that was used to train the model. In addition, the project team may also wish to evaluate the model's performance against entirely new data (external validation), which may be collected from a separate trial or even carried out by a separate team.</p>"},{"location":"skills-tracks/rri/rri-101-3/#importance-of-task_2","title":"Importance of Task","text":"<p>Where a dataset is split into testing and training data, or where a model's performance is evaluated against wholly new data (e.g. external validation from a separate trial or project team), there are options to assess more than just the model's performance.</p> <p>For instance, testing the generalisability of a model to a new domain or context can also help ensure the model is both sustainable and fair (e.g. has similar levels of accuracy or performance when validated externally). In addition, the model can be evaluated for its interpretability and explainability (e.g. how well the model's predictions can be explained by the features that were used to train it). If the model has low interpretability or explainability, then the project team may wish to consider retraining the model with a different set of features or employ some post hoc explanation techniques (e.g. LIME or SHAP).</p>"},{"location":"skills-tracks/rri/rri-101-3/#illustrative-example_2","title":"Illustrative Example","text":"Health careEnvironmental Sciences <p>Illustrative examples coming soon!</p> <p>Illustrative examples coming soon!</p>"},{"location":"skills-tracks/rri/rri-101-3/#model-documentation","title":"Model Documentation","text":""},{"location":"skills-tracks/rri/rri-101-3/#task-description_3","title":"Task Description","text":"<p>This task involves the documentation of both formal and non-formal properties of the model and the processes by which it was developed.  This includes (but is not limited to):</p> <ul> <li>Data sources and summary statistics</li> <li>Model used (e.g. proprietary model purchased from vendor)</li> <li>Model parameters (e.g. weights)</li> <li>Evaluation metrics (e.g. model performance)</li> <li>Model performance (e.g. accuracy)</li> <li>Model limitations (e.g. bias)</li> <li>Model assumptions (e.g. normality of data)</li> </ul> <p>The categories of information that are documented will depend on the project's requirements. For example, if the project is part of an academic research project, then the team may wish to follow the standard format of a scientific paper or a pre-specified protocol.  In contrast, if the project is part of a commercial development project, then the team may have additional requirements based on a procurement process or other contractual obligations.</p> <p>Model Cards for Model Reporting</p> <p>See Mitchell et al. (2018) Model Cards for Model Reporting for one proposed template that project teams can use as a starting point for model documentation. Teams may wish to adapt this template to suit their own needs.</p>"},{"location":"skills-tracks/rri/rri-101-3/#importance-of-task_3","title":"Importance of Task","text":"<p>Clear and accessible documentation is an important form of responsible project governance for the following reasons:</p> <ul> <li>In research projects it ensures reproducibility and replicability of results, as well as other values associated with open research, such as public accessibility.</li> <li>In commercial projects it ensures accountability and transparency of decision-making processes, which may be required by law or by contractual obligations.</li> <li>In can help affected individuals seek redress for any harms that may arise from the design, development, or deployment of data-driven technologies.</li> <li>It allows project team members, who may be responsible for downstream tasks, to review and reflect on the project's progress and outcomes so far.</li> </ul> <p>Thinking further</p> <pre><code>What other reasons are there for clear and accessible documentation?\n</code></pre>"},{"location":"skills-tracks/rri/rri-101-3/#illustrative-example_3","title":"Illustrative Example","text":"Health careEnvironmental Sciences <p>Illustrative examples coming soon!</p> <p>Illustrative examples coming soon!</p>"},{"location":"skills-tracks/rri/rri-101-4/","title":"Stage 3: System Deployment","text":"<p>In this section we will begin by looking at the final four tasks of the project lifecycle model, which are concerned with the deployment of the system. For each task, a description is given as well as information about the importance of the typical activities associated with the task, including issues that have an ethical significance.</p> <p></p> <p>Important</p> <p>Some projects, most notably those concerned primarily with research, may not reach the system deployment stage. However, for those that do it is important to emphasise that the system deployment stage marks the beginning of a new phase of the project lifecycle, which is concerned with the ongoing maintenance and monitoring of the system.</p>"},{"location":"skills-tracks/rri/rri-101-4/#system-implementation","title":"System Implementation","text":""},{"location":"skills-tracks/rri/rri-101-4/#task-description","title":"Task Description","text":"<p>System implementation is the process of putting a model into production, and implementing the resulting system into an operational environment. The system enables and structures human interaction with the model, within the respective environment (e.g. a recommender system that converts a user\u2019s existing movie ratings into recommendations for future watches). As such, this task often requires a significant level of involvement from systems and software engineers and designers.</p>"},{"location":"skills-tracks/rri/rri-101-4/#importance-of-task","title":"Importance of Task","text":"<p>Regardless of how well the preceding stages have gone, unless the encompassing system is implemented effectively, the model's performance will be impacted.  Here, we can note the importance of two forms of implementation:</p> <ul> <li>Technical implementation: designing and building the hardware and software infrastructure (e.g. server, interfaces) that will host the model.  Among other things, it is important to ensure the technical system is secure, performant, and accessible.</li> <li>Social or organisational implementation: how the technical system is situated within broader social and organisational practices is also important when considering the project's goals and objectives (e.g. appropriately informed users, complementarity with organisational practices).</li> </ul> <p>Both types give rise to a large number of considerations, such as the need to ensure the system is certified for sale (e.g. according to necessary safety legislation); that it complies with relevant regulatory requirements (e.g. processing of personal data in accordance with the General Data Protection Regulation); that it is performant and secure (e.g. against cyberattacks), and that it is accessible (e.g. for users with disabilities).</p>"},{"location":"skills-tracks/rri/rri-101-4/#illustrative-example","title":"Illustrative Example","text":"Health careEnvironmental Sciences <p>Illustrative examples coming soon!</p> <p>Illustrative examples coming soon!</p>"},{"location":"skills-tracks/rri/rri-101-4/#user-training","title":"User Training","text":""},{"location":"skills-tracks/rri/rri-101-4/#task-description_1","title":"Task Description","text":"<p>'User training' includes any form of support, upskilling, or capacity building that is offered to or carried out with the individuals or groups who are required to operate the system in question (e.g. mandatory training in a safety\u2013critical context), or who are likely to use the system (e.g. consumers).</p>"},{"location":"skills-tracks/rri/rri-101-4/#importance-of-task_1","title":"Importance of Task","text":"<p>User training is rarely carried out by the same team members who designed and developed the system. While developers may produce documentation for the model (see above), this is often insufficient as a form of user training\u2014additional forms of formal training workshops or courses may be required depending on the complexity of the system.</p> <p>Insufficient or inadequate training can create conditions in which cognitive biases such as algorithmic aversion thrive (e.g. users do not trust the performance or behaviours of a trustworthy algorithmic system), or conversely, users trust the outputs of an untrustworthy system).</p> <p>Human Factors</p> <p>The field known as 'human factors' is concerned with the study of human\u2013machine interaction, and is a useful resource for understanding the importance of user training. See Durso (2014)<sup>1</sup> for an overview of the field of human factors and see Burton (2019)<sup>2</sup> for a review and discussion of six forms of algorithmic aversion and proposed solutions to them, including the importance of user training.</p>"},{"location":"skills-tracks/rri/rri-101-4/#illustrative-example_1","title":"Illustrative Example","text":"Health careEnvironmental Sciences <p>Illustrative examples coming soon!</p> <p>Illustrative examples coming soon!</p>"},{"location":"skills-tracks/rri/rri-101-4/#system-use-and-monitoring","title":"System Use and Monitoring","text":""},{"location":"skills-tracks/rri/rri-101-4/#task-description_2","title":"Task Description","text":"<p>Depending on how a system has been designed, its deployment and use in an environment (physical or virtual) can create conditions for ongoing feedback and learning (e.g. robotic systems that employ reinforcement learning, digital twins linked to a monitored counterpart). Regardless, the use of metrics and evaluation methods are commonly used to monitor the performance of a system and ensure that it retains (or ideally improves on) the same level of performance that it had when first validated.</p>"},{"location":"skills-tracks/rri/rri-101-4/#importance-of-task_2","title":"Importance of Task","text":"<p>The potentially dynamic (and sometimes unpredictable) behaviour of machine learning models and AI systems means that ongoing monitoring and feedback of the system, either automated or probed, is important to ensure that issues such as model drift have not affected performance or resulted in harms to individuals or groups.</p> What is model drift? <p>Model drift is a phenomenon that occurs when the underlying data distribution used to train a model changes over time, resulting in a change in the model's performance. This can happen in one of two ways.</p> <p>On the one hand, drift can occur when the statistical properties of an input variable change (i.e. there is a shift in the underlying data distribution). For example, perhaps house prices start increasing and a model becomes more and more inaccurate at predicting them.</p> <p>On the other hand, there could be a more nuanced reason related to the conceptual or social meaning of the input variables. An example of this could be a machine learning algorithm used in finance that aims to predict whether someone is likely to default on a loan using variables with social meaning, such as occupation. If the model detects a relationship between specific values for certain occupations and the employee's ability to pay back a loan in a timely manner, the system may recommend more loans to people in this occupation. However, if something happens that has a global impact on these jobs (e.g. increased investment in the sector creating a rise in average wages), this association will change. The result is that people who could otherwise afford a loan may still be denied one due to inaccurate models.</p>"},{"location":"skills-tracks/rri/rri-101-4/#illustrative-example_2","title":"Illustrative Example","text":"Health careEnvironmental Sciences <p>Illustrative examples coming soon!</p> <p>Illustrative examples coming soon!</p>"},{"location":"skills-tracks/rri/rri-101-4/#model-updating-or-deprovisioning","title":"Model Updating or Deprovisioning","text":""},{"location":"skills-tracks/rri/rri-101-4/#task-description_3","title":"Task Description","text":"<p>If the use and monitoring of a model or system identifies vulnerabilities or inadequate levels of performance, it may be necessary to either update the model through retraining (i.e. looping back through some of the <code>model development</code> tasks) or deprovision the system if it is no longer fit for purpose.</p> <p>Where the latter option occurs, the project team or organisation may need to start a new project to address any gaps in their business or organisation that arise because of the deprovisioning of the present system. This would then restart the project lifecycle from the beginning, bringing us full circle.</p>"},{"location":"skills-tracks/rri/rri-101-4/#importance-of-task_3","title":"Importance of Task","text":"<p>An algorithmic model that adapts its behaviour over time or context may require updating or deprovisioning (i.e. removing from the production environment). While this can include elements such as improvements to the system's architecture (e.g. for speed or security), the more important component here is the model itself (e.g. the model parameters, the features used).</p> <p>The need to update or deprovision a model or system can arise for a number of reasons, including:</p> <ul> <li>The model is no longer fit for purpose (e.g. it is no longer accurate or reliable).</li> <li>The system contains too many vulnerabilities as it is based on an outdated architecture.</li> <li>The purpose of the system has changed, and it is no longer commercially viable or scientifically useful.</li> <li>The system is no longer compliant with relevant legislation, regulation, or with the organisation's policies and procedures.</li> </ul>"},{"location":"skills-tracks/rri/rri-101-4/#illustrative-example_3","title":"Illustrative Example","text":"Health careEnvironmental Sciences <p>Illustrative examples coming soon!</p> <p>Illustrative examples coming soon!</p>"},{"location":"skills-tracks/rri/rri-101-4/#next-steps","title":"Next Steps","text":"<p>At this point in the skills track you can choose different routes through the remaining modules. First, you will have a choice between five ethical principles, known as the SAFE-D principles, which help guide responsible design, development, and deployment of data-driven technologies:</p> <ul> <li>Sustainability Coming soon.</li> <li>Accountability Coming soon.</li> <li>Fairness</li> <li>Explainability</li> <li>Data Quality, Integrity, Privacy and Protection Coming soon.</li> </ul> <p>The modules associated with these principle can be carried out in any order, and you are not required to complete all of them. Rather, you can choose to focus on the ones that are most relevant to your project or area of interest. However, at least one of these modules will need to be completed before proceeding to the final module of the skills track.</p>"},{"location":"skills-tracks/rri/rri-101-4/#further-resources","title":"Further Resources","text":"<p>The following resources provide additional and relevant information on the project lifecycle model:</p> <ul> <li>Burr, C., &amp; Leslie, D. (2022). Ethical assurance: A practical approach to the responsible design, development, and deployment of data-driven technologies. AI and Ethics. https://doi.org/10.1007/s43681-022-00178-0</li> </ul> <ol> <li> <p>Durso, Frank T., Margulieux, Lauren E., Blickensderfer, Elizabeth L. (2014). Human factors. Oxford University Press. DOI: 10.1093/obo/9780199828340-0159\u00a0\u21a9</p> </li> <li> <p>Burton, .. (2019).\u00a0\u21a9</p> </li> </ol>"},{"location":"skills-tracks/rri/rri-101-index/","title":"About this Module - The Project Lifecycle","text":"<p>This module introduces the Project Lifecycle model which depicts the process of designing, developing, and deploying an AI system. </p> <p>It unpacks the usefulness of the model for several processes or outcomes, and then presents each of the twelve stages of the model in detail, accompanied by illustrative examples.</p>"},{"location":"skills-tracks/rri/rri-101-index/#learning-objectives","title":"Learning Objectives","text":"<p>This module has the following learning objectives:</p> <ul> <li>Understand why the project lifecycle model is called a heuristic model, and why models can only ever be heuristic.</li> <li>Identify the main properties of the model and who it is for. </li> <li>Explore the pragmatic uses of the model.</li> <li>Identify the twelve stages of the project lifecycle and why they are relevant to the different ethical, legal, and social issues that may arise throughout the design, development, and deployment of a project.</li> </ul>"},{"location":"skills-tracks/rri/rri-101-index/#table-of-contents","title":"Table of Contents","text":"<ul> <li> <p> What is the Project Lifecycle?</p> <p>This section introduces the project lifecycle, its properties, and who is likely to be able to make use of it. It also explores the model's overarching stages and some illustrative cases for its use.</p> <p> Go to module</p> </li> <li> <p> Project Design</p> <p>This section focuses on the first overarching stage of the project lifecycle model: project design. It then goes through the four sub-stages in detail, explaining its relevance and giving some illustrative examples.</p> <p> Go to module</p> </li> <li> <p> Model Development</p> <p>This section focuses on the second overarching stage of the project lifecycle model: model development. It then goes through the four sub-stages in detail, explaining its relevance and giving some illustrative examples.</p> <p> Go to module</p> </li> <li> <p> System Deployment</p> <p>This section focuses on the third overarching stage of the project lifecycle model: system deployment. It then goes through the four sub-stages in detail, explaining its relevance and giving some illustrative examples.</p> <p> Go to module</p> </li> </ul>"},{"location":"skills-tracks/rri/rri-200-index/","title":"About these Modules - The SAFE-D Principles","text":"<p>The following are the five optional modules for the RRI skills track.  Each module focuses on one of the SAFE-D principles. These are overarching ethical principles which highlight crucial dimensions relevant when thinking through the practical applications of responsible research and innovation.</p> <p>The principles are: sustainability, accountability, fairness, explainability, and data stewardship.  To complete the RRI skills track, at least one of the five modules must be completed.  However, there is no limit as to how many modules may be completed before moving into the final core module: responsible communication and open science.</p>"},{"location":"skills-tracks/rri/rri-200-index/#table-of-contents","title":"Table of Contents","text":"<ul> <li> <p> Sustainability</p> <p>This module explores the principle of sustainability. Module coming soon.</p> </li> <li> <p> Accountability</p> <p>This module focuses on accountability, and what the concept means practically in the context of data-driven systems. Module coming soon.</p> </li> <li> <p> Fairness</p> <p>This module looks at the concept of fairness within AI and data science projects. In particular, it focuses on three aspects of fairness: sociocultural, statistical, and practical fairness. </p> <p> Go to module</p> </li> <li> <p> Explainability</p> <p>This module focuses on the importance of AI systems being explainable. It looks at what is meant by explainability in the context of AI and data-driven systems, as well different relevant aspects of explainability.</p> <p> Go to module</p> </li> <li> <p> Data Stewardship</p> <p>The final optional module, explores different aspects of responsible data stewardship, including issues around data governance, quality, privacy, and security. Module coming soon.</p> </li> </ul>"},{"location":"skills-tracks/rri/rri-203-1/","title":"What is Fairness?","text":""},{"location":"skills-tracks/rri/rri-203-1/#the-children-and-the-flute-an-example-of-value-pluralism","title":"The Children and the Flute: An Example of Value Pluralism","text":"<p>Three children are arguing about which of them should have a flute.</p> <ul> <li>Anne claims the flute should belong to her on the grounds that she is the only one who can play the flute.</li> <li>Bob claims the flute should belong to him on the grounds that he is the only one among the three who is so poor that he has no toys.</li> <li>Carla claims the flute should belong to her on the grounds that she was the one who made the flute.</li> </ul> <p>None of the children deny the claims of the others.</p> <p>Question</p> <pre><code>Is it fair to give the flute to Anne, Bob, or Carla?\n</code></pre> <p>The story of the children and the flute is proposed by Amartya Sen, in his seminal work The Idea of Justice.<sup>1</sup> It is designed as a way to test our intuitions about our ability to reconcile incommensurable values (i.e. values that cannot be judged by the same standards), when evaluating claims regarding fair treatment or outcomes. In the story, the three children represent competing theories of fairness that emphasise different values.</p> <ul> <li>Anne represents utlitarian theories of fairness (e.g. fairness involves maximising the overall welfare of the group)</li> <li>Bob represents economic egalitarian theories of fairness (e.g. fairness involves ensuring equal distribution of goods)</li> <li>Carla represents libertarian theories of fairness (e.g. fairness involves freedom to pursue one's own interests and benefit from one's own efforts)</li> </ul> <p>While the values expressed by these theories do not always have to come into conflict, the purpose of Sen's example is to present a toy example of how and where they can.</p> <p>This module is not about incommensurable values. However, it is about how to manage and address (sometimes) competing claims of fairness in the context of data-driven technologies. As such, Sen's example provides a useful bookend and a reminder that fairness is not a single, objective concept, but rather a process of evaluating various claims and reasons on the basis of their respective merits, and in a way that strives to ensure all voices (and values) are heard and included.</p> <p>What would you save?</p> <pre><code>Your house is on fire and you can take only three things with you before the entire structure becomes engulfed in flames. \nWhat would you save?[^taboo]\n</code></pre> <p>Taboo trade-offs</p> <pre><code>People answer this question in very different ways.\nWhat can be invaluable to one person might seem like a trinket to another or vice versa.\n\nWe seem to have a lot of flexibility in the way we ascribe value to our possessions, in a way that sometimes gives them the quality of the _sacred_.\nThis quality can be extended to values as well, and we usually hold on extra tightly to those we (consciously or not) consider sacred.\n\nResearch suggests that once something has become sacred or holds sacred value, people are (perhaps unsurprisingly) much more reluctant to compromise over them.\nWhen they are asked to, they often express moral outrage, anger, and even disgust, a phenomenon that has been referred to as the _taboo trade-off_.[^tetlock]\nThis means that they become insensitive to cost-benefit analysis, and they are _less_ likely to accept an exchange if they are offered money to relinquish whatever they hold as of sacred value (think of how you would react if someone offered you money to compromise on your principles).\n\nThis is related as it is another example of a kind of _value pluralism_. Individuals, groups, and even cultures can value certain objects, ideas, and practices in widely different ways.\n</code></pre>"},{"location":"skills-tracks/rri/rri-203-1/#defining-fairness","title":"Defining Fairness","text":"<p>Given the previous example, you will find it unsurprising that this module will not attempt to define fairness in the traditional way. Rather, it will explore different facets of the concept, how they relate to each other, and most importantly how they can be practically applied to the design, development, and deployment of data-driven technologies.</p> <p>In doing so, we will appeal to a variety of interrelated theories and concepts, including:</p> <ul> <li>Legal concepts of non-discrimination and equality</li> <li>Moral concepts of social justice and distributive justice</li> <li>Practical forms of bias mitigation and statistical fairness</li> <li>Social concepts of diversity, inclusion, and representation</li> </ul> <p>We will strive to identify common (or, underlying) themes that can be used to operationalise and proceduralise fairness in the context of data-driven technologies.</p> <p>Clarification</p> <pre><code>It is important to note that this module is not a summary of or introduction to any of these areas.\nTherefore, while we will strive to make explicit where specific concepts are drawn from (e.g. legal versus moral literature), it is important to note that many of the concepts are inherently multi-disciplinary and interrelated.\n</code></pre> <p>Let's start by looking at some questions that span the different perspective outlined above.</p>"},{"location":"skills-tracks/rri/rri-203-1/#questions-of-fairness-1-procedures","title":"Questions of Fairness (1): Procedures","text":"<p>Here are three questions that all relate to fair procedures:</p> <ol> <li>Who should get what?</li> <li>How should we decide?</li> <li>Who decides?</li> </ol> <p>These three questions are related. The first question relates to the challenges of deciding how to allocate or distribute goods. For instance, who should be granted a mortgage? And, which factors are relevant to the way we carve up different groups (e.g. those eligible and those ineligible)?</p> <p>The second question relates to the decision procedures (or rules) we use to actually allocate or distribute goods. For instance, will different groups be offered different mortgage rates? If so, what will influence this decision (e.g. will weights be applied to the previous set of factors)?</p> <p>And the third question relates to the authority of the decision procedure. That is, who is designing and implementing the decision procedure, and what provides them with the authority (or, legitimacy) to do so?</p> <p>To give one example, perhaps senior decision-makers for a national department of health and social care are trying to decide how to allocate limited resources for accessing counselling services. They start by deciding that the services should be allocated to those who are most in need. However, they soon realise that they do not have a clear definition of what \"most in need\" means. As such, they reach out to a group of stakeholders (e.g. patients, clinicians, and researchers) to help them define the criteria for eligibility.</p> <p>Here, the procedures for answering each of the above questions is inclusive and aims to uphold values such as democratic forms of deliberation. But whether the procedures will a) maximise the overall welfare of the group, b) ensure equal distribution of goods, or c) allow people to pursue their own interests and benefit from their own efforts, is not directly addressed.</p> <p>So, we can now see that the underlying values that characterise fairness can be separated from the procedures used to achieve them.</p> <p>What is a good?</p> <pre><code>As we use the term in this module, a good is anything that is valued by a person or a group of people.\nThis can be some resource (e.g. money, food, water), a service (e.g. healthcare, education, housing), an outcome (e.g. improved mobility, winning the lottery), or a status (e.g. power, prestige, respect).\nIt is important to note that a good can be both tangible and intangible, and, therefore, is intentionally an ambiguous term.\n</code></pre>"},{"location":"skills-tracks/rri/rri-203-1/#questions-of-fairness-2-outcomes-comparisons-and-trade-offs","title":"Questions of Fairness (2): Outcomes, Comparisons, and Trade-Offs","text":"<p>In addition to the previous questions, there are also questions that help us identify which elements are responsible for any comparisons and trade-offs between outcomes. For example:</p> <ol> <li>Is a system that gives everyone an equal opportunity fairer than one that ensures everyone has an equal outcome?</li> <li>Is an algorithm that follows consistent and impartial logic fairer than a human that shows empathy and compassion?</li> <li>Is it fairer to use a more accurate model, which is complex and opaque in its workings, than a less accurate but interpretable model?</li> </ol> <p>Answering questions such as these may involve both the identification and evaluation of competing values that pertain to specific outcomes, as well as the different decision procedures employed.</p> <p>When we ask questions such as these, or those from the previous set, it is important to be clear on our target. Is our question focused on a comparison of different values, as is the case with the choice between which of the children should get the flute? Is our question seeking to clarify whether a decision procedure is legitimate, or is it trying to determine if it is effective in realising some pre-determined outcome? Or, is our question seeking to clarify the reasons that are relevant to resolving some trade-off?</p> <p>Until we can clearly specify the question we are asking, it is unlikely we will be able to form consensus on an answer or even be sure that we are disagreeing over the same thing. Over the course of this module we will slowly build up our conceptual vocabulary to develop both a shared understanding of the different questions that can be asked, as well as the different answers that can be given.</p> <p>We have already started this process, but let's now turn to the first set of conceptual distinctions that directly address data-driven technologies.</p> <p></p>"},{"location":"skills-tracks/rri/rri-203-1/#conceptual-matters-1-two-perspectives-on-fairness","title":"Conceptual Matters (1): Two Perspectives on Fairness","text":"<p>There are two perspectives on fairness that we will use throughout this module to help us emphasise different features of data-driven technologies that relate to matters of fairness:</p> <ul> <li>Sociocultural Fairness</li> <li>Statistical Fairness</li> </ul> <p>What do we mean by 'perspective'?</p> <pre><code>Our use of the term 'perspective' is intended to draw attention to the fact that each perspective is focused on the same questions, but approaches the questions from a different stance or starting point.\nNo single perspective will be able to capture everything that matters about fairness.\nRather, an inclusive and diverse set of backgrounds and people will be required to ensure that the different perspectives are well represented and that all considerations have been taken into account.\n\nThere are many ways that we could have defined or labelled these perspectives.\nHowever, the perspectives we have chosen were selected to achieve a balance between a) maximising applicability across different domains while b) minimising the number of categories.\n</code></pre> <p>Let's look at each of these perspectives in turn.</p>"},{"location":"skills-tracks/rri/rri-203-1/#sociocultural-fairness","title":"Sociocultural Fairness","text":"<p>Sociocultural fairness emphasises the importance of the social and cultural context in which a data-driven technology is developed, deployed, and used. Adopting this perspective helps us to consider the ways in which different groups or communities may be impacted by a technology, whether the implementation of a system into a particular context is appropriate, and whether the technology is being used in a way that is consistent with the values of the community.</p> <p>Addressing questions about sociocultural issues typically requires a deep understanding of the historical and current practices and dynamics of the society or community in question.</p> <p>Sociocultural fairness in healthcare</p> <pre><code>In the context of healthcare for example, it might be important to consider relevant aspects at the research and deployment stages of the system.\n\nWhich research projects get funded?\nWhat kinds of conditions get the most attention?\nFor example, a disproportionate percentage of resources might go to funding treatments which are very lucrative (e.g., treatment for chronic conditions). Similarly, a lot of resources might end up in research for medical (and cosmetic) anti-aging products and procedures, not necessarily because they are life-saving treatments, but because there is a high demand for them.\n\nOn the other hand, sometimes quite deadly conditions go underfunded due to their rarity. For example, research on a vaccine for the Hantavirus Pulmonary Syndrome (HPS) transmitted by rodents in Southern Chile and Argentina, one of the most sparsely populated regions on Earth, has been halted due to lack of funds even though the disease proves to be fatal for around 30-40% of those who contract it.\n\nAdditionally, at the deployment stage, it will be important to ask who will be most likely to access the new treatment, drug, or procedure?\nAre there any barriers of access to protected or other groups?\n</code></pre>"},{"location":"skills-tracks/rri/rri-203-1/#statistical-fairness","title":"Statistical Fairness","text":"<p>As a module on data-driven technologies, especially those that can be described as \"algorithmic\" or \"autonomous\", statistical fairness is a vital perspective to adopt. This perspective focuses on the statistical properties of the data-driven technology in question, including properties of the data used to develop the system, the metrics chosen to evaluate its performance, as well as formal ways of representing the sociocultural context in which the technology is deployed.</p> <p>Addressing questions about statistical fairness typically requires high levels of technical expertise and familiarity with up-to-date statistical techniques and concepts.</p> <p>Statistical fairness in healthcare</p> <pre><code>Say your team is developing a new ML model which helps diagnose people for a novel disease with symptoms that do not become apparent immediately.\n\nWhen training the model, some of the questions your team will need to think of in terms of statistical fairness are:\n\n- Are the classes in the training set balanced? As we will see, this is often not the case in healthcare data. So if they are not, how will your team address this issue?\n\n- Are there any relevant biases in your training data?\n\n- Which metrics will your team use to evaluate model performance?\n\n- Does the model perform similarly across different subgroups?\n\n- How will you decide when inevitable trade-offs, such as the trade-off between minimising false negatives or false positives, come up?\n\nThe topic of statistical fairness can quickly get quite technical. \nIf you are feeling confused, do not worry.\nWe will focus on statistical fairness in the context of healthcare in the [third section](rri-203-3.md) of our module.\n</code></pre> <p>These perspectives will form the basis of the remaining sections in this module, with a final section on the different biases that may arise during the design, development, and deployment of AI systems which can negatively impact the system's fairness.</p>"},{"location":"skills-tracks/rri/rri-203-1/#conceptual-matters-2-equality-and-equity","title":"Conceptual Matters (2): Equality and Equity","text":"<p>Before we end this section, we should also address another (smaller) conceptual distinction: the difference between equality and equity.</p> <p>Fairness is often spoken of in connection with these two neighbouring concepts. And, while related, they are not synonymous. As we will see, in some cases equality or equity may not even be desirable goals to pursue in the context of a particular project. And, if equality was our only goal, we would not be as interested in fair machine learning and AI, because we would be able to use simpler algorithms that just automate our chosen procedural rules and treat everyone the same.</p> <p>Equal Rights</p> <pre><code>While moral and legal concepts, such as universal human rights, ascribe some *fundamental notion* of equality to all people, at a practical level the idea of equality rarely goes very far as most people are not equal in a whole variety of ways.\nAs such, it is often not a good idea to treat people *equally* except in rare cases (e.g. sports or talent competitions).\nThis is why we often speak of \"equal opportunity\" rather than \"equal outcomes\".\n</code></pre> <p>For now, let's put these issues aside and just define the two concepts as follows:</p> <ul> <li>Equality is when all people are treated the same, regardless of any relevant differences in characteristics (e.g. age, sex, religious beliefs).</li> <li>Equity is where people are treated differently based on what they deserve, which will vary between contexts.</li> </ul> <p>Both require us to define key metrics.</p> <ul> <li>Equality requires us to define what counts as \"the same\" in terms of procedural rules or decisions.</li> <li>Equity requires us to determine how to measure what people \"deserve\".</li> </ul> <p>And, both decisions connect us back to our three procedural rules from earlier (e.g. who gets to decide on what people deserve), as well as the possible trade-offs and comparisons between relevant values (e.g. whether all stakeholders agree about whether a person is deserving of a particular outcome, and how desirable the outcome may be).</p> <p>If you are feeling a bit overwhelmed by all of this, please don't worry. Understanding fairness is challenging, as it requires a deep understanding of a wide network of concepts and a practical grasp on how to move around this network in a dynamic but purposeful way\u2014this is where our perspectives will come in handy, as they will help us focus on which part of the network is most relevant or salient to our concerns. To help you develop this ability, we will be returning to these concepts a lot throughout this module and anchoring them in concrete and illustrative examples. Doing so will allow you to build up a more intuitive understanding of the concepts, and how they relate to each other.</p> <p>In the next section we are going to start to build up our conceptual vocabulary by introducing the first perspective: sociocultural fairness.</p> <ol> <li> <p>Sen, A. (2009). The idea of justice. Penguin, United Kingdom.\u00a0\u21a9</p> </li> <li> <p>This example is taken from the following article. Waytz, A. (2010, March 9). The psychology of the taboo trade-off. Scientific American. https://www.scientificamerican.com/article/psychology-of-taboo-tradeoff/ \u21a9</p> </li> <li> <p>Fiske, A. P., &amp; Tetlock, P. E. (1997). Taboo trade-offs: Reactions to transactions that transgress the spheres of justice. Political psychology, 18(255-297). https://doi.org/10.1111/0162-895X.00058 \u21a9</p> </li> </ol>"},{"location":"skills-tracks/rri/rri-203-2/","title":"Sociocultural Fairness","text":"<p>Data-driven technologies are not designed, developed, or deployed in a vacuum. They are embedded in a social and cultural context from which they cannot be isolated, and are created by people with perspectives that are both indicative of and constrained by their own sociocultural context.</p> <p>This plurality of perspectives can problematise the fair design, development, and deployment of data-driven technologies because of the many different interpretations of what is fair. For instance, for some people, ensuring fairness will involve prioritising the reasonableness of a system's behaviour, leading to a practical emphasis on the transparency and accountability of the system's production. For others, fairness will mean prioritising the equitable impact of the system, brought about through a focus on the users that are deemed vulnerable or marginalised in the context of the system's use. While these two notions of fairness are not prima facie incommensurable in the manner introduced at the start of this module (recall Sen's example of the flute), their relative importance within a project can lead to diverging and incompatible choices in the governance of a project and implementation of a system (e.g. a highly transparent system that does not prioritise any group of users, or an opaque system that improves outcomes for a sub-group of users but in a way that is hard to explain).</p> <p>This plurality can also be problematic for those wishing to learn about the fair design, development, and deployment of data-driven technologies, because it can be difficult to know where to start when attempting to get a grasp on the necessary conceptual matters. The approach adopted in this section, therefore, will be to largely focus on a sociocultural understanding of fairness. Many of the conceptual lessons learned in this section will be challenging to apply directly in practice, and in some cases you may deem it beyond the immediate responsibility of your role to try to address them (e.g. complex and systemic issues of social inequality).</p> <p>If you find yourself thinking this way, then you would not be alone. The topics discussed in this section do not lend themselves to simple technical solutions. But this is sort of the point. The aim of this section is to help you understand the sociocultural context in which data-driven technologies are created, and to help you understand the sociocultural factors that can influence the fairness of a data-driven technology. Sometimes it is enough from an individual perspective to just be aware of these factors, in order to avoid doing anything to further exacerbate them. However, the more that teams and organisations do to understand and reflect on these factors, the more likely they are to be able to make a positive contribution to creating a fairer society for all.</p> <p>To quote a popular 80's cartoon:</p> <p>Sometimes knowing is half the battle.</p>"},{"location":"skills-tracks/rri/rri-203-2/#levelling-the-playing-field","title":"Levelling the Playing Field","text":"<p>Consider the following scenario. A game of football is organised between two teams. However, on the day of the match the teams have been organised such that one team is made up of the best players, with the highest level of fitness, and with access to the best equipment. Furthermore, the football pitch is located on a field with an uneven surface, such that the gradient declines steeply towards the goal of the weaker team, and the teams do not rotate the direction of play.</p> <p>How would you feel about playing this game if you were on the team with the disadvantageous conditions?</p> <p>Unless you like masochistic and unreasonable challenges or find it fun to lose, you would probably not be very happy about this situation. In fact you would probably feel that the game is unfair, and you would not be alone in thinking this way. In arguing your case you would be able to point to several clear factors that make the game unfair:</p> <ul> <li>Variation in the levels of fitness between the teams</li> <li>Variation in the quality in the equipment</li> <li>Variation in the skills of the players</li> <li>An uneven playing field</li> </ul> <p>Now, let's assume that the other team concedes to the (valid) reasons you have given for the game being unfair, and make certain concessions. First, they ensure everyone has access to the same equipment. Second, the match is relocated to a different field with an even surface.</p> <p>However, the composition of the teams remains the same. The game goes ahead, and unsurprisingly, the disadvantaged team loses.</p> <p>In this toy example, some of the factors that contribute to the initial unfairness of the game are ones that can be easily resolved (e.g. access to equipment, uneven field). But others remain harder to resolve, such as variation in skills. Some would argue here that variation in skills and fitness is a natural part of the game of football (and sports in general). As such, they would argue that the game is fair in this sense once the other factors are resolved.</p> <p>But what if we delve further into the historical factors behind this variation in skills and fitness?</p> <p>Perhaps the advantaged team come from a wealthy background, and have always been able to afford the best equipment and training facilities. And, perhaps the disadvantaged team have a lower level of respiratory fitness, in part, because of high levels of pollution in the area where they live.</p> <p>While the game is fairer in the adjusted scenario, it's still true to say that one of the teams has been playing on an uneven field before the game even begins. It's just that the field, here, is no longer a physical one; but a sociocultural one that spans back across many years.</p> <p>In relation to data-driven technologies, sociocultural fairness is about identifying the factors (or, sociocultural determinants) that precede the choices made during a project's lifecycle but which affect the overall fairness of the technology. In other words, before the game of the project lifecycle begins, how is the field already uneven and how can we level it to make sure that the game is fair? And, once the game has finished, how do the results affect future games?</p> <p>There may be little motivation to address unfairness in the context of sports, but life and society are not games.</p> <p> </p> Illustration by Public Health England which uses the football field analogy to depict social inequalities in the context of healthcare."},{"location":"skills-tracks/rri/rri-203-2/#identifying-fairness-factors","title":"Identifying Fairness Factors","text":"<p>Let's call the factors that are identified or appealed to when identifying or evaluating the overall fairness of some event or situation, 'fairness factors'. That is, the factors that we appeal to as reasons for why something is fair or unfair. Let us also try to further distinguish between a few types of fairness factors.</p> <p>For instance, in the case of the football game, there is clearly a difference between these three conditions:</p> <ul> <li>Having an uneven playing field that slopes towards one team's goal</li> <li>Variation in the skills of the players</li> <li>One team being given orange squash at half-time and the other one being given blackcurrant squash</li> </ul> <p>The first condition is highly discriminatory towards one of the teams, and has no place in a fair game of football. In fact, the organising committee for this league may even have some regulation in place that prohibits this. Let's call the set of fairness factors that are similar to this one, 'impermissible factors'. In other words, such factors are never permissible in a fair game (or, project).</p> <p>The second condition, in the context of sport, is still a discriminatory factor like the first because it plays a contributory role in determining or influencing the outcome of the game. However, although discriminatory in this sense, it is not necessarily unfair. Whether it is perceived as a relevant fairness factor will depend on a broader range of social and historical factors (e.g. whether the variation is itself the result of unfair patterns of discrimination). Therefore, let's call these fairness factors, 'potentially permissible factors' to highlight the need for further consideration and deliberation in evaluating their status.</p> <p>And, finally, the third factor seems utterly irrelevant to the outcome of the game and should, therefore, not be considered a 'fairness factor' at all. We can, therefore, call this set of (non fairness) factors 'irrelevant factors'. Needless to say, we won't spend much time on this final set.</p> <p>Equality Act 2010 and protected characteristics</p> <pre><code>The UKs [Equality Act 2010](https://www.gov.uk/guidance/equality-act-2010-guidance) makes it illegal to discriminate against someone because of a protected characteristic. The set of protected characteristics are\n\n- age\n- disability\n- gender reassignment\n- marriage and civil partnership\n- pregnancy and maternity\n- race\n- religion or belief\n- sex\n- sexual orientation\n\nSuch characteristics are often used as proxies for what we are calling 'impermissible factors'.\nThat is, they cannot be used (either directly or indirectly) as a means for discriminatory or prejudicial treatment.\nWe will return to these factors in the next section.\n</code></pre> <p>Differentiating between the three sets of factors in this way helps draw attention to the importance of public reason giving, deliberation, and justification in the context of fair design, development, and deployment of data-driven technologies. That is, in many cases it will not be sufficient to just identify those factors that are impermissible due to some legal or regulatory requirement (e.g. protected characteristics). In the remainder of this section, we will help illustrate this point by looking at three use cases of data-driven technologies, which intuitively seem to be unfair but where this may not arise due to some simple or easily identifiable 'impermissible factor'.</p>"},{"location":"skills-tracks/rri/rri-203-2/#the-digital-divide-who-gets-to-play-the-game","title":"The Digital Divide (Who Gets to Play the Game?)","text":"<p>In our running analogy, one of the football teams had access to better equipment and training facilities. The other team, in contrast, do not have the same access to high-quality resources. As such, while both teams are able to train in principle, it is likely that the team with access to better resources will be able to train more effectively or efficiently.</p> <p>Question</p> <pre><code>Are there barriers to access that prevent some people from participating at the same level as others?\n</code></pre> <p>Barriers to accessing resources, in the context of data-driven technologies, are factors that contribute to what is commonly referred to as the digital divide. In short, this term refers to the gap between those who have access to digital technologies and those who do not, as well as the varying levels of access in-between.</p> <p>A digital divide can exist in many forms, including:</p> <ul> <li>Differential access to the internet (e.g. variation in speed and reliability, barriers from higher costs)</li> <li>Differential access to compute resources (e.g. faster processors, more memory, etc.)</li> <li>Variation in digital literacy and skills (e.g. ability to think critically about online content, ability to manipulate data)</li> </ul> <p>Evaluating the sociocultural fairness of data-driven technologies requires us to explore holistic factors that are not always directly linked to the outcomes of the technology itself. Perhaps a team of developers are building an online app that recommends free activities in their local neighbourhood and uses data about their interactions to improve these recommendations over time. To improve the security and data protection properties of the app, the developers decide to use edge computing techniques<sup>1</sup> to process the data locally on the user's smartphone\u2014thereby enhancing the user's privacy. However, this design choice builds in a requirement that use of the app will require a smartphone with a certain level of processing power. In addition, the recommendations depend on a strong base of users in the local area, which may not be the case in some neighbourhoods.</p> <p>As such, two seemingly innocuous (and perhaps initially desirable) factors nevertheless create a divide that prevents some people from using the app.</p> <p>Fairness factors that are related to the digital divide are often raised under the banner of accessibility concerns. And, although there are some legal requirements or regulatory precedents that compel organisations to proactively consider and improve accessibility, because so many 'digital divides' arise without explicit forms of impermissible discrimination, they often go overlooked.</p> <p>Poverty</p> <pre><code>Epidemiologists and social scientists have long recognised that poverty is a key factor in determining outcomes in many areas of life (e.g. health, education, employment).\nBut, poverty is not a protected characteristic, and as such, despite many technologies implicitly discriminating against wide areas of society on the basis of economic conditions, there are few legal protections in place to ensure the use and implementation of technologies do not create unfair impacts on those who are already economically disadvantaged.\n</code></pre> <p>To be clear, not all barriers to access constitute impermissible factors. For instance, some systems or services may specifically target a group of people who are disadvantaged in some way (e.g. a web browser plugin that improves accessibility for people with visual impairments). This creates a barrier, in some sense, to the group of non-users who would not benefit from the technology. But it is hardly a barrier that many would consider unfair.</p> <p>Understanding and evaluating the socio-cultural fairness of data-driven technologies, therefore, requires us to consider whether any barriers exist that prevent people from using the technology, and whether these barriers are permissible or not.</p> <p>Question</p> <pre><code>Are any groups unfairly excluded from using the technology or benefiting from its use?\n</code></pre>"},{"location":"skills-tracks/rri/rri-203-2/#ghost-labour-who-built-the-field","title":"Ghost Labour (Who Built the Field?)","text":"<p>Before our game of football can begin, the field has to be prepared by a team of groundskeepers (whose work, we shall assume, is often invisible to the players). The previous question of 'who get's to play the game' is, therefore, one that presumes the existence of a playable location, which connects us to our next topic of sociocultural fairness.</p> <p>Question</p> <pre><code>Who is responsible for maintaining the field, and is their labour fairly compensated?\n</code></pre> <p>In the context of data-driven technologies, especially those that purport to demonstrate artificial intelligence, a lot of the groundwork is done by teams of data cleaners, labellers, or annotators. In some cases, this falls on the shoulders of junior members of a research team (e.g. research assistants). In other cases, this tiring and laborious work is carried out by ghost labourers\u2014people who work behind the scenes and are, typically, paid miniscule amounts for their work.</p> <p>The example of OpenAI's ChatGPT model is a good example of this latter kind of work, and also illustrates another factor related to ghost labour: extraction of value from the work of others. In January 2023, a Time article reported that OpenAI\u2014the company that developed ChatGPT\u2014used outsourced Kenyan laborers earning less than $2 per hour to help train the model by tagging and labelling data that was deemed toxic or offensive. While the end goal here may be laudable, the means by which it was achieved raises clear red flags about the fairness of developing such a model in the first place. Furthermore, the initial data used to train ChatGPT (and other models in OpenAI's library) depends on the availability of large amounts of data that is freely available on the internet. While these data are 'public', and, therefore, likely permissible to reuse by others, the fact that they are freely available is, in part, a result of the work of others who are not compensated for their efforts.</p> <p>Understanding and evaluating the sociocultural fairness of data-driven technologies, such as AI, require us to consider the ghost labour that enables them to demonstrate \"intelligent\" capabilities.</p> <p>Question</p> <pre><code>How should the people whose labour is responsible for the development of data-driven technologies be fairly compensated?\n</code></pre>"},{"location":"skills-tracks/rri/rri-203-2/#representation-how-are-the-different-teams-represented","title":"Representation (How are the Different Teams Represented?)","text":"<p>Returning to our running analogy, for the final time, and stretching the analogy to its very limits, let us consider how each of the teams were represented leading up to the game. Let's pretend that because of their ongoing success one of the teams has a large marketing budget and receives the best form of media representation. While this may not directly affect the outcome of the game, it has certainly been something that the team have long found appealing and motivating. In contrast, the other team has no marketing budget and have to rely on whatever coverage they receive, much of which is negative or false.</p> <p>Question</p> <pre><code>There is a clear difference in the way that the two teams are *represented*. Is this difference unfair?\n</code></pre> <p>This final theme is different from the others we have considered so far. Differential representation can lead to harmful physical consequences, such as a patient receiving the wrong treatment due to being misclassified by an algorithmic triaging system, or a person being wrongly sentenced to time in prison due to a biased algorithmic risk assessment system. But, fair representation matters in its own right. That is, whether certain groups of people are more or less likely to be represented by a technology in a way they perceive to be fair is an issue with intrinsic value, and is independent from whether the mis-representation leads to harmful outcomes. To see why, let's first consider the set of data-driven technologies that are used to classify people into groups.</p> <p>Many data-driven technologies sort and classify people and objects into groups. Some do this as a result of categories pre-selected by humans (e.g. demographic variables in a census), whereas others attempt to use forms of machine learning to organise people or objects into groups (or buckets) that are represented via a set of labels. In the context of objects, like the fruit below, the labels we choose will have no effect on the objects themselves. A banana will not mind if we call it a 'wonky yellow thing'.</p> <p></p> <p>The same cannot be said for humans.</p> <p>Try to think about the last time you felt as though you were either unfairly represented, or anxious about how other people saw you. Perhaps you want to be seen by your manager and colleagues as a hard-working and diligent employee. Maybe you worry that your friends do not think you are loyal, or that you are not a responsible and loving parent. Regardless of the underlying truth, the way that we perceive ourselves and the way that others perceive us is an important part of our identity and overall sense of well-being.</p> <p>So, now imagine that you find out that an organisation has an algorithm that automatically decides you are not worth showing job adverts for senior positions to, or that your interests on social media suggest you may be interested in counselling services.<sup>2</sup> Your misrepresentation in these examples may not be causing any immediate physical harm, but many would agree that there is something inherently unfair about this mis-representation and the systemic mis-representation of groups of people like you.</p> <p>The reasons you may give in explanation of this perceived unfairness will likely differ from those given by others. Some may emphasis matters of human dignity as a core motivating value in rationalising their position. Others may draw attention to the fact that some labels may perpetuate psychologically harmful stereotypes. Others still may emphasise the moral value of freedom of self-determination.</p> <p>For our present purposes it is not necessary to weigh or evaluate the relative importance of these myriad reasons. What matters here is that we recognise that understanding and evaluating the sociocultural fairness of data-driven technologies, such as AI, requires us to consider the way that they inevitably represent individuals and groups of people, and whether these representations are inherently fair.</p> <p>Question</p> <pre><code>How do those represented (and classified) by data-driven technologies perceive and judge the inherent fairness of these representations?\n</code></pre> <p>Meaningful Stakeholder Engagement</p> <pre><code>This section has focused on introducing some important conceptual distinctions, and, therefore, has largely set aside practical considerations (albeit temporarily).\nWe will address this limitation in the coming sections, but two important points are worth mentioning here.\n\nFirst, one of the primary means for identifying and evaluating fairness factors that are relevant to a particular data-driven technology is through stakeholder engagement.\nFurthermore, determining whether their influence on a project or system is permissible or impermissible will likely involve a similar process (e.g. consulting domain experts, seeking feedback from affected users).\nA key challenge here is resolving differences between separate groups who will inevitably disagree on whether some factor is fair or not (i.e. permissible or impermissible influence on the design, development, and deployment of data-driven technologies).\n\nHowever, and to the second point, stakeholder engagement can itself present barriers to accessibility for some groups.\nFor instance, if you choose to carry out stakeholder engagement workshops through a video conferencing platform, you may (unintentionally) create a digital divide that excludes those who do not have access to the internet or a computer.\n\nDiscussing these points in sufficient detail is beyond the scope of this section and module.\nHowever, if you are interested in exploring how to design and facilitate responsible forms of public and stakeholder engagement, we have a [separate skills track](#) that addresses these topics in the context of data science and AI projects.\n</code></pre> <ol> <li> <p>Edge computing is a distributed computing paradigm which brings computation and data storage closer to the sources of data, such as for example IoT devices or local edge servers.\u00a0\u21a9</p> </li> <li> <p>There have been real-life instances on both of these cases. For example, in 2015, it was reported that female job seekers were much less likely to be shown highly paid job adverts on Google than men did. On a more personal note, in this article, the author reflects on the amount of content on grief and loss they received, after some Google searches on the topic.\u00a0\u21a9</p> </li> </ol>"},{"location":"skills-tracks/rri/rri-203-3/","title":"Statistical Fairness","text":""},{"location":"skills-tracks/rri/rri-203-3/#the-computational-allure-of-statistical-fairness","title":"The Computational Allure of Statistical Fairness","text":"<p>Humans have tried to quantify fairness for centuries. The Law of Hammurabi, for example, states that if a man \"destroy's the eye of a man's slave or beaks a bone of a man's slave, he shall pay one-half his price\".<sup>1</sup> We rightfully recoil at the idea of such a law being espoused and adhered to in modern society. However, the idea that we can quantify fairness is still very much alive, and still not without its problems.</p> <p> </p> Photograph of the basalt stele with the Law of Hammurabi inscribed (By Mbzt - Own work, CC BY 3.0, https://commons.wikimedia.org/w/index.php?curid=16931676). <p>Fairness in ML and AI has seen enormous interest over the last decade, as many researchers and developers try to bring conceptual and pragmatic order to a vast and complex landscape of statistical methods and measures. This is understandable. Statistical measures are, typically, well-defined and can bring formal precision or a quantitative dimension to the discussion of fairness.</p> <p>Attempting to translate fairness into a statistical problem, therefore, has a certain allure because it suggests that if we can get the right measure or formula we can \"solve\" the problem of fairness in an objective manner. However, the allure of this statistical or computational approach needs to be properly seen for what it is\u2014an important tool, but one that has many limitations and needs to be used responsibly.<sup>2</sup> This section is about understanding both the strengths and limitations of statistical fairness.</p> <p>As a simple example of one limitation to get us started, consider the following question:</p> <p>Question</p> <p>Is it fair for a company to build an AI system that monitors the performance of employees in a factory and automatically notifies them when they are not meeting their targets?  Or, should the company instead invest in better training and support for employees and work with their staff to understand obstacles or barriers?</p> <p>As you can probably guess, based on the questions posed in the first section of this module, there is no easy answer to this question. In fact, you may have realised that a lot depends on factors that are not appropriately specified in the question (e.g. what happens if the staff continue to miss targets, who is this company, and who are their staff?). However, putting aside these unspecified factors, it should be evident that such a question cannot be reduced solely to a statistical problem.</p> <p>Problem Formulation</p> <p>This issue is a key motivator for the dual perspectives required by the <code>Problem Formulation</code> stage in our project lifecycle model.</p> <p>Here, judging the fairness of the company's actions would require a more holistic approach\u2014one that takes into account the social and cultural context of the company and its staff when defining the problem. Within this more holistic approach, statistical fairness could be an important tool that is used to help inform the discussion, but it will not be the only tool.</p> <p> </p> Venn diagram showing statistical fairness issues are a subset of wider sociotechnical considerations. <p>Therefore, in this section we will take a critical look at some proposed statistical concepts and techniques for understanding fairness in ML and AI, paying close attention to how they can contribute to the discussion of fairness, and where they fall short.</p>"},{"location":"skills-tracks/rri/rri-203-3/#fairness-in-classification-tasks","title":"Fairness in Classification Tasks","text":"<p>The area of ML and AI that has seen the most attention, with respect to fairness, is the task of classification.</p> <p>In a classification task, the goal is to predict which class a given input belongs to. This high-level description can be applied to a wide range of tasks, including:</p> <ol> <li>Which object is depicted in an image?</li> <li>Is an applicant likely to default on a loan?</li> <li>Is a post on social media an instance of hate speech?</li> <li>Will this patient be readmitted to hospital within 30 days?</li> </ol> <p>Types of Classification Task</p> <pre><code>These four questions are all instances of classification tasks, but some are instances of binary classification (e.g. yes or no answers) and others are multi-class problems (e.g. which of several classes does an instance or object belong to?).\n\nDo you know which of the four tasks are which?\n</code></pre> Answers <ol> <li>Multi-class: there are many objects that could be present in an image (e.g. cup, book, cat, dog).</li> <li>Binary: an applicant will either default on a loan or not (within the specified time of the loan agreement)</li> <li>Binary: a post either contains hate speech or it does not</li> <li>Binary/Multi-Class: this question appears to be a binary problem as it is currently framed, but could also be a multi-class problem if there were further buckets (e.g. 5 days, 10 days, 20 days, 30 days).</li> </ol> <p>There are also further complications with some of the other questions. For instance, although (3) is binary, much depends on how 'hate speech' is defined and operationalised. From a statistical perspective, there is a 'yes' or 'no' answer, but this abstraction screens off a lot of societal nuance and disagreement that exists where such clearly defined boundaries are unlikely to exist (e.g. vagueness of social categories).</p> <p>In this section, we are going to follow this approach and build up our knowledge of statistical fairness by using a simple binary classification task as a running example.</p> <p>This simplification has a cost and a benefit:</p> <ul> <li>Cost: we will not be able to capture the full complexity of statistical fairness in ML and AI, such as exploring all relevant metrics, or different types of problems (e.g. multi-class classification, non-classification tasks, etc.)</li> <li>Benefit: we will be able to focus more closely on key concepts and ideas and link them to ethical and social issues of fairness.</li> </ul> <p>This trade-off is appropriate for our present purposes, as we are primarily interested in responsible research and innovation, rather than, say, learning how to implement statistical fairness criteria in Python. Therefore, with these caveats in mind, let's dive in.</p>"},{"location":"skills-tracks/rri/rri-203-3/#classifying-patients-a-running-example","title":"Classifying Patients: A Running Example","text":"<p>First of all, let's introduce our classification task:</p> <p>A project team are developing a classification model, using supervised learning. The model will try to predict whether a patient has a disease or not based on a set of features. We're not going to worry about what this disease may be or which features are being used to train the model\u2014our first oversimplification. Instead, let's just assume the training data contains information about 100 patients and 2 of them are known to have the disease.</p> <p> </p> Graphic showing a population of one hundred people, with two of them sick (depicted in red)."},{"location":"skills-tracks/rri/rri-203-3/#statistical-fairness-issue-1-class-imbalance-and-accuracy","title":"Statistical Fairness Issue #1: Class Imbalance and Accuracy","text":"<p>When designing this model, the project team may decide that evaluating the performance of this model is best achieved by measuring its accuracy.</p> <p>However, this is where we come to our first problem that statistical fairness has to address: class imbalance.</p> <p>In our sample of 100 patients, only 2 of them have the disease. These two patients represent our <code>positive class</code> and are a clear minority when compared to the <code>negative class</code> (i.e. those who do not have the disease).</p> <p>Positive Classes</p> <pre><code>In ML classification, the term 'positive class' is used to refer to the class that we want to identify or predict.\nFor example, in a medical diagnosis task, the positive class might represent the presence of a particular disease, while the negative class represents its absence.\n\nThis can sometimes cause confusion, as the term 'positive' is often used to refer to something that is good or desirable, but in some tasks the identification of the positive class may be related to an undesirable or negative outcome.\n</code></pre> <p>Because of this imbalance between our two classes, if we were to build an (admittedly terrible) model that always predicted that a patient does not have the disease (i.e. classifies all patients as belonging to the <code>negative class</code>), then the model would have an overall accuracy of 98%. From one (very limited) perspective, the model would be performing very well, but we obviously know that this is not the case.</p> <p>When we have imbalanced classes (i.e. a small number of positive examples), which is quite common in real-world data, such a simplistic notion of accuracy is not going to be sufficient. Fortunately, there is a well-understood approach to dealing with this initial limited notion of accuracy: a confusion matrix that displays additional performance metrics.</p> <p> </p> Illustration showing that having balanced classes is important for optimal model performance. <p>A confusion matrix is a table that helps us to visualise the performance of a classification model by showing where it is doing well and what errors it is making. The table below shows a typical confusion matrix, where the columns represent the predicted label (i.e. the model's prediction) and the rows represent the ground truth (i.e. the actual state of the world).</p> Predicted Positive Predictive Negative Actual Positive True Positive (TP) False Negative (FN) Actual Negative False Positive (FP) True Negative (TN) <p>Table 1: Confusion matrix</p> <p>Different Terminology</p> <pre><code>Depending on your background, you may know some of the terms in this section by different names.\nFor example, 'False Positive' is sometimes referred to as a 'Type I Error', and 'False Negative' is sometimes referred to as a 'Type II Error'.\n\nThis will also hold true for terms we define later in the section\u2014an unfortunate, but unavoidable occurrence in such a multi-disciplinary context.\n</code></pre> <p>Let's translate this general table into our running example.</p> <p>Here, the positive class is the presence of the disease, while the negative class is the absence of the disease. Therefore, the four cells would correspond to the following:</p> <ul> <li>True Positive (TP): The model correctly predicts (T) that the patient has the disease (P).</li> <li>False Positive (FP): The model incorrectly predicts (F) that the patient has the disease (P) when they do not.</li> <li>False Negative (FN): The model incorrectly predicts (F) that they do not have the disease (N) when they do.</li> <li>True Negative (TN): The model correctly predicts (T) that the patient does not have the disease (N).</li> </ul> <p> </p> Illustration of a confusion matrix using the prediction of a disease as an example. <p>Prediction and Reality</p> <pre><code>It is common to distinguish between the class predicted by the model and the actual state of affairs by using the following formal convention:\n\n- $Y$: the actual value (e.g. presence of disease)\n- $\\hat{Y}$: the predicted value (e.g. prediction of disease)\n\nHere, $Y$ and $\\hat{Y}$ take the values $1$ or $0$ depending on whether the patient has (or is predicted to have) the disease ($1$) or not ($0$) respectively.\nWe will return to these variables later.\n</code></pre> <p>We can also represent this visually based on how our model performs with respect to the original dataset. Let's return to our original 100 patients, but this time let's make the disease a bit more prevalent, and let's see how well our model does at predicting the disease this time around.</p> <p> </p> Graphic showing the outcome of a classification task. The model correctly classified 65 out of 70 healthy people as healthy, and 25 out of 35 sick people as sick. However, it incorrectly classified 5 healthy people as sick, and 5 sick people as healthy. <p>This graphic allows us to see where our model has classified positive and negative instances correctly and incorrectly\u2014we won't worry about the proportions of true and false positives and negatives for now. This approach also allows us to use the initial four variables to define additional metrics that can help us to understand the performance of our model.</p> <p>Let's define some key terms, all of which are based on the original confusion matrix:</p> Name Label Definition Formal Definition Actual Positives <code>P</code> The number of data points in the positive class, regardless of whether they were predicted correctly or incorrectly. <code>P</code> = <code>TP</code> + <code>FN</code> Actual Negatives <code>N</code> The number of data points in the negative class, regardless of whether they were predicted correctly or incorrectly. <code>N</code> = <code>TN</code> + <code>FP</code> Accuracy <code>ACC</code> The proportion of data points that were correctly predicted from the entire dataset. <code>ACC</code> = (<code>TP</code> + <code>TN</code>) / (<code>P</code> + <code>N</code>) Precision (or Positive Predictive Value) <code>Precision</code> or <code>PPV</code> The proportion of data points that were predicted to be in the positive class that were actually in the positive class. <code>PPV</code> = <code>TP</code> / (<code>TP</code> + <code>FP</code>) Recall (or True Positive Rate, or Sensitivity) <code>Recall</code> or <code>TPR</code> The proportion of data points in the positive class that were correctly predicted. <code>TPR</code> = <code>TP</code> / <code>P</code> False Positive Rate <code>FPR</code> The proportion of data points in the negative class that were incorrectly predicted. <code>FPR</code> = <code>FP</code> / <code>N</code> <p>Table 2: A partial list of performance metrics</p> List of Metrics <p>There are many more performance metrics that can be extracted from a confusion matrix. We focus on the above ones simply because they are the ones needed for understanding some of the key concepts we discuss later on in this section.</p> <p>The following table from (Ruf and Detyniecki, 2019)<sup>3</sup> is a helpful reference for the most common metrics, and their paper is also an excellent resource in general.</p> <p></p> <p>We have already looked at the first three terms from Table 1. Let's now look at <code>Precision</code> and <code>Recall</code>.</p> <p>To get a general understanding of the two terms, consider the following image.</p> <p></p> <p>Here, the objective of the two fishing boats is to catch as many fish as possible. We can think of catching a fish as our positive class.</p> <p>The \"model\" used by the boat on the left (i.e. the fishing rod) is very precise\u2014it catches some of the fish (i.e. true positives), but not all of them, and it does not produce a lot of by-catch (e.g. false positives).</p> <p>The \"model\" used by the boat on the right, however, has good recall\u2014it catches (almost) all of the fish (i.e. true positives), but it also produces a lot of by-catch in the process (e.g. false positives).</p> <p>This analogy is useful because it allows us to understand the trade-off between precision and recall. As we raise the classifier's boundary (i.e. increase the size of the net), we will increase the number of true positives for our model (or minimise the number of false negatives). However, in doing so, we will also increase the number of false positives.</p> <p>But, if we lower the boundary (i.e. decrease the size of the net), we return to a situation where we have a higher number of false negatives but a lower number of false positives.</p> <p>This is known as the precision-recall trade-off, and it brings us to our next statistical fairness issue.</p>"},{"location":"skills-tracks/rri/rri-203-3/#statistical-fairness-issue-2-the-precision-recall-trade-off","title":"Statistical Fairness Issue #2: The Precision-Recall Trade-off","text":"<p>This issue of where to \"draw\" the boundary for our classifier is a common one in machine learning, and the choice is in part a value-laden one. Both precision and recall are trying to maximise the number of true positives, but they also try to combine this objective with the further goal of minimising the number of false positives and false negatives, respectively.</p> Metric Measures What it Optimises Precision TP / (TP + FP) Lower Rate of False Positives Recall TP / (TP + FN) Lower Rate of False Negatives <p>Table 3: Comparison between precision and recall.</p> <p>Another way to frame the intuition here is that <code>precision</code> is not as concerned with a higher number of false negatives, whereas <code>recall</code> is not as concerned with a higher number of false positives.</p> <p>But, this translates into very different scenarios in the real world.</p> <p>In our running example, a model with high <code>precision</code> will result in a higher rate of false negatives, which means that some patients may not receive treatment when they need it. In contrast, a model with high <code>recall</code> will capture more of these patients but some patients who do not need to be sent for further tests may have their time wasted and receive unsettling news.</p> <p>In general, therefore, a high recall value is typically preferred because we generally deem the harm caused by a false positive to be less than the harm caused by a false negative. But this is not always the case\u2014recall (pun intended) our example of the fishing boat, where minimising the amount of by-catch may be more important to ensure sustainable fishing practices.</p> <p>The following quotation is helpful in understanding this trade-off:</p> <p>\"A general rule is that when the consequences of a positive prediction have a negative, punitive impact on the individual, the emphasis with respect to fairness often is on precision. When the result is rather beneficial in the sense that the individuals are provided help they would otherwise forgo, fairness is often more sensitive to recall.\" -- Ruf and Detyniecki (2021)</p> <p>F0.5, F1, and F2 Scores</p> <pre><code>In machine learning, there is another approach to measuring accuracy that combines precision and recall into a single metric, known as the `F1` score.\nThe `F1` score is the harmonic mean of precision and recall, and is defined formally as follows:\n\n$$\nF1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n$$\n\nA high F1 score means that the model has both high precision and high recall, and it balances the contribution of these two metrics such that they have equal weight.\n\nHowever, as we have already seen, there are many real-world applications where either precision or recall may be more important goals.\nIn such cases, there are two variants of the F1 score that place different levels of emphasis on precision and recall: the F0.5 score (which favours precision), and the F2 score (which favours recall):\n\n$$\nF0.5 = (1 + 0.5^2) \\times \\frac{Precision \\times Recall}{0.5^2 \\times Precision + Recall}\n$$\n\n$$\nF2 = (1 + 2^2) \\times \\frac{Precision \\times Recall}{2^2 \\times Precision + Recall}\n$$\n\nNote the addition of what is known as the `beta` parameter in the F0.5 and F2 scores, which is used to control the relative weight of precision and recall.\nWe do not discuss these variants in significant detail here, but [this article](https://machinelearningmastery.com/fbeta-measure-for-machine-learning/) provides a helpful introduction to the various scores, how to derive them, and shows how to use them in practice with code examples in Python.\n</code></pre> <p>So far, we have been treating our sample of patients as a homogeneous group. That is, we haven't considered whether there are different subgroups of patients that can be identified based on relevant characteristics (e.g. age, sex, race, etc.). This is a big gap from the perspective of fairness, but addressing it requires us to add some further complexity to our running example.</p> <p>Therefore, let's assume that our 100 patients are now split according to their sex, and we have the following distribution.</p> <p> </p> Graphic showing a population of one hundred people, divided into forty females and 60 males. <p>With this update to our example, we need to return to our confusion matrix once more and consider how this new information alters our understanding of the performance of our model. Let's pretend that the performance of our model is as follows:</p> Predicted Positive Predicted Negative Class Total Actual Positive 10 10 20 Actual Negative 2 38 40 <p>Table 4: Confusion Matrix for Females.</p> Predicted Positive Predicted Negative Class Total Actual Positive 7 6 13 Actual Negative 22 5 27 <p>Table 5: Confusion Matrix for Males.</p> <p>Again, we can represent this visually:</p> <p> </p> Graphic showing the outcome of a classification task for a population of one hundred people, divided into forty females and 60 males. The figures in red represent people who have the disease and the figures in blue represent healthy people. The red shading represents the people the model (correctly or incorrectly) classified as sick, while the blue shading is the analogous for healthy predictions.. <p>Let's now calculate some of the performance metrics for these two confusion matrices and show them side by side:</p> Metric Male Female True Positive Rate (<code>TP</code> / <code>P</code>) 0.5 0.5 False Positive Rate (<code>FP</code> / <code>N</code>) 0.05 0.8 True Negative Rate (<code>TN</code> / <code>N</code>) 0.95 0.2 False Negative Rate (<code>FN</code> / <code>P</code>) 0.5 0.5 Accuracy ((<code>TP</code> + <code>TN</code>) / (<code>P</code> + <code>N</code>)) 0.8 0.3 Precision (<code>TP</code> / (<code>TP</code> + <code>FP</code>)) 0.8 0.24 Recall (<code>TP</code> / (<code>TP</code> + <code>FN</code>)) 0.5 0.5 <p>Table 6: Metrics for Females and Males.</p> <p>As you can see, there is variation in the performance of our model across the two subgroups in a number of categories. Although <code>recall</code> is the same for the two groups, the <code>accuracy</code> and <code>precision</code> of the models differ significantly because of the higher number of false positives for females.</p> <p>Translated into practical terms, this means our current model will send a large proportion of non-sick female patients for further tests or treatment, because it incorrectly classifies them as sick.</p> <p>What can we do about this?</p> <p>A few options include:</p> <ul> <li>Collecting more data and improving the representativeness of the dataset to see if we can reduce the class imbalance</li> <li>Altering the decision threshold for the model to see if the precision-recall trade-off can be improved, perhaps by using the F-score as a guide</li> <li>Oversampling from the minority group to rebalance the classes</li> </ul> <p>There are myriad pros and cons to these approaches. But before we even consider such approaches, we need to touch upon another important statistical fairness issue: the base-rate.</p>"},{"location":"skills-tracks/rri/rri-203-3/#statistical-fairness-issue-3-variation-in-base-rate","title":"Statistical Fairness Issue #3: Variation in Base Rate","text":"<p>First, the <code>base rate</code> is just the number of positive cases divided by the total number of cases:</p> \\[ BR = \\frac{P}{P+N} \\] <p>In the previous example, the <code>base rate</code> was the same for both subgroups. But this is often not the case in real-world datasets, especially in healthcare where the prevalence of a disease can vary drastically between different subgroups (e.g. affecting elderly patients more than young patients).</p> <p>So, let's now change the <code>base rate</code> and review how this affects our model.</p> Predicted Positive Predicted Negative Class Total Actual Positive 10 10 20 Actual Negative 2 38 40 <p>Table 7: New Confusion Matrix for Females</p> Predicted Positive Predicted Negative Class Total Actual Positive 18 9 27 Actual Negative 11 2 13 <p>Table 8: New Confusion Matrix for Males</p> <p>Let's now calculate some of the performance metrics for these two confusion matrices and show them side by side:</p> Metric Male Female Base Rate (<code>P</code> / <code>P</code> + <code>N</code>) 0.33 0.68 True Positive Rate (<code>TP</code> / <code>P</code>) 0.5 0.67 False Positive Rate (<code>FP</code> / <code>N</code>) 0.05 0.85 True Negative Rate (<code>TN</code> / <code>N</code>) 0.95 0.15 False Negative Rate (<code>FN</code> / <code>P</code>) 0.5 0.33 Accuracy ((<code>TP</code> + <code>TN</code>) / (<code>P</code> + <code>N</code>)) 0.8 0.5 Precision (<code>TP</code> / (<code>TP</code> + <code>FP</code>)) 0.8 0.62 Recall (<code>TP</code> / (<code>TP</code> + <code>FN</code>)) 0.5 0.67 <p>Table 9: New Metrics for Females and Males</p> <p>So, now we have a situation where the likelihood of having the disease is higher for females (i.e. a base rate of 0.68 for females and a base rate of 0.33 for males), and they are under-represented in our dataset. Furthermore, although there is an improvement from the previous set of metrics, this hypothetical model still has higher levels of performance for males who are over-represented in the dataset and less likely to have the disease.</p> <p>Intuitively, this is not a fair model. But can we be more precise in saying why?</p> <p>A first pass would appeal to moral and legalistic notions of non-discrimination and equality. We have not spoken about which features our model is using in this running example, but let's pretend that the project team did not use the <code>sex</code> of the patients as features for their model. Rather, they kept this data back solely as a means for evaluating the fairness of the model.</p> <p>Perhaps their reason for this decision was that they were motivated by a conception of fairness-as-blindness, where the model should not be able to discriminate against any protected characteristic.<sup>4</sup> However, despite not using <code>sex</code> when training their model, the model still ended up discriminating against female patients. Because of the variation in base rate, perhaps having a model that took account of <code>sex</code> would have been preferable, as it would have allowed the model to condition on the presence of this variable when forming predictions.</p> <p>In legal terms, the above decision of the project team would give rise to an instance of 'indirect discrimination', whether intentional or not. Indirect discrimination means that some rule, policy, practice, or in this case an 'algorithm', appears to be neutral with respect to how it treats different groups, but has a disproportionate impact on a protected group.</p> <p>The notion of a protected group or characteristic is a helpful one in fair data science and AI, because it allows us to bring additional precision to our discussion and helps us identify different types of fairness (and unfairness) as we will see in the next section.</p> <p>What is a protected group?</p> <p>As defined in the UK's Equality Act 2010<sup>5</sup>, it is against the law to discriminate against anyone because of the following protected characteristics:</p> <ul> <li>age</li> <li>gender reassignment</li> <li>being married or in a civil partnership</li> <li>being pregnant or on maternity leave</li> <li>disability</li> <li>race including colour, nationality, ethnic or national origin</li> <li>religion or belief</li> <li>sex</li> <li>sexual orientation</li> </ul> <p>When we refer to a 'protected group', we are referring to a group whose members share one of these characteristic.</p> <p>However the concept of a 'protected characteristic' is not without its problems. For instance, poverty is not a protected characteristic in the UK, but it is a well-known predictor of poor health outcomes. As such, if a model is trained on data that includes a proxy for poverty (e.g. the area in which a patient lives), it may end up discriminating against patients from poorer areas, but there would be little legal recourse for the patients to seek redress.</p> <p>What is a proxy?</p> <p>The term 'proxy' in data science is used to refer to the situation where one feature (A) is highly correlated with another feature (B). That is, A is a proxy for B, such that even if we do not use B as a feature in our model, the presence of A in our dataset could still capture the statistical effects of B.</p> <p>Your activity on social media, for example, can serve as a proxy for many different characteristics, including your <code>age</code>, <code>political beliefs</code>, and <code>sexual orientation</code>, and has been used by various companies (with dubious ethical standards) to infer these characteristics where they are not directly provided by the user.</p> <p>Therefore, although we will refer to 'protected characteristics' in the remainder of this discussion, it is worth keeping in mind whether the moral scope of this term is adequately captured by the legal definitions.</p>"},{"location":"skills-tracks/rri/rri-203-3/#statistical-fairness-or-non-discrimination-criteria","title":"Statistical Fairness (or, Non-Discrimination) Criteria","text":"<p>If we are to choose responsible methods for addressing inequalities in model performance, such as the ones we have just discussed, we need to be able to precisely identify how and where the model is failing to be fair.</p> <p>Researchers have put forward dozens of different statistical criteria that all seek to capture different intuitions about what is fair. However, in their excellent book on fair machine learning, Barocas et al. (2019)<sup>4</sup> show how these different criteria can be grouped together, forming three over-arching criteria of non-discrimination that all bear on the relationship between the model's performance across sub-groups. These three criteria formally define a variety of 'group-level' fairness criteria, which we can then use to determine targets for our model to achieve or constraints to impose during the development of the model.</p> <p>Individual vs. Group Fairness</p> <p>This section is concerned with group-level fairness criteria, and the introduction of a tool that helps to decide between different definitions of group fairness. Group-level fairness only provides assurance that the average member of a (protected, marginalised or vulnerable) group will be treated fairly. No guarantees are provided to individuals.</p> <p>However, it is worth noting that there are also individual-level fairness criteria, which are concerned with the fairness of the model's predictions between individuals. For instance, if two individuals are similar in terms of relevant characteristics, then they should have similar chances of being classified as positive or negative.</p> <p>See (Mehrabi, 2019) for a survey of bias and fairness in machine learning that includes a discussion of individual-level fairness criteria.<sup>6</sup></p> <p>To understand these criteria, we first need to define a few key variables:</p> <ul> <li>\\(Y\\) is the outcome variable, which is the variable we are trying to predict.</li> <li>\\(\\hat{Y}\\) is the prediction of the model.</li> <li>\\(A\\) is the protected characteristic. For simplicity, we will assume there is only one relevant protected characteristic \\(A\\), and that individuals either have it or do not.</li> </ul> <p>Now, we can define the three criteria as follows:</p> <ul> <li>Independence: the protected characteristic \\(A\\) is unconditionally independent of the model's predictions \\(\\hat{Y}\\)</li> <li>Separation: \\(A\\) is conditionally independent of the model's predictions \\(\\hat{Y}\\), given the true outcome \\(Y\\)</li> <li>Sufficiency: \\(A\\) is conditionally independent of the true outcome \\(Y\\), given the model's predictions \\(\\hat{Y}\\)</li> </ul> <p>These formal criteria can be a little hard to understand, and moreover, it can be hard to see why meeting them would be a good thing from the perspective of fairness.</p> <p>However, Barocas et al. (2019) explain that meeting each of these criteria equalises one of the following across all groups:</p> <ul> <li>Acceptance rate: the acceptance rate of the model, i.e. the proportion of individuals who are predicted to have the outcome, should be equal across all groups.</li> <li>Outcome frequency: the distribution of the outcome variable, i.e. the proportion of individuals who have the outcome, should be equal across all groups.</li> <li>Error rates: the error rates of the model, i.e. the distribution of false negatives and false positives, should be equal across all groups.</li> </ul> <p>There is not a simple one-to-one mapping that exist between the criteria and these three rates/frequencies, because for each of the three criteria there are myriad compatible definitions of fairness that aim at different rates/frequencies. For instance, Ruf and Detyniecki provide the following list of compatible definitions of fairness for each of the three criteria.<sup>3</sup></p> Criteria Compatible definitions of fairness Independence Demographic parityConditional statistical parityEqual selection parity Separation Equalized oddsEqualised opportunityPredictive equalityBalance Sufficiency Conditional use accuracy equalityPredictive ParityCalibration <p>Table 10: Various Definitions of Statistical Fairness grouped by three Main Criteria.</p> <p>It can quickly become overwhelming to try to decide which of these criteria to aim for, which type of fairness is most important to your project, and which of the many definitions of fairness to use for each criteria. Fortunately, there is a tool that can help us decide which of these criteria to aim for based on specific properties that may be present in our data or project.</p>"},{"location":"skills-tracks/rri/rri-203-3/#the-fairness-compass-a-tool-for-choosing-fairness-criteria","title":"The Fairness Compass: A Tool for Choosing Fairness Criteria","text":"<p>The Fairness Compass is a tool created by Ruf and Detyniecki (2021) to help researchers and developers choose between the three fairness criteria we have just discussed. It allows a project team to ask a series of questions related to their data and project, which can narrow down the permissible fairness criteria.</p> <p>Some of these questions include the following:</p> <ul> <li>Is there a policy in place that requires the project to go beyond the mere equal treatment of different groups to proactively reducing existing inequalities by boosting underprivileged groups (e.g. through affirmative action)?</li> <li>Are there equal base rates of the outcome variable across groups?</li> <li>Is your conception of fairness sensitive to a particular error type, such that you need to optimise for precision or recall?</li> </ul> <p>These questions are helpfully presented as a decision tree algorithm, terminating in the three fairness criteria discussed above, as well as a variety of specific implementations of each criteria.</p> <p> </p> Graphic showing the Fairness Compass Tool developed by Ruf &amp; Detyniecki (2021). <p>Interactive Tool and Paper</p> <p>We are not going to go through the entire tool here. Rather, an interactive version is available, which has helpful tooltips to explain the questions and the options available to the user. We also highly recommend reading the accompanying paper.</p> <p>One of the questions raised by the tool, however, brings us to our final statistical fairness issue: whether we have access to the ground truth.</p>"},{"location":"skills-tracks/rri/rri-203-3/#statistical-fairness-issue-4-ground-truth","title":"Statistical Fairness Issue #4: Ground Truth","text":"<p>The final assumption made in our running example that we will discuss is the existence of a ground truth label for each of our data points. In the case of the medical diagnosis example, this would be the presence of a disease, confirmed via some medical test, which we previously referred to as \\(Y\\).</p> <p>To effectively evaluate the performance of a model, we need to know the true value of the outcome variable for each data point \\(Y\\) and how far the model's predictions \\(\\hat{Y}\\) are from this true value.</p> <p>In the case of medical diagnosis, confidence in our ground truth may be achieved through secondary testing. For instance, if we know that an initial diagnostic test (responsible for generating our training data) is good but not perfect, then we can use a second test that has higher precision to confirm an initial diagnosis and reduce the number of false positives. In cases where the secondary test is more invasive or expensive, this may be the best option.</p> <p>But what happens when there is no such procedure?</p> <p>Mental health diagnosis is a good example of this. There is a huge literature on whether the procedures and labels we use to diagnose patients with mental health conditions are valid. For instance, how valid and reliable are the psychometric tests used during assessment or diagnosis (e.g. PHQ-9), given they rely on self-reporting? Are such tests able to accommodate intercultural variation in the way that mental health conditions are expressed and understood? Are constructs like depression or anxiety able to adequately capture the biopsychosocial complexity of mental health conditions?</p> <p>None of these questions should be misconstrued as undermining the existence or lived experience of mental health conditions. But they do raise questions about the use of such labels as the \"ground truth\" in the context of machine learning classification. Perhaps the labels are imperfect proxies for a more complex and unobservable phenomenon that is difficult to measure. But, if so, should we be using them as the basis for training and evaluating our models, and what further limitations does this place on our ability to reliably measure the fairness of our models?</p> <p>Unfortunately, there is no simple answer to such a question, as it is inherently entangled within the context in which the model is being used.</p> <p>The Problem with Missing Ground Truth: An Infamous Case Study</p> <pre><code>A now infamous case study published by Obermeyer et al. (2019), explored and dissected an algorithm that contained racial bias due to its use of predicted `health care cost` as a proxy for the actual target variable `illness` (or `healthcare need`).\n\nThe indirect consequence of this design choice was that the algorithm systematically underestimated the healthcare needs of Black patients, while overestimating the healthcare needs of White patients. \nThe result was significant disparities in care between the two groups.\n\nAs the authors go on to explain, this issue arises in part due to systematic biases in American healthcare (e.g. barriers to accessing healthcare, or implicit biases of healthcare professionals), which become encoded in the proxy variable `healthcare cost`.\n\nAs the authors note:\n\n&gt; We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.\n</code></pre> <p>This section has explored several concepts and issues with statistical fairness. However, we have barely scratched the surface of the topic.</p> <p>Nevertheless, we have seen enough to understand some of the limitations of the statistical fairness criteria we have discussed, and also introduce some practical metrics and tools that can be applied to help improve the fairness of our models in terms of non-discrimination and equal opportunity.</p> <p>In the final section of this module, we are going to look at why some of these issues arise in the first place, by exploring the multifaceted concept of 'bias'.</p> <ol> <li> <p>Nielsen, A. (2020). Practical fairness. O'Reilly Media.\u00a0\u21a9</p> </li> <li> <p>Powles, J., &amp; Nissembaum, H. (2018, December 7). The seductive diversion of \u2018solving\u2019 bias in artificial intelligence. Medium. https://onezero.medium.com/the-seductive-diversion-of-solving-bias-in-artificial-intelligence-890df5e5ef53 \u21a9</p> </li> <li> <p>Ruf, B., &amp; Detyniecki, M. (2021). Towards the right kind of fairness in AI. https://axa-rev-research.github.io/static/AXA_FairnessCompass-English.pdf \u21a9\u21a9</p> </li> <li> <p>Barocas, S., Hardt, M., &amp; Narayanan, A. (2019). Fairness and machine learning: Limitations and opportunities. fairmlbook.org http://www.fairmlbook.org \u21a9\u21a9</p> </li> <li> <p>See: https://www.gov.uk/discrimination-your-rights \u21a9</p> </li> <li> <p>Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., &amp; Galstyan, A. (2019). A Survey on Bias and Fairness in Machine Learning. ArXiv:1908.09635 [Cs]. http://arxiv.org/abs/1908.09635 \u21a9</p> </li> </ol>"},{"location":"skills-tracks/rri/rri-203-4/","title":"Identifying and Mitigating Bias","text":"<p>In the previous two sections, we explored theoretical and conceptual topics related to sociocultural and statistical fairness. In this section, we will bring this knowledge and understanding together to consider how we can identify and mitigate bias in machine learning and AI systems.</p> <p>Mitigating Bias</p> <p>The use of the term 'mitigating' bias is used in place of 'removing' or 'eliminating', as it is important to acknowledge that bias can never be completely removed or eliminated.</p>"},{"location":"skills-tracks/rri/rri-203-4/#identifying-biases","title":"Identifying Biases","text":"<p>Biases can affect the design, development, and deployment of machine learning and AI systems at many stages and in many ways. This is why the project lifecycle model has been designed to scaffold collective reflection, deliberation, and practical decision-making. The use of the project lifecycle model allows individuals and teams to systematically identify where biases may be present before the start of a project, or how specific biases can impact particular stages and cascade downstream.</p> <p>For example, perhaps you have heard about biased algorithms used in hiring or recruitment, which can discriminate against certain groups of people and make it comparatively harder for them to get a job.<sup>1</sup> Or, perhaps you have heard about biased machine learning systems that learn discriminatory patterns from the healthcare data they are trained on, resulting in unfair decisions being made about certain groups of patients.<sup>2</sup></p> <p>Identifying the source of these biases, their impact on the design, development and deployment of the respective system, and the societal effects on individuals and communities is a complex task. In the case of a biased hiring algorithm, for example, an important source of the biased decision-making was the data used to train the algorithm, but this data itself was an encoding of biased hiring practices within society and the organisation that predate the algorithm. Moreover, the deployment and ongoing use of the algorithmic system would further exacerbate bias in hiring, leading to a self-reinforcing cycle of discrimination.</p> <p>While the project lifecycle model is one supporting structure for identifying and mitigating bias, it is not the only one we will need. We also need to have a conceptual understanding of the different types of bias that can impact machine learning and AI systems, and how these biases can be mitigated. Therefore, in this section, we will introduce a simple taxonomy of biases and discuss how we may identify and mitigate them.</p>"},{"location":"skills-tracks/rri/rri-203-4/#types-of-bias-social-statistical-and-cognitive","title":"Types of Bias: Social, Statistical, and Cognitive","text":"<p>You will likely have some prior familiarity of the term 'bias', but your understanding of the concept may be grounded in a particular context, such as a disciplinary background (e.g. a formal understanding of bias as statistical bias) or a lived experience (e.g. political or social discrimination).</p> <p>This section will look at three types of 'bias' as they apply to and affect the project lifecycle model. The three perspectives that we will look at are: social, statistical, and cognitive bias.</p>"},{"location":"skills-tracks/rri/rri-203-4/#social-bias","title":"Social Bias","text":"<p>Social bias is the most wide-ranging of our three terms. In general, we can define social bias as follows:</p> <p>The presence of systematic and unfair assumptions, prejudices, or stereotypes that exist within society and that become encoded within the data used to design and develop algorithmic systems.</p> <p>As such, 'social bias' is the most morally loaded type of bias in our taxonomy, as it is often associated with some form of prejudice or discrimination that is morally abhorrent or wrong. For example, an inclination or disposition to treat an individual or organisation in a way that is considered to be unfair based on the presence of some characteristic or attribute (e.g. age, race, gender, political orientation). This understanding of bias is necessary to draw attention to pre-existing or historical patterns of discrimination and social injustice that can be perpetuated, reinforced, or exacerbated through the development and deployment of data-driven technologies. However, it is also one of the hardest to identify from within the project lifecycle, as many of the sources of social bias predate the design or development of the system in question.</p> <p>This is, perhaps, why many focus on 'data bias' as a more tractable and manageable problem, as identifying biased data is one step along the path to a deeper understanding and appreciation of the social biases that underpin it.</p> <p>However, it is also important to recognise that while some social biases can be identified through exploration and analysis of the data, the way to mitigate the underlying bias is not always to simply improve or augment the data themselves.</p> <p>Selection Bias: An Example</p> <p>To illustrate the above point, consider the following example of selection bias.</p> <p>Selection bias is a term used for a range of biases that affect the selection or inclusion of data points within a dataset.  For instance, where individuals differ in their geographic or socioeconomic access to a study that is being performed to collect data, the fact that some participants may struggle to travel could be a reason for their systematic exclusion from the study.  Or, in the case of the digital divide issue raised in the first section, if data collection is only carried out online, then members of society without access to a smartphone or the internet may not be appropriately represented in the data.</p> <p>While it may be possible to spot this missingness during data analysis, the appropriate solution to this social bias (e.g. improving access or reducing the digital divide) is clearly not just a matter of improving the data itself.  Even if the dataset could be augmented to include the missing data, it would not address the underlying social bias that is causing the data to be missing in the first place.</p>"},{"location":"skills-tracks/rri/rri-203-4/#statistical-bias","title":"Statistical Bias","text":"<p>If you are a data scientist, or use techniques from data science in your research or development, then it is likely that your understanding of bias is influenced by statistical concepts.</p> <p>Jeff Aronson explores the etymology of the term 'bias' in a series of interesting blog articles, which emphasise the statistical understanding of bias.<sup>3</sup> He begins by tracing it back to the game of bowls, where the curved trajectory of the bowl as it ran along the green reflected the asymmetric shape of the bowl (i.e., its bias). However, according to Aronson, the term was not used in statistics until around the start of the 20<sup>th</sup> century where it was used to refer to a systematic deviation from an expected statistical result that arises due to the influence of some additional factor. This understanding is common in observational studies where bias can arise in the process of sampling or measurement.</p> <p>On the basis of his historical review, Aronson identifies six features of definitions of bias that help to characterise an understanding of statistical bias:</p> <ol> <li>Systematicity: bias arises from a systematic process, rather than a random or chance process.</li> <li>Truth: a realist assumption that the deviation is from a true state of the world.</li> <li>Error: the bias reflects an error, perhaps due to sampling or measurement.</li> <li>Deviation (or Distortion): a quantity in which the observed result is taken to differ from the actual result were there no bias.</li> <li>Affected elements: the study elements that may be affected by the bias include the conception, design, and conduct of the study, as well as the collection, analysis, interpretation, and representation of the data.</li> <li>Direction: the deviation is directional, as it can be caused by both an under- or over-estimation.</li> </ol> <p>While some of these features are specific to a statistical framing of 'bias', others can be used to enhance our understanding of the other types of bias. For instance, 'systematicity' is arguably a necessary property for social biases (i.e., a bias that systematically leads to discriminatory outcomes). And, 'error' is sometimes a property of our next category: cognitive biases.</p> <p>Following Aronson's lead, we can define statistical bias as follows:</p> <p>\"A systematic distortion, due to a design problem, an interfering factor, or a judgement, that can affect the conception, design, or conduct of a study, or the collection, analysis, interpretation, presentation, or discussion of outcome data, causing erroneous overestimation or underestimation of the probable size of an effect or association\".</p> <p>Training-Serving Skew: An Example</p> <p>Training-serving skew is a bias that occurs when there is a mismatch between the data used to train a machine learning model and the data encountered during deployment.  More specifically, training-serving skew arises when the distribution of the data used to train the model is different from the distribution of the data that the model encounters during deployment and use within its production (or runtime) environment.</p> <p>Consider a model that predicts credit risk for loan applicants.  The model is trained on a dataset that includes information about the loan applicants, such as their income, employment history, and credit score.  But, there is a disproportionate number from one demographic group in particular (e.g. elderly applicants). As we train the model on this dataset, it may learn to associate certain characteristics with lower credit risk, which are not representative of the underlying relationship in the broader population.</p> <p>When the model is deployed in production, therefore, and used to make predictions for loan applicants from other demographic groups (e.g. younger applicants), the model's performance will be biased in favour of the older applicants.</p>"},{"location":"skills-tracks/rri/rri-203-4/#cognitive-bias","title":"Cognitive Bias","text":"<p>There are many cognitive biases, and others have put forward taxonomies that focus specifically on cognitive biases. This level of detail is not needed for our present purpose, so let's just define cognitive bias as follows:</p> <p>A tendency or predisposition for a person to form a judgement, recall information, or make a decision on the basis of a (potentially adaptive) heuristic, which in certain contexts can lead to the deviation from a proposed norm of rationality (e.g. logical reasoning).</p> <p>Our modern understanding of cognitive biases has been most heavily influenced by research conducted by Daniel Kahneman and Amos Tversky<sup>4</sup>. A lot of their work exposed a wide variety of psychological vulnerabilities, which impact our judgement and decision-making capabilities. In short, their experiments showed how individuals rely on an assortment of heuristics or biases, which speed up judgement and decision-making but also lead us astray.</p> <p>For example, consider the following example.</p> <p>The Linda Problem</p> <p>Linda is 31 years old, single, outspoken, and very bright.  She majored in philosophy.  As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations.</p> <p>Which is more probable?</p> <p>1) Linda is a bank teller. 2) Linda is a bank teller and is active in the feminist movement.</p> <p>Try to answer this question yourself, before you reveal the answer.</p> Reveal <p>The correct answer is (1). Did you get it right?</p> <p>If you got it wrong, you have just committed what is known as the 'conjunction fallacy'.  But don't worry you're in very good company! When Tvserky and Kahneman posed this question to a group of 88 undergraduate students, only \\(15%\\) got the correct answer.<sup>5</sup></p> <p>The reason it is (1) is because the probability of two events occurring in conjunction, such as Linda being both a 'bank teller' and 'active in the feminist movement' must be less than or equal to the probability of either event occurring on its own.  Formally, for two events \\(A\\) and \\(B\\):</p> <p>\\(Pr(A \\wedge B) \u2264 Pr(A)\\) and \\(Pr(A \\wedge B) \u2264 Pr(B)\\)</p> <p>Or, to put it more simply, someone cannot belong to the set of feminist bank tellers without also belonging to the set of bank tellers \ud83d\udc47</p> <p></p> <p>Tversky and Kahneman attributed this systematic error to what is known as the representativeness heuristic.  In short, people don't think about the conjunction of events or consider probability theory when formulating an answer. Instead, their choice is based on which of the two options is most representative of the description of Linda.  That is, they employ a mental shortcut (or, a heuristic) that in some instances lead to the right answer\u2014hence, their efficiency.  However, in in other cases their use lead to mistakes or errors in judgement.</p> <p>A critical perspective on the view of judgement and decision-making put forward by Kahneman and Tversky would view it as an attempt to catalogue a variety of cognitive failures or irrationalities that stem from an individual\u2019s inability to perform rational calculations. In this sense, cognitive bias would overlap with the 'error' component of statistical bias identified above.</p> <p>However, those who adopt a view known as 'ecological rationality' argue that such a perspective judges human agents against a normative standard of rationality that is unsuitable for situated agents whose choice behaviour is constrained by myriad cognitive and environmental factors (e.g. temporal constraints that force decisions, limited information). This alternative account, made famous by Herbert Simon, and later developed by Gerd Gigerenzer reframes a lot of human judgement and decision-making as underpinned by \u201cfast and frugal\u201d heuristics, which are highly adaptive and support decision-making in complex and uncertain environments. It's not necessary to delve into this debate for the present purposes, but it is an interesting tangent for those interested in exploring how the choice to present statistical information in different ways (e.g., as probabilities versus frequencies) can affect comprehension and understanding.<sup>6</sup></p> <p>When carrying out research and innovation in data science and AI, cognitive biases can have a variety of impacts on the tasks of the project lifecycle.</p> <p>Confirmation Bias: An Example</p> <p>Confirmation biases arise from tendencies to search for, gather, or use information that confirms pre-existing ideas and beliefs, and to dismiss or downplay the significance of information that disconfirms one\u2019s favoured hypothesis.  This can be the result of motivated reasoning or sub-conscious attitudes, which in turn may lead to prejudicial judgements that are not based on reasoned evidence.</p> <p>During the <code>data analysis</code> stage of the project lifecycle, for instance, confirmation bias could lead a data scientist to interpret the data in a way that supports her prior beliefs or assumptions, rather than considering alternative explanations.  Perhaps the data scientist is exploring a dataset on the effectiveness of a new drug, and has a hypothesis that the drug is highly effective.  Confirmation bias would cause her to selectively focus on information that supports her hypothesis, while overlooking information that contradicts it.</p> <p></p>"},{"location":"skills-tracks/rri/rri-203-4/#mitigating-bias","title":"Mitigating Bias","text":"<p>Now that we have a better understanding of the different types of bias, we can consider how to mitigate them. Although the specific mitigation strategies will vary depending on the particular bias, the bias taxonomy allows us to consider some general strategies and constraints.</p> Type of Bias Mitigation Strategies Social Bias Carry out inclusive, diverse, and meaningful stakeholder engagement with specific attention paid to identifying communities or groups that have additional barriers to participation.  Analyse gaps in demographic data in consultation with community groups and domain experts. Develop appropriate methods to address gaps and limitations based on context-aware reflection.  Organise participatory design workshops during <code>project planning</code> and <code>problem formulation</code> to ensure that the project is addressing the right problem and that the solution is appropriate for the intended users. Statistical Bias Ensure that the data collection and analysis processes are transparent and well-document, and that, where possible, data and methods are publicly available to allow for reproducibility or replicability of results.  Use additional evaluation metrics for your model to determine whether its performance applies equally for all individuals or sub-groups. Where relevant carry out intersectional analysis of multiple demographic or identity characteristics to identify biases that may not be apparent when considering a single characteristic.  Augment your dataset using techniques appropriate to the objective (e.g. addressing sparsity), such as data linkage or mixing, synthetic data generation, imputation, adding noise, transformation. Cognitive Bias Request a targeted and critical review of work by a committee, red team, or other group, who are tasked with finding gaps or issues. It is much easier to spot cognitive biases in the work or reasoning of others than ourselves.  Carry out early and ongoing forms of user engagement and training to ensure that best practices are adopted in the use of and interaction with the system (e.g. minimising automation bias).  Organise and facilitate skills and training events, such as webinars, workshops, self-directed learning, to upskill project team members or users (e.g. understanding and communicating uncertainty of predictive models, interactions with system interface). <p>You will note from the selection above that the mitigation strategies are likely to require diverse forms of expertise and knowledge. For instance, the use of 'evaluation metrics' to mitigate statistical biases will require a good understanding of the statistical properties of the data and the model. As such, this type of work can only be carried out by members of the team with high levels of statistical literacy or technical expertise (e.g. data scientists, data engineers, data analysts, etc.). In contrast, many of the mitigation strategies for social biases will require a good understanding of the social context in which the project is being carried out, which will require the involvement of individuals with specific domain expertise or experience in facilitation (e.g. social scientists, community organisers, etc.).</p> <p>However, a responsible approach to project governance and management will ensure that the team has the necessary expertise and capacity to carry out the work required to mitigate the identified biases. Admittedly, this can be a challenge for many projects, either because they may be carried out by teams that are highly specialised in a particular technical domain (e.g. computer vision, natural language processing, etc.) or because the team are under-resourced and do not have the capacity to bring in additional expertise. However, as is the case with many of the SAFE-D principles, the proposals outlined above can be treated as a set of practical guidelines to strive for, rather than as a set of non-negotiable criteria that serve as a checklist to be ticked off.</p> <p>Bias Checklist Reflectlist Activity</p> <p>The activity that is associated with this module takes the concepts introduced in this section further, and allows you to carry out a structured process of identifying possible biases and recording where in the project lifecycle they may arise, and what sorts of mitigation strategies might be appropriate to address them.</p> <p>The activity can be carried out in a group or on your own, and we encourage you to give it a try. You can download a copy of the activity handout here.</p> <ol> <li> <p>Dastin, J. (2018, October 11). Amazon scraps secret AI recruiting tool that showed bias against women. Reuters. https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G \u21a9</p> </li> <li> <p>Obermeyer, Z., Powers, B., Vogeli, C., &amp; Mullainathan, S. (2019). Dissecting racial bias in an algorithm used to manage the health of populations. Science, 336(6464), 447 - 453. https://doi.org/10.1126/science.aax2342 \u21a9</p> </li> <li> <p>Aronson, J. (2018). A word about evidence: 4. Bias \\textemdash Etymology and usage. Catalogue of Bias. https://catalogofbias.org/2018/04/10/a-word-about-evidence-4-bias-etymology-and-usag/ \u21a9</p> </li> <li> <p>For an excellent overview and introduction to this area, see Daniel Kahneman's book, Thinking, Fast and Slow.\u00a0\u21a9</p> </li> <li> <p>Tversky, A. &amp; Kahneman, D. (1983). Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment. Psychological Review, 90(4), 293 - 315. https://doi.org/10.1037/0033-295X.90.4.293 \u21a9</p> </li> <li> <p>For those who want to reconstruct the debate between Kahneman, Tversky, and Gigerenzer, the following papers can be read in order: (1) Tsversky, A. &amp; Kahneman, D. (1974). Judgement under uncertainty: Heuristics and biases. Science, 185(4157), 1124 - 1131. https://doi.org/10.1126/science.185.4157.1124, (2) Gigerenzer, G. (1991). How to make cognitive illusions disappear: Beyond \"heuristics and biases\". European Review of Social Psychology, 2(1), 83 - 115. https://doi.org/10.1080/14792779143000033, (3) Kahneman, D. &amp; Tversky, A. (1996). On the reality of cognitive illusions. Psychological Review, 103(3), 582 - 591. https://doi.org/10.1037/0033-295X.103.3.582, (4) Gigerenzer, G. (1996). On narrow norms and vague heuristics: A reply to Kahneman and Tversky. Psychological Review, 103(3), 592 - 596. https://doi.org/10.1037/0033-295X.103.3.592 \u21a9</p> </li> </ol>"},{"location":"skills-tracks/rri/rri-203-index/","title":"About this Module - Fairness","text":"<p>Important</p> <p>This module is not a technical introduction to Fair ML methods, nor does it attempt to provide an up-to-date overview of the current methods in the field. New methods are currently being developed at a rapid pace, and many of these methods are designed to solve problems with specific techniques (e.g. privacy-preserving federated learning to protect interest of vulnerable groups). It is not possible, nor desirable, to keep these resources up-to-date with these sorts of developments. Rather, we aim to provide clarity on the practical and ethical consequences of fairness in data-driven technologies. As such, we discuss those methods (or classes of methods) that are well established and have wide applicability across domains and use cases.</p> <p>This module is about the importance of fairness in data-driven and AI systems. It approaches fairness as a multi-faceted concept comprising different, equally relevant, aspects.  The module is divided into four sections: first, a broad look into what fairness is and the difficulties associated with defining the concept.  It then introduces two relevant aspects of fairness in the context of AI and data-driven systems: sociocultural fairness and statistical fairness.  The last section focuses on the role bias identification and mitigation play in the project lifecycle and how it intersects with questions of fairness.</p>"},{"location":"skills-tracks/rri/rri-203-index/#learning-objectives","title":"Learning Objectives","text":"<p>This module has the following learning objectives:</p> <ul> <li>Explore the concept of fairness in general, understanding the difference between utilitarian, economic egalitarian, and libertarian theories of fairness.</li> <li>Understand what is meant by fair procedures in allocating a good or service, as well as what is meant by outcome fairness and how we can make comparisons between different outcomes.</li> <li>Analyse the concept of sociocultural fairness and how it relates to sociotechnical systems (such as AI and data-driven ones). </li> <li>Learn how to identify fairness factors and the difference between impermissible factors and potentially permissible factors.</li> <li>Reflect upon aspects of sociocultural fairness such as: whether there are divides or barriers that obstruct people from accessing technology, whether there are differences in how different groups are represented by these technologies, and consider the invisible or ghost labour that goes into creating most sociotechnical system.</li> <li>Understand what is meant by statistical fairness, why it is relevant and what its limitations as a concept are.</li> <li>Identify different issues arising within statistical fairness (imbalanced classes, the precision-recall trade-off, variation in base rates, and establishing ground truth).</li> <li>Understand the three main criteria for non-discrimination for group fairness (independence, separation, and sufficiency).</li> <li>Identify the three different kinds of bias (social, statistical, and cognitive) and how these can arise within and AI or data-driven system.</li> <li>Explore different avenues for mitigating biases.</li> </ul>"},{"location":"skills-tracks/rri/rri-203-index/#table-of-contents","title":"Table of Contents","text":"<ul> <li> <p> What is Fairness?</p> <p>This section is the introduction for the module and it is a broad-stroke look into what the concept of fairness means.  As we will see, there is no easy answer to this as it will depend on underlying values which might be different for different people. It also focuses on questions around procedures for adjudicating goods or services (who should get what and how we decide on this), as well as questions around fairness in outcomes.</p> <p> Go to module</p> </li> <li> <p> Sociocultural Fairness</p> <p>This section introduces the concept of sociocultural fairness and its relevance to design, development, and deployment of AI and data-driven systems. We look at the pre-conditions required to make a system fair, which we call fairness factors and when and why they may or may not be in place.  This section also explores issues around the digital divide between different sectors in society and the ghost labour implicit in the creation of many of these systems.  </p> <p> Go to module</p> </li> <li> <p> Statistical Fairness</p> <p>This section focuses on statistical notions of fairness. It uses a binary classification task as a running example to help illustrate different group fairness considerations that arise when working with data. It looks at problems with imbalanced classes and accuracy, the prediction-recall trade-off, variations in base rates, and the problems with establishing ground truth. It also provides a broad overview of the most important statistical fairness or non-discrimination criteria for group fairness (independence, separation, and sufficiency). </p> <p> Go to module</p> </li> <li> <p> Practical Fairness</p> <p>In the module's final section we turn our attention to the concept of biases: what they are, how they can arise in a machine learning or AI project, and what can be done to appropriately mitigate them. We divide biases into three broad categories; social, statistical, and cognitive, and then look at each category in detail. Finally, we give an overview of different strategies teams and researchers can employ to mitigate biases once they have been identified.</p> <p> Go to module</p> </li> </ul>"},{"location":"skills-tracks/rri/rri-204-1/","title":"What is Explainability?","text":"<p>When digital artist Jason M. Allen of Pueblo West, Colorado created the image, 'Th\u00e9\u00e2tre D\u2019op\u00e9ra Spatial', and entered it into the Colorado State Fair's annual art competition, he caused quite a controversy. The admittedly captivating image was in one sense of the term \"created\" by Mr. Allen, but not with the traditional software tools of a digital artist (e.g. Photoshop, Illustrator's Tablet). Instead, all it took for Mr. Allen to create this image was a single text prompt used as input into a generative AI model known as Midjourney.</p> <p>Like its more messy and organic counterpart, digital art typically requires hours of painstaking work. But AI images like those generated by Midjourney can be created in a matter of seconds, using carefully chosen prompts such as the following:</p> <p><code>/imagine prompt: unexplainable AI, surrealism style</code></p> <p>The results are often stunning, as is the case with 'Th\u00e9\u00e2tre D\u2019op\u00e9ra Spatial'. But in other cases, its clear from minor details that the AI lacks any real understanding of certain concepts or objects, as can be seen from the image generated by the above prompt:</p> <p></p> <p>When Mr. Allen entered his image into the competition, which he ended up winning, the public response was mixed. Notably, other artists whose livelihoods and welfare depend on the public valuing and appreciating the fruits of their labour were anxious and angry about the widespread adoption of AI-generated art. Many decried the fact that no real skill or effort was required to create such images, and that the widespread use of AI image generators would in turn devalue both human labour and an important source of aesthetic value in human society.</p> <p>You may have an opinion on this controversy, but this module is not about digital art, the value of AI art, or whether such images are in fact \"art\". Rather, it is about explainability of data-driven technologies.</p> <p>So, what does an AI generated image have to do with explainability? Simply put, it is the fact that no one has any idea how these systems produce the images they do. Like other use cases of generative AI, such as large language models, they are a great example of black box systems.</p> <p>You may not think an ability to explain how a system produces images matters much from a societal perspective, as the harms they can cause do not arise from the fact that the generative models are unexplainable. Or, you may think that the value of generative AI in such sectors can improve regardless of the model's transparency or interpretability.</p> <p>In both cases, a plausible case could be made in favour of either stance, and vice versa.</p> <p>But, as we will see over the course of this model, many use cases of such systems, and even more simple algorithmic tools, depend crucially on properties such as transparency, interpretability, and accessible explanations. This module is, therefore, about understanding what these properties mean and require from us, and when and why they are important.</p>"},{"location":"skills-tracks/rri/rri-204-1/#the-scope-of-explainability","title":"The Scope of Explainability","text":"<p>A lot of research has already been published on the concept of 'explainability' and neighbouring concepts (e.g. model interpretability, project transparency). Some research seeks to introduce practical methods and techniques for making predictive models more interpretable, whereas other research seeks to clarify the differences between the related concepts. Because this module is focused on understanding why explainability matters for responsible research and innovation, we will explore both areas. However, there are two important caveats here:</p> <ol> <li>This is not a module to teach data scientists or machine learning engineers how to use or implement existing methods or techniques.</li> <li>This module aims to be consistent with widely agreed uses of concepts and terminology, but also has its own unique perspective on the topic.</li> </ol> <p>On the second point, the first thing to note is that 'explainabilty' is employed with the broadest scope of any of the concepts we discuss and, as such, it subsumes many of the neighbouring concepts. For instance, 'interpretability' is necessary for 'explainability', but not sufficient. Or, to put it another way, 'explainability' is our catch-all term that encompasses many of the related and neighbouring concepts. Therefore, it makes sense to start the module by explaining what we mean by 'explainability'.</p>"},{"location":"skills-tracks/rri/rri-204-1/#what-is-explainability_1","title":"What is Explainability?","text":"<p>Before we discuss explainability, let's look at a closely related concept, 'interpretability'. Following Miller (2019), let's define 'interpretability' as follows:</p> <p>Interpretability is the degree to which a human can understand the cause of a decision.<sup>1</sup></p> <p>In the context of data-driven technologies, <code>interpretability</code> is the degree to which a human can understand the cause of an output given by a predictive model, or why a generative AI model produced a particular image. As a property of the model or system, interpretability can be measured in degrees, and it is often the case that a model is more or less interpretable dependent on factors such as 'who is doing the interpreting' and 'what is being interpreted'. We will look at these factors in more detail later in the module.</p> <p>Interpreting Images</p> <p>To give two examples of the above aspects of interpretability, ask yourself if you can interpret the meaning behind the following images:</p> <p> Example 1. A chess board showing a popular tactic for winning a game. </p> <p> Example 2. A chest X-ray showing pulmonary disease. </p> <p>Unless you are a chess player or radiologist you will not be able to interpret the significance of the patterns in either of these images. And, dependant on how complex the chess game or physiological issue, it may be the case that only highly experience chess players or radiologists could interpret similar images.</p> <p>Understanding the cause of a decision is no guarantee that the decision can be explained in the appropriate manner, however, nor that the explanation would be deemed acceptable by the recipient of the explanation.</p> <p>Consider the following example. You are asked to provide an explanation for how you made a highly accurate prediction about your partner returning home from work at a very specific time one day. If you responded, \"They messaged me as they left the office.\", this would probably suffice as what we can call a 'folk psychological explanation'\u2014that is, an explanation provided in everyday discourse. Furthermore, most people would be able to infer from this explanation that your partner follows the same route home each day and that, traffic permitting, their journey takes 25 minutes.</p> <p>But now, let's assume you are asked to explain how you were able to predict the behaviour of a complex, dynamic system, such as the local weather in the next 12 hours. Perhaps in this scenario, you are a data scientist working for a national meteorological centre, who are using data-driven technologies to improve forecasting. A government representative is in attendance at a seminar, trying to determine whether to invest centralised funds in improving the techniques you have been trialling. This time around, an explanation such as \"our system provided me with a notification\" is not going to cut it.</p> <p>These two examples highlight an important point about explanations: they are situated within a specific context that has its own expectations and norms for what constitutes a valid or acceptable explanation. As such, we can define explainability as follows:</p> <p>\"The degree to which a system or set of tools support a person's ability to explain and communicate the behaviour of the system or the processes of designing, developing, and deploying the system within a particular context.\"</p> <p>Defining explainability in this way help us draw attention to the fact that it will vary depending on the sociocultural context in which it is being assessed. While this context sensitivity also holds true, to some extent, for interpretability, there is a greater emphasis on communicability of reasons in the definition of explainability. To see why, let's look at one reason why being able to explain the behaviour of a system or the processes by which it produces an output is important.</p>"},{"location":"skills-tracks/rri/rri-204-1/#reliable-predictions-and-inferential-reasoning","title":"Reliable Predictions and Inferential Reasoning","text":"<p>The philosopher, Bertrand Russell, had a good (albeit morbid) illustrative example of what is known as the 'problem of induction'. A turkey that is fed by a farmer each morning for a year comes to believe that it will always be fed by the farmer\u2014it's a smart turkey! Each morning is a new observation (or data point) that confirms the turkey's hypothesis. Until, on the morning of Christmas Eve, the turkey eagerly approaches the farmer expecting to be fed but this time has its neck broken instead!</p> <p>The problem of induction that is highlighted by this cautionary tale can be summarised as follows:</p> <p>Cautionary tale</p> <p>What reasons do we have to believe, and justify, that the future will be like the past? Or, what grounds do we have for justifying the reliability of our predictions?</p> <p>Being able to deal satisfactorily with problem of induction matters because we do not want to be in the position of the turkey. We want reliable and valid reasons for why we can trust the predictions made by our systems, especially those that are embedded within safety critical parts of our society and infrastructure.</p> <p>This is also why we care about more than only measuring the accuracy of our predictions. The turkey's predictions were highly accurate (99.7% over the course of its life), but the one time it was wrong really mattered!</p> <p>Obviously, not all predictions carry the same risk to our mortality. Where we were making a prediction about when our partner would arrive home, for example, we all intuitively understand the inherent uncertainty associated with the inference from past observations\u2014traffic is unreliable, and journeys during rush hours can vary widely. In such cases, we are happy with both the high level of variance in our predictions and a corresponding folk explanation that we can provide to others. But, in cases where the consequences of unreliable or false predictions are more severe or impactful, it is important that we can provide justifiable assurances to others about the grounds for trusting the behaviour and processes of a system.</p> <p>In this regards, explainability is about ensuring we have justifiable reasons and evidence for why the predictions and behaviour of a model or system are trustworthy and valid. Understanding this communicative and social perspective also helps us appreciate why explainability is so important in safety-critical domains, as well as other areas like criminal justice where false predictions have high costs associated with them.</p> <p>If we don't have trustworthy reasons for the reliability and validity of a model's predictions, we are not going to want to deploy it in a context like healthcare where people's well-being depend on high-levels of accuracy or low levels of uncertainty.</p> <p>So, now we have a grasp on what explainability is and why matters, what do we need to do to ensure its existence?</p>"},{"location":"skills-tracks/rri/rri-204-1/#factors-that-support-explanations","title":"Factors that support explanations","text":"<p>Over the course of this module, we will look at the following three factors in more detail. For now, just consider how they are both conceptually different from explainability but also support it as a goal:</p> <ul> <li>Transparent and accountable processes of project governance that help explain and justify the actions and decisions undertaken throughout a project's lifecycle</li> <li>Interpretable models used as components within encompassing sociotechnical systems (e.g. AI systems)</li> <li>An awareness of the sociocultural context in which the explanation is required. A special focus should be given to the potential communication barriers that may be in place, with an emphasis in building explanations which are suited to the  relevant stakeholders.</li> </ul> <ol> <li> <p>Miller, T. (2019). Explanation in artificial intelligence: Insights from the social sciences. Artificial intelligence, 267, p. 1-38. https://doi.org/10.1016/j.artint.2018.07.007 \u21a9</p> </li> </ol>"},{"location":"skills-tracks/rri/rri-204-2/","title":"Project Transparency","text":"<p>A team of lawyers are building a case and are at the stage of undertaking discovery\u2014a process of gathering information about a case that typically requires a legal team to request information from another company or organisation (e.g. the legal team representing the other party in a dispute). One of the areas of concern is about access to information regarding the use of an algorithmic decision-making system, which was used to make a decision that is subject to dispute in this case. They make a request to see information about the algorithm, including how it was designed, who was involved, and how it works.</p> <p>When the team of lawyers receive the requested information, there are two problems:</p> <ol> <li>The other team have sent across mountains of documents and files, in an attempt to bury any incriminating information (e.g. sending over thousands of hours of transcripts of meetings, emails, and other documents).</li> <li>Within this documentation there is information about the structure of the algorithm, but it is written in technical jargon that is hard for the lawyers to understand.</li> </ol> <p>This hypothetical scenario introduces two concepts of relevance to both explainability in general, and project transparency in particular:</p> <ol> <li>Transparency is not the same as accessibility.  In the example above, the second team of lawyers have made information \"accessible\" to the first team in principle but have done so in a way that few would argue is transparent in a meaningful way.<sup>1</sup></li> <li>Transparency is necessary for explainability.  The first team of lawyers will need to be able to explain the algorithm's structure in court in a way that is understandable to the judge and jury. Developing this explanation requires a certain degree of transparency in the documentation about how the algorithm was designed, developed, and deployed.</li> </ol> <p>As with many of the SAFE-D principles, explainability is closely related to and overlaps with other neighbouring concepts, such as accessibility and transparency.</p> <p>In the course of this section we will look at how these related concepts intersect, some practical choices and mechanisms by which project transparency and accessibility can be achieved, and how transparency and accessibility can improve and enhance explainability. Before we do this, however, let's consider a question that raises other questions about what we are trying to explain and where we may need transparency.</p>"},{"location":"skills-tracks/rri/rri-204-2/#what-are-we-trying-to-explain-where-do-we-need-greater-transparency","title":"What are we trying to explain? Where do we need greater transparency?","text":"<p>Consider the following scenario.<sup>2</sup> A team of data analysts who work for a travel booking website are asked to explain why a model has altered its predictions about customer purchasing behaviour. This time, the model is used to drive a recommender system, which shows holiday packages to customers based on its predictions about which are most likely to be purchased. Perhaps the system is recommending significantly more trips to beach resorts, whereas previously it was recommending ski trips. Here, if the features used by the model were investigated it would be easy enough to identify that <code>season</code> is a feature with high importance for the model. It is well known that customers alter their purchasing behaviour between seasons (e.g. Winter, Summer).<sup>3</sup> From this we could explain the change in the model's predictions, as a result of a significant change in the data distribution, which itself is a representation of a change in the underlying phenomena (i.e. changing seasons). Simple enough.</p> <p>But now let's assume that there is another change, which results in a significant drop in conversion rate (i.e. the ratio of the number of people who view, say, a holiday deal, to the number who actually purchase the holiday suddenly drops). That is, customers are not just booking different holidays, they are not booking as many holidays at all. Again, this may seem like another case where there is a need to explain the model's behaviour in terms of an underlying shift in the data distribution, which is in turn representative of some change in the underlying phenomena. However, this time, let's pretend that the problem turns out to be a fault with a third-party piece of software, used as a dependency in the team's data pipeline, which is now causing the data about a user's <code>location</code> to be incorrectly recorded.</p> <p>As it turns out, the company's model has learned that those who live in affluent neighbourhoods are more likely to purchase more expensive packages, and the company's recommendation system uses this to show customers holidays that are in their predicted price range, or dynamically alter the price of holiday packages based on their estimated \"willingness-to-pay\"\u2014two ethically dubious practices known as personalised and dynamic pricing<sup>4</sup>. However, due to the aforementioned fault in the data pipeline, all customers are now being shown the same, more expensive, holiday deals because their <code>postcodes</code> are all being recorded as all being located in affluent neighbourhoods. As such, fewer customers are purchasing their packages, because they cannot afford them, and the conversion rate has dropped.</p> <p>Again, there is no fault with the model (or its parameters). Rather, the target of any explanation lies in the data and the generative mechanisms responsible for producing the data. The model is still making the same predictions, but the predictions are now incorrect and the recommender system is now unable to recommend the correct holiday packages to customers.</p> <p>This hypothetical example exposes an important point about transparency and explainability: the locus of our explanation will not always be the model. Instead, the focus of an explanation may be the data pipeline used to drive the model's predictions or other parts of the system, such as the user interface. As such, the sort of transparency that we are interested in is not merely the transparency of the model itself, but rather the transparency of the project (and system) as a whole. We will look at model interpretability techniques in the next section, some of which can also help project teams debug or identify the source of issues at other points in a project's lifecycle.</p> What about the transparency of the learning algorithm? <p>Related to the previous example, it is also possible that when trying to diagnose or triage issues to explain a model's (perhaps unexpected) behaviour, the algorithm by which a model was trained can be an informative source when constructing an explanation. For instance, understanding how a convolutional neural network learns to classify images (e.g. the initial layers that extract predictive features) may help to diagnose (and subsequently explain) why a model is misclassifying images.</p> <p>A famous example here is the instance of an algorithm that learned to classify a picture of a husky as a wolf because of the presence of snow in the background of the images. The algorithm learned that snow in the background was a good predictor of an image being of a wolf\u2014as most wolves in the training set were show against a snowy backdrop.<sup>5</sup> However, an over-reliance on this feature created a spurious correlation that did not hold true for the actual subjects of the images (e.g. huskies).</p> <p>The type of transparency that is required here pertains to the algorithmic process by which the predictive model is trained and developed.</p> <p>As you may tell from the above two examples, the nature of our problem will determine the location of the desired transparency and the locus of our explanation. However, this will largely remain a context-dependent issue, and so in this section we will take a simpler approach by advocating for global project transparency by default. And, where there are overriding factors such as intellectual property concerns, or sensitive information disclosure issues, these can be evaluated by a project team on an ad hoc basis.</p>"},{"location":"skills-tracks/rri/rri-204-2/#what-does-responsible-project-transparency-look-like","title":"What does responsible project transparency look like?","text":"<p>In a previous module (required for this module) we introduced the project lifecycle model. This model is a useful framework for thinking about the different stages of a project, and the different types of transparency that may be required at each stage. Rather than going through each stage separately, we will instead group the stages (and their corresponding tasks) into several categories that can help us understand the practical mechanisms and processes that are likely to be required to achieve transparency in a project.</p> <ul> <li>Tasks that involve choices about how a project should be governed (e.g. defining the nature of the problem that a data-driven technology is designed to address and the algorithmic procedure by which it is implemented)</li> <li>Tasks that involve what we can term 'data stewardship' (i.e. the management of data and the data pipeline)</li> <li>Tasks that involve the engagement of stakeholders (e.g. members of the public, or other professionals in the relevant domain).</li> </ul> <p>Starting with the first category, the following tasks are significant (although not exhaustive) examples of decisions or actions that can have an impact on explainability, and, therefore, require certain levels of transparency:</p> <ul> <li>Determining the Problem the System is Designed to Address: this task includes information about why the problem is important and why the technical description (e.g. translation of the set of input variable into target variables) is adequate for the problem at hand. For instance, why a set of features about a candidate are adequate measures for assessing their <code>suitability for a job role</code>. Aside from the technical \"solution\" to the problem, there is also a social dimension that needs to be justified, such as why an automated system is appropriate for use in hiring decisions (e.g. demonstrating that the system is not biased against protected groups)</li> <li>Identification and Mitigation of Biases: decisions about which biases may be relevant for the project and why any chosen mitigation strategies are sufficient to address them.</li> </ul> <p>Tasks such as these ones may pose challenging questions during the process of deliberation, but when it comes to project transparency, clear and accessible documentation is usually sufficient to ensure good levels of project transparency.</p> <p>The second category of tasks, data stewardship, is also important for explainability, and the following are illustrative examples of tasks that would require appropriate levels of transparency:</p> <ul> <li>Data Provenance: the source of the data used throughout the project lifecycle has myriad implications for explainability, including explanations about how the quality of the data was evaluated, or how the legal basis for its use was established.  Clear and accessible documentation on data sources will therefore be crucial for project transparency.</li> <li>Data Pipeline Engineering: many stakeholders will be interested in assessing the safety and security of data, especially where it contains sensitive or personal information.  Being able to provide explanations about how the data pipeline was constructed, therefore, will be important for explainability.</li> <li>Exploratory and Confirmatory Data Analysis: although data analysis is highly iterative, a clear record of the analysis techniques employed and the rationale for their use can help ensure a high level of transparency.</li> </ul> <p>Many data scientists, analysts, and engineers are already familiar with the use of notebooks for documenting data processing and analysis (e.g. Jupyter). And, similarly the use of open access repositories for storing and sharing code and data is also becoming more common\u2014guides and templates are available to support those who may be unfamiliar with transparency best practices of this kind.</p> <p>Finally, the third category emphasises the importance of accessibility for those who are directly or indirectly involved with a project:</p> <ul> <li>Stakeholder Identification: choices made about which stakeholders are relevant to the project, and by proxy which are irrelevant or non-significant, will affect the types of explanations that are prioritised (e.g. members of the public or professionals in the relevant domain).  This is not limited to those who are involved with, say, participatory design workshops early on to establish high-level constraints. Rather, it can extend across the whole project lifecycle to include, say, users of the system being developed, to ensure they are able to access information needed to carry out their associated responsibilities.</li> <li>External Auditing: some projects will require auditing as a pre-requisite for the deployment of a system, whether by a separate regulatory body or by a partner organisation.  The degree of transparency required will depend on the nature of the audit, and so early engagement and assessment of the transparency needs and the basis for any requested explanations can help to ensure that the project team is able to meet the requirements of the audit.</li> </ul> <p>Questions</p> <ul> <li>What other tasks can you think of, which may occur during one of the project lifecycle stages, that would require transparency? </li> <li>How would this transparency be achieved and how would it contribute to explaining any decisions or actions taken?</li> </ul>"},{"location":"skills-tracks/rri/rri-204-2/#limits-of-transparency","title":"Limits of Transparency","text":"<p>Like explainability, transparency is a nuanced concept and it is important to recognise that there are limits to what can be achieved and what may be desirable.</p> <p>Firstly, while this section has proposed examples of mechanisms and processes for documenting and sharing information about a project and the associated tasks, it should be recognised that these tasks add additional resource demands to the project. This can create (sometimes insurmountable) barriers for smaller teams, ranging from the obvious ones\u2014not having sufficient time to accommodate while also meeting deadlines\u2014to more subtle (but nonetheless important) ones, such as the potential for increased stress and burnout, especially for more junior members of the team that may be expected to take on these types of tasks.<sup>6</sup> Therefore, as with some of the other SAFE-D principles, it is important to adopt a meta-principle of proportionality when determining the extent to which a principle should be applied. While doing the bare minimum is rarely an ethically praiseworthy approach, it is also the case that spending too much time on a task that is not proportionate to the potential impact of the decision or action in question can be a waste of resources.</p> <p>Secondly, for some projects, transparency may not be possible for a variety of reasons: intellectual property restrictions, legal restrictions on the disclosure of sensitive information and the need to protect the privacy of individuals, or security concerns about the potential for malicious actors to exploit the information being shared. In cases such as these, it is important to remember that the SAFE-D principles are not absolute rules\u2014they are guides and starting points for further reflection. Providing overriding reasons, therefore, for why data have not been made accessible or why some decisions about a project's governance have not been disclosed publicly, is fine\u2014and may even be the appropriate form of transparency (i.e. transparency of reasons).</p> <ol> <li> <p>Put aside the question of whether the second team of lawyers are acting in a responsible manner here, as it would be easy to argue that they are upholding their professional duties to their client by sending over mountains of evidence.\u00a0\u21a9</p> </li> <li> <p>This scenario is inspired by and adapted from this article: Lazaridis, D. (2021). Explainable AI (XAI) and interpretable machine learning (IML) models. Ambiata. Accessed: January 22, 2023.\u00a0\u21a9</p> </li> <li> <p>Admittedly, this could probably have been inferred without the expertise of a data analyst.\u00a0\u21a9</p> </li> <li> <p>This example refers to a practice known as 'personalised pricing', or sometimes 'price discrimination'. Neither are new practices (see here), but the widespread use of algorithmic techniques is enabling more dynamic and hyper-personalised forms of both personalised pricing and price discrimination (see this article).\u00a0\u21a9</p> </li> <li> <p>Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2016, August). \"Why should I trust you?\" Explaining the predictions of any classifier. In Proceedings of the 22<sup>nd</sup> ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1135-1144). https://dl.acm.org/doi/abs/10.1145/2939672.2939778 \u21a9</p> </li> <li> <p>See this article on \"glue work\" for an illuminating discussion about the unfair distribution of tasks within projects, and the professional development barriers this can create for specific groups (e.g. females, PhD students) in disciplines such as data science or software engineering.\u00a0\u21a9</p> </li> </ol>"},{"location":"skills-tracks/rri/rri-204-3/","title":"Model Interpretability","text":"<p> Here's a hypothetical scenario that serves as a cautionary tale for the remainder of this section.<sup>1</sup></p> <p>A model, trained on a dataset of cars, predicts a car's <code>top speed</code> based on the following three features:</p> <ol> <li><code>Colour</code></li> <li><code>Number of doors</code></li> <li><code>Convertible</code> (binary value: 'yes' or 'no')</li> </ol> <p>The model does very well for cars that are green, have two doors, and are convertible\u2014the model predicts they have very high speeds, and surprisingly has a high level of accuracy.</p> <p>A team of data analysts are puzzled by this ability and want to employ model interpretability methods to understand how important each of these features are to the prediction and their relative effects. It turns out that all of the features are important, but none of them are \"explaining\" why such cars are faster than others.</p> <p>As you may have guessed, the real reason that the model is accurate in predicting a high top speed in such cases is because most of the cars that are green, have two doors, and are convertible happen to be Lamborghinis, which are known for their high top speeds.</p> <p> </p> A green Lamborghini Hurac\u00e1n Evo Spyder. <p>However, none of the features in the model have any causal relationship with the top speed of a car. Rather, it is the engine (among other things) that is responsible for the top speed of a car. The other features are what we would call 'confounding variables'\u2014they are not the cause of the top speed, but they are correlated with the cause.</p> <p>Here is a diagram (reprinted from this article) that illustrates the problem:</p> <p> </p> Figure 7. The car manufacturer determines the colour, number of doors, whether the car is a convertible or not and the top speed. But in our dataset we only observe the variables in the blue box where there are no arrows between the predictors and top speed. <p>What is a feature? What is a variable?</p> <p>For those who are new to machine learning, the difference between 'features' and 'variables' can appear confusing, especially because the two are sometimes used synonymously. However, the concepts can come apart, and the term 'feature' has a specific meaning that goes beyond the general meaning of 'variable' as employed in statistics more generally.</p> <p>In short, a feature is some measurable property or characteristic of the data that is used as input for a machine learning algorithm to make predictions or decisions. In a simple example, the set of features could be the columns in a dataset that represent properties of an object (e.g. <code>number of doors</code>, <code>colour</code>, and <code>convertible</code> in the previous example). However, features are not always hand selected by ML engineers. For more complex algorithms, features can be selected or engineered through computational methods, resulting in features that are not readily interpretable by humans due to their lack of semantic meaning.</p> <p>As such, the variables contained in training datasets, may end being the same as the variables (or, features) that are used as inputs into the model, but this does not have to be the case. In other cases, many of the original set of variables that may be explored during initial data analysis can be discarded based on their lack of relevance or contribution to model performance. Therefore, in machine learning, the term 'feature' is used specifically to refer to those input variables that are used for making predictions.</p> <p>We won't belabour the well-trodden point about correlation not equalling causation in this section. However, it is a useful means for drawing attention to the limitation of model interpretability methods to explain causal relationships between the input features and the model's predictions, among other types of explanations (e.g. explanations about data collection issues, or project governance choices). Moreover, it serves as a gentle reminder about the importance of understanding the limitations of any tool before using it.</p> <p>In this section, we will explore the more technical aspects of explainability, which rely on the interpretability of the underlying model (including its development and architecture). This section does not go into the practical details of how to use interpretability methods, but does provide a brief overview of some of the most common methods and tools. As always, the focus is on the ethical implications of these methods and how they help promote a responsible approach to data science and AI research and innovation.</p>"},{"location":"skills-tracks/rri/rri-204-3/#what-is-model-interpretability","title":"What is model interpretability?","text":"<p>In the opening section of this module, we introduced the following definition of interpretability to help distinguish it from explainability:</p> <p>Interpretability is the degree to which a human can understand the cause of a decision.<sup>2</sup></p> <p>In this section specifically, when we use the term 'interpretability' we will be referring to model interpretability, which is the degree to which a human can understand the cause of a model's prediction or behaviour.  However, it is also appropriate to use the term 'interpretability' to refer to other components of a machine learning system, such as the data or the behaviour of the system itself, which may depend on several models working in concert.</p>"},{"location":"skills-tracks/rri/rri-204-3/#methods-for-interpreting-models","title":"Methods for Interpreting Models","text":"<p>There are many methods for interpreting models. Before we look at a way for categorising them, let's just start with a look at some examples that are loosely representative of the myriad techniques currently available.</p> <p>Overlaps</p> <p>When reading the following list, you may think that some of the techniques resemble one another. You may also find it hard to notice a substantial difference at all.  If this is the case, do not fret, we will look at a taxonomy shortly that will help bring some conceptual clarity to the miscellany of techniques.</p> <ul> <li>Rule-based models: models that use simple decision rules to arrive at predictions (e.g. decision trees). The rules are often represented in a human-readable format.</li> <li>Linear models: models where the relationship between the input variables and output variables can be expressed though a simple equation, and the coefficients enable a human to interpret the relative importance of each input variable (e.g. weights in logistic regression models).</li> <li>Feature importance techniques: techniques that can be used to identify the input variables that are most important for making predictions. Various techniques exist, such as permutation importance (i.e. repeated and random permutation of the values of each input feature to observe how this affects the model's predictions) or sensitivity analysis (i.e. observing sensitivity of a model to variations in specific features).  As we will see shortly, feature importance techniques can vary in terms of their scope (i.e. local versus global measures of importance).</li> <li>Prototypes and criticisms: a technique for explaining how a model classifies instances by first generating 'prototypes' of each class, which are both representative of the class members and, ideally, as differentiable from the other classes as possible. In contrast, 'criticisms' are instances that do not belong to the class to which they were assigned.  Both prototypes and criticisms can be informative in understanding where a model is working well and poorly.</li> <li>Surrogate models: a technique that aims to generate simplified, interpretable models that emulate the predictions of a more complex model (e.g. black box models). Surrogate model techniques, such as LIME, can help data scientists and machine learning engineers gain insights into how a more complex model operates.</li> <li>Visualisations: there are many visualisation techniques, familiar to data analysts, that can also help display the outputs of other techniques, such as feature importance, in formats that improve interpretability.</li> <li>Concept activation vectors: a model-specific technique for neural networks, where a user identifies a concept of interest, and explores how this concept is activated across the neural network by applying several additional techniques (e.g. training supplementary classifiers on a subset of data to predict the behaviour of the underlying neural network).  Here, 'concept' is a high-level abstraction, which could be made up of many lower-level features in the model (e.g. the concept of 'ears' in a neural network designed to classify images of animals).</li> <li>Counterfactual explanations: a counterfactual explanation shows how the output of a model (e.g. prediction) would change if the input data were different.</li> <li>Bayesian networks: the benefit of Bayesian networks is their ability to model the conditional dependencies between input features and output variables.  Like counterfactual explanations, Bayesian networks can be a useful tool for building causal explanations, when used carefully and with a robust understanding of the data, problem, and the limitations of the tools.</li> </ul> Causal Inference in Machine Learning and AI <p>The topic of causal inference in machine learning and AI is a well-studied area of research, but is not within the scope of this section or module.</p> <p>For more information, see the following resources:</p> <ul> <li>Pearl, J. and Mackenzie, D. (2018). The book of why: The new science of cause and effect. https://www.penguin.co.uk/books/289825/the-book-of-why-by-judea-pearl-and-dana-mackenzie/9780141982410</li> <li>Sgaier, S. K., Huang, V., and Charles, G. (2020). The case for causal AI. Stanford Social Innovation Review. https://ssir.org/articles/entry/the_case_for_causal_ai</li> <li>Forney, A. and Mueller, S. (2022). Causal inference in AI education: A primer. Journal of Causal Inference. https://www.degruyter.com/document/doi/10.1515/jci-2021-0048/html</li> </ul>"},{"location":"skills-tracks/rri/rri-204-3/#understanding-types-of-interpretability-methods","title":"Understanding types of interpretability methods","text":"<p>Now that we have seen some specific examples of model interpretability methods, let's try to bring some conceptual order to the landscape. Christopher Molnar's excellent book on Interpretable ML provides several useful ways to differentiate model interpretability, which we will take as our starting point:</p> <ol> <li>Intrinsic vs Post Hoc</li> <li>Model-specific vs model-agnostic</li> <li>Global vs Local</li> <li>Results of interpretability methods</li> </ol> <p>It is important to note that while grouped together into a taxonomy, the four pairs of categories are not necessarily of the same type. For instance, 'model-specific vs model agnostic' refers to a means for understanding the method by which interpretability is achieved, whereas 'results of interpretability methods' refers to the type of output that is produced by the interpretability method. The taxonomy is nevertheless highly informative, and so we will now look at each of the elements in further detail.</p>"},{"location":"skills-tracks/rri/rri-204-3/#1-intrinsic-versus-post-hoc","title":"1. Intrinsic versus post hoc","text":"<p>Intrinsic interpretability is a property of a model that can be measured on a scale from the class of models that are intrinsically interpretable (e.g. simple logistic regression models used for binary classification), through to the class of models that are highly complex and have very low (to no) intrinsic interpretability (e.g. large language models). For example, the following decision tree would have high levels of intrinsic interpretability because of its simple structure.</p> <p> A graphic showing a decision tree where the branches split based on whether a feature exceeds some threshold or not.</p> <p>In contrast, post hoc interpretability is a property of the model that is dependant on the application of additional methods or techniques applied after a model's training (hence, post hoc). For example, random forests\u2014a type of machine learning method known as an 'ensemble method' because it combines the predictions of several simpler models (i.e. decision trees) to predict an outcome (based on the consensus among the different trees within the forest)\u2014have low levels of intrinsic interpretability. However, post hoc methods (e.g. LIME) can be applied to help extract interpretations about, say, the importance of specific features, which would otherwise be too complex to extract.</p> <p>Clarifications</p> <p>Two clarifications are worth making:</p> <ol> <li>These two types of interpretability are not mutually exclusive.  Post hoc methods can also be applied to intrinsically interpretable models to improve interpretability.</li> <li>Intrinsically interpretable models may also depend on the use of techniques to improve interpretability that have been applied prior to training (e.g. feature engineering) for the model to remain intrinsically interpretable.  As such, they are not strictly speaking 'post hoc' in the sense of being applied after a model has been developed.</li> </ol>"},{"location":"skills-tracks/rri/rri-204-3/#2-model-specific-versus-model-agnostic","title":"2. Model-specific versus model-agnostic","text":"<p>There are a variety of ML models, ranging from simple logistic regression through gradient boosting and to more complex forms of reinforcement learning involving linked neural networks. The application of interpretability techniques and methods is sometimes constrained by the choice of model. For instance, integrated gradients for neural networks is a technique for visualising feature importance, as is the case with the following image, where the technique allows the interpreter to see which pixels were important in an image classification task.</p> <p></p> <p>An example of an integrated gradients technique showing which parts of an image were important in determining the output of a classification task. Reprinted from TensorFlow.</p> <p>This is an example of a model-specific method, because it requires a calculation to be carried out on specific elements of the underlying neural network, and this method would not apply to other models that have different internal structures. This is the case even if the objective of determining feature importance is similar to the goal of other methods that enable the interpretation of feature importance (e.g. LIME).</p> <p>In contrast, model agnostic methods can be used for any model. As such, they do not utilise any structural information about the model, but typically treat the algorithm as a black box and just focus on understanding which data, inputs, or features were important in the model's prediction.</p> <p>A common model agnostic method is to compute a partial dependence plot (PDP) to help visualize the relationship between a feature and a model's prediction. It shows how the model's prediction changes as the value of a feature changes, holding all other features constant.</p> <p>Cautionary Tale Redux</p> <p>Recall the cautionary tale at the start of this section! Model agnostic methods, such as PDPs, may not help uncover any causal relationships between features and outcomes, as the following example (reprinted from Molnar (2019)) shows:</p> <p> PDP of cancer probability and the interaction of age and number of pregnancies. The plot shows the increase in cancer probability at 45. For ages below 25, women who had 1 or 2 pregnancies have a lower predicted cancer risk, compared with women who had 0 or more than 2 pregnancies. But be careful when drawing conclusions: This might just be a correlation and not causal!</p>"},{"location":"skills-tracks/rri/rri-204-3/#3-global-versus-local","title":"3. Global Versus Local","text":"<p>We also have the distinction between local and global scope of interpretability methods.</p> <p>The example of the partial dependence plot given in the previous section was an example of a global method. This is because it helps individuals to interpret the relationship between a feature and a model's prediction, independent of any specific observation. In other words, it focuses on the global interpretability of the model itself.</p> <p>In contrast, there are also local methods that help interpret why specific predictions were made for a particular observation. Rather than focusing on the model itself, here the focus is on an individual data point or observation.</p> <p>Both types of interpretability are likely to be useful, but for different circumstances. For instance, let's assume a user receives a classification that they wish to contest. Here, local interpretability methods can help the system owner understand why a particular instance (perhaps their data point) was classified as belonging to one bucket, rather than another. This type of interpretation is likely to be more relevant in forming an explanation that is deemed adequate by the affected user than an explanation that is supported by a global interpretation.</p> <p>However, if an auditor or procurer is looking to assess the overall robustness and validity of a model as part of an evaluation process, they are more likely to want to understand, say, the overall relationships between features and predictions, which can be generated by global interpretability methods.</p>"},{"location":"skills-tracks/rri/rri-204-3/#4-outcomes-of-interpretability-methods","title":"4. Outcomes of Interpretability Methods","text":"<p>Finally, Molnar introduces five different outcomes that can be realised through the application of interpretability methods. These are:</p> <ol> <li>Feature summary statistics: a variety of metrics that can be used to help quantify the importance of features (e.g. a score that shows the increase in prediction error following permutation to a feature's value), or a pairwise interaction strength between two features.</li> <li>Feature summary visualisations: a different representational format that can also be used to help illustrate many of the feature summary statistics, in a way that can be more meaningful to a human.</li> <li>Model internals: values or information used to represent some element in the model's structure, such as its parameter (e.g. weights and biases).  For intrinsically interpretable models, such as linear regression models, this is how the model is said to be interpretable.</li> <li>Data points: techniques that return data points as output fall under this category.  For instance, counterfactual explanations return a data point that is similar to the data point being queried, but where the variation results in a different prediction.</li> <li>Intrinsically interpretable models: an interpretable model itself constitutes a result of some method, even if the method was to explicitly choose to develop a model that required no additional post hoc methods to be applied.</li> </ol> <p>You will notice that the first three types overlap and intersect with these five categories of outcomes. Again, this is because the taxonomy Molnar provides is not designed to provide a set of categories or types that are both mutually exclusive and jointly exhaustive. Rather, the taxonomy provides many overlapping and complementary perspectives on the different types of interpretability methods that are available.</p>"},{"location":"skills-tracks/rri/rri-204-3/#model-interpretability-and-responsible-research-and-innovation","title":"Model Interpretability and Responsible Research and Innovation","text":"<p>By now you will have identified some of the benefits and limitations of model interpretability. Let's now embed this understanding in the context of responsible research and innovation.</p>"},{"location":"skills-tracks/rri/rri-204-3/#understanding-your-data","title":"Understanding your data","text":"<p>While the term 'model interpretability methods' would suggest that the focus is on understanding the model itself, by now you will realise this is not always the case. Rather, model interpretability methods can also be a powerful tool for better understanding your data. For instance, interpretable ML methods can:</p> <ul> <li>Provide insights into which input variables are most important for forming predictions</li> <li>Help a project team identify errors, biases, or gaps in their dataset (e.g. missing data)</li> <li>Identify patterns in large, complex, or high-dimensional datasets that would otherwise be too difficult for humans to recognise</li> </ul> <p>However, these benefits depend upon a crucial step in the project lifecycle: selecting the right interpretability method.</p> <p>As the above taxonomy demonstrates, not all methods are created equally and selecting the right tool for the job remains an important maxim. For example, in many datasets, there will be complex, non-linear relationships between the input variables. Therefore, a narrow focus on local techniques that help user's interpret the contribution of a single feature to a prediction will not help disclose how, say, a positive weight for one individual feature depends on the existence of a negative weight for some other feature. As Molnar states:</p> <p>The interpretation of a single weight always comes with the footnote that the other input features remain at the same value, which is not the case with many real applications. A linear model that predicts the value of a house, that takes into account both the size of the house and the number of rooms, can have a negative weight for the room feature. It can happen because there is already the highly correlated house size feature. In a market where people prefer larger rooms, a house with fewer rooms could be worth more than a house with more rooms if both have the same size. The weights only make sense in the context of the other features in the model.<sup>3</sup></p> <p>This is why the other sections of this module emphasise the importance of a broader awareness of the project lifecycle and the sociocultural context in which the project operates as a crucial means for situating explanations (more on this in the next section).</p>"},{"location":"skills-tracks/rri/rri-204-3/#building-explanations","title":"Building Explanations","text":"<p>As we will see in the next section, multiple explanations for a system's behaviour may be required, depending on the recipient of the explanation. Therefore, in contrast to the above points about choosing the right tool for the job, here the plurality of methods can (in some cases) be a benefit.</p> <p>For instance, concept activation vectors may be very useful for a team of ML engineers who are communicating results to another team of ML engineers working in a separate research institution. However, if the original team decide to do public engagement and outreach they may need to rely on simpler forms of feature importance, surrogate models, or visualisations to help generate an accessible explanation.</p> <p>Therefore, while the maxim, 'choose the right tool for the job' still applies, it is also important to recognise that there may be multiple tools required for different explanatory purposes.</p> <p>Here again, we see a major difference between our use of the terms 'interpretability' and 'explainability'. Whereas we reserve the concept of interpretability for the methods and techniques that are used to understand the inner workings of a model (or system), explainability has a broader scope and refers to any process or mechanism across the whole project lifecycle that supports communication and reason-giving between project team members and stakeholders.</p>"},{"location":"skills-tracks/rri/rri-204-3/#accountability-and-transparency","title":"Accountability and Transparency","text":"<p>Consider the following question.</p> <p>Question</p> <p>Why would a stakeholder or user request an explanation for a model's behaviour or the outcomes of an AI system?</p> <p>Perhaps they're just interested and want to understand how different types of machine learning models work. But aside from curiosity and education, there are several other important reasons why stakeholders and users may request an explanation. These include:</p> <ul> <li>Compliance: stakeholders may need to demonstrate that their AI system is compliant with a particular regulation or standard before adopting it in a production environment.</li> <li>Bias detection: groups that are under-represented in a dataset may be at a greater risk of being misclassified by a model.  Therefore, understanding how a model makes predictions can help stakeholders (and team members) carry out bias detection and mitigation tasks, as a broader part of equality impact assessments.</li> <li>Risk management and redress: regulators have a duty to protect the public from various types of harm.  Therefore, model interpretability methods can play an important role in the broader mechanisms of project transparency, which can a) feed into risk management activities, and b) be used retrospectively if some harm occurs and redress for the (possibly unintended) harm is required.</li> </ul> <p>These non-exhaustive examples demonstrate how model interpretability methods also feed into broader objectives of responsible research and innovation, such as accountability and transparency. We cover these topics in more detail in a separate module (Note: this module is coming soon).</p> <p>Now, let us turn to the final section of this module to explore how we can bring all of this knowledge together to help us build (situated) explanations.</p> <ol> <li> <p>The scenario is adapted and summarised from the same case presented in this article.\u00a0\u21a9</p> </li> <li> <p>Miller, T. (2019). Explanation in artificial intelligence: Insights from the social sciences. Artificial intelligence, 267, p. 1-38. https://doi.org/10.1016/j.artint.2018.07.007 \u21a9</p> </li> <li> <p>Molnar, C. (2022). Interpretable Machine Learning. A Guide for Making Black Box Models Explainable. https://christophm.github.io/interpretable-ml-book/scope-of-interpretability.html \u21a9</p> </li> </ol>"},{"location":"skills-tracks/rri/rri-204-4/","title":"Situated Explanations","text":"<p> At the start of this module we looked at a case of a generative AI producing images (or, digital art) that were difficult to explain.</p> <p>Image creation is just one example of generative AI's capabilities. In addition to artwork, generative AI can also be used to create realistic images and videos of scenes or people (often referred to as 'deepfakes'). And beyond visual media, generative AI can also be used to generate text, code, and even music. Let's look at the first case, text. Most, if not all, forms of generative AI suffer the same explainability barrier as the image generation example. However, this does not mean that the requirements for explainability are the same. This is because the sociocultural context in which the systems are deployed create different expectations for valid and accessible explanations. Therefore, we need to consider the situation in which the system is deployed, and the users of the system, when we think about explainability. In other words, we need to understand what is required for the delivery and communication of situated explanations.</p> <p>Ethics of Generative AI</p> <p>There is a lot to say about the ethics of generative AI, outside of the context of explainability. However, as this would be tangential to our current focus for this section, we will not say more about this beyond the need to maintain a critical perspective on its development and use. The following resources are useful for further reading:</p> <p>Note: a resource list is coming soon!</p>"},{"location":"skills-tracks/rri/rri-204-4/#building-upon-interpretability-and-transparency-to-develop-situated-explanations","title":"Building upon interpretability and transparency to develop situated explanations","text":"<p>At the time of writing, the most popular approach to AI generated text is the use of large language models (LLMs). In the words of one LLM, known as ChatGPT:</p> <p>A large language model is a machine learning model that has been trained on a large dataset of text data, and can generate or predict text. It can understand and respond to human language, and can be used for a variety of natural language processing tasks, such as language translation, text summarization, and question answering. Examples include GPT-3, BERT, T5, etc.</p> <p>Note: This text was generated by an AI system (built using a large language model) in response to the following prompt, 'What is a large language model?'. You can notice that there are dubious statements and possible inaccuracies, such as the claim about LLMs being able to \"understand\" human language. This is why it is always important to ensure that LLMs and other forms of generative AI are used responsibly, and that their limitations are well understood.</p> <p>LLMs such as ChatGPT rely on a neural network approach for training the model. Specifically, they use an architecture known as a transformer.</p> <p>Understanding a little about how transformers operate can help us tease out some important lessons for \"situated explanations\". Therefore, we're going to explore a short tangent before coming back to the main topic of this section.</p>"},{"location":"skills-tracks/rri/rri-204-4/#transformers-and-llms","title":"Transformers and LLMs","text":"<p>Transformers are trained using unsupervised learning\u2014a type of machine learning where the learning algorithm attempts to find patterns and structure in data without explicit supervision or labelled examples. In the case of LLMs, large datasets of text are used as training data and the LLM learns to predict the next word in a sentence (i.e. the prompt) based on the preceding words. The key innovation of the transformer architecture is the use of so-called \"attention mechanisms\".</p> <p>These mechanisms are an important part of how LLMs can be made interpretable, although it is important to remember that LLMs are first and foremost an example of black box systems\u2014a point we will return to shortly. The attention mechanism used by transformers allows the model to \"attend\" to specific parts of the input (e.g. the sequence of word, or 'tokens') when making predictions. Different weights are then applied to each position in the sequence based on its relevance to the current context. This process can be repeated millions (or sometimes billions) of times on a vast datasets of text until the model has learned to generate coherent and meaningful sentences.</p> <p>What are tokens?</p> <p>In natural language processing (NLP), an input sequence is typically divided into individual units of meaning called \"tokens\". Depending on the task, these tokens can be words, subwords, or individual characters. LLMs such as GPT-3 and BERT are two examples of NLP systems.</p> <p>You will notice here, a similar process used by some of the other algorithmic approaches and models discussed in the previous sections: feature summary statistics and values for model internals are popular approaches to interpretability. However, the attention mechanism that neural-network based LLMs use is applied to multiple layers of the transformer's architecture. This allows the model to capture long-range dependencies and encode complex relationships between different parts of the input sequence, which in turn allows LLMs to produce more coherent and meaningful text. For instance, there is a substantive difference in the following two tasks, both of which involve a prompt:</p> <ol> <li>Complete the sentence: \"Mark is looking for ... keys.\"</li> <li>Write me a press release for a new quarterly earnings report. The company is called 'Widgets Inc.' The profits are up 20% from last quarter.</li> </ol> <p>In the first example, filling the gap with the word \"his\" is sufficient to complete the sentence and the important features would probably be 'Mark' and the target object, 'keys'. Such an approach would also allow members of a project team to easily identify gender bias in the model (e.g. associating specific genders with social roles, such as doctor or nurse).</p> <p>However, in the second example, the task is much more complex, and the space of valid responses to the prompt is significantly more diverse. As such, simply showing the model's attention weights\u2014calculated by comparing each token to all other tokens in the sequence and measuring the similarity between them\u2014leads to a radically underdetermined explanation for why a particular answer was given.</p> <p>While we have used LLMs as our illustrative example here, the same issues occur with many other types of black box AI systems, especially with neural networks. This raises an important question:</p> <p>Question</p> <p>If we can't fully understand or interpret the behaviour of a model or AI system, what supplementary information can we provide to ensure the behaviour and outcomes of the system are explainable?</p>"},{"location":"skills-tracks/rri/rri-204-4/#explainable-processes-or-explainable-outcomes","title":"Explainable Processes or Explainable Outcomes","text":"<p>By now, you may have noticed that there are two over-arching targets for explainability:</p> <ul> <li>The processes by which a model and it's encompassing system/interface are designed, developed, and deployed.</li> <li>The outcomes of the predictive model, which may be responsible for automated decisions or feed into human decision-making (e.g. decision support systems, visualisations).</li> </ul> <p>This distinction between process-based explanations and outcome-based explanations was put forward in a guidance, titled 'Explaining decisions made with AI', which was developed by the Information Commissioner's Office (ICO) and the Alan Turing Institute. It is also how we will understand the differential needs for situated explanations.</p> <p>The distinction helps to make clear some of the limitations referred to over the course of this module (e.g. limitations of post hoc methods of model interpretability). For example, a local, post hoc method will support explanations about why a model produced a specific prediction. This is in one sense, an outcome-based explanation, but it is also an incomplete outcome-based explanation.</p> <p>To see why, consider a predictive model that classifies surveillance images as depicting illegal deforestation or not. The outcome may be a binary classification (i.e. yes/no), perhaps accompanied by a confidence score. But, the tangible real world consequences of this classification may be very different depending on the context in which the model is used. For instance, due to regulatory and compliance differences, two national bodies may choose to do very different things with this prediction. One may choose to notify local authorities to investigate, whereas the other may choose to do nothing.</p> <p>This simple (hypothetical) example shows that there is a further societal outcome that encompasses, but goes beyond, the outcome of the model itself. And, in between the model outcome and the societal outcome, there may also be a salient system outcome, depending on how the model has been implemented within the system that user's interact with (e.g. a recommendation system or a decision support system that serves as the interface between the model and the user).</p> <p></p> <p>Explaining the different outcomes (model, system, societal) will likely require varying levels of detail and granularity and may require those doing the explaining to refer to different processes across the project lifecycle.</p> <p>For instance, asking the question \"why did the model classify an image as an instance of illegal deforestation?\" may require a more localised explanation about the model's internal processes and the data it was trained on. But asking the question, \"why did the national body choose to do nothing on the basis of a prediction of illegal deforestation?\" will require a more extensive and wide ranging explanation about how the system was designed, developed, and deployed, as well as an understanding of the societal and regulatory context in which the system is used.</p> <p>Consider again the above example of two national bodies receiving predictions about illegal deforestation. Let's look at why the second body may have chosen to do nothing. Maybe they have a policy of not taking action on predictions with a confidence score below some threshold without additional assurance of how the confidence score was calculated. As such, they may have a need for an explanation about the following processes:</p> <ul> <li>Which experts were involved in the design of the system, and how did their involvement lead to the choice of decision threshold for the classifier?</li> <li>Which evaluation metrics were selected to assess the performance of the model, and why were these metrics chosen? How were biases such as training-serving skew assessed and mitigated?</li> <li>Were any steps taken during the model's implementation to accommodate variations in the quality of input data (e.g. low resolution images)?</li> </ul> <p>All of these questions are requests for an explanation. Furthermore, they are all requests for an explanation about the processes that may or may not have contributed to the outcomes of the model.</p> <p>Using the project lifecycle model to support explanations</p> <p>The project lifecycle model introduced in the previous model provides a scaffold for thinking about which processes may be the target of an explanation. For example, each stage can serve as a deliberative prompt to help identify whether the typical tasks carried out during that stage could provide salient information to help address the request for an explanation.</p> <p>Because the recipients of explanations have needs that may be different to those with technical expertise and deep familiarity with the model's architecture, it is not sufficient to simply provide an outcome-based explanation that relies on the outputs of post hoc methods of model interpretability. For instance, if the request for an explanation come from a machine learning engineer, the questions may have been very different.</p> <p>This is why we refer to 'situated explanations'. The term 'situated' is used to emphasise the importance of the context (or, situation) in which explanations are requested, and the stakeholder that is being engaged. Having an awareness and appreciation of these contextual factors is important for identifying what the 'explainability' requirements for a project will be, and can only be suitably addressed through informed and meaningful stakeholder engagement\u2014ideally, early on in the project's lifecycle and in an ongoing and iterative manner.</p> Situated Cognition <p>There is a further sense in which explanations are situated, which draws upon research in the behavioural and cognitive sciences known as 'situated cognition'.</p> <p>Situated cognition is a theory that emphasises the importance of our environments and social context in which learning, decision-making, and cognition more broadly take place.  The theory suggests that our cognition is not a process that exists solely within our brains, but is instead shaped and influenced by the situation or context in which it occurs.</p> <p>A simple example would be a chef who relies on her external environment to offload certain cognitive tasks to her environment (e.g. \"remembering\" the ingredients and order for a recipe by ensuring the ingredients are carefully laid out in front of her, and any tools or equipment are within easy reach).</p> <p>Situating explanations is important in this sense, because it also recognises the importance of the context in which a recipient of an explanation will evaluate the explanation based on their own contextual or environmental influences.</p> <p>If you're interested in reading more about situated cognition, the following resources are a good place to start:</p> <ul> <li>Hutchins, E. (1995) Cognition in the Wild. MIT Press.https://mitpress.mit.edu/9780262581462/cognition-in-the-wild/</li> <li>Clark, A (1998) Being There: Putting brain, body and world together again. MIT Press.https://mitpress.mit.edu/9780262531566/being-there/</li> <li>Robbins, P. and Aydede, M. (2009) A Short Primer on Situated Cognition. Preprint Available: https://philarchive.org/archive/ROBASP-4</li> </ul>"},{"location":"skills-tracks/rri/rri-204-4/#proportionality-and-the-demands-of-explainability","title":"Proportionality and the Demands of Explainability","text":"<p>In closing this section, let's look at one final question:</p> <p>Question</p> <p>How much resources should a project team invest in improving the explainability of their model or system?</p> <p>As we have seen, improving the explainability of a system is not a trivial feat. It can require, among other things, access to technical and domain-specific expertise for ensuring model interpretability, resources for clear and accessible documentation to support project transparency, and opportunities for meaningful engagement with stakeholders to ensure a broad understanding of the sociocultural context in which the system will be used. The capacity of a team and organisation to meet these demands will obviously vary.</p> <p>Therefore, a proportional approach to explainability will typically be required (as is also the case with the other SAFE-D principles). In short, this means weighing up what can be practically achieved within the constraints of the project (e.g. time, budget, and available expertise) against the normative demands of explainability. It does not mean identifying the bare minimum required and then stopping there, regardless of what the capacity of the team supports.</p> <p>A general rule for helping assess what a proportional investment in explainability should be is the following maxim:</p> <p>The greater the impact and scope of a system, the greater the need for explainability.</p> <p>For instance, the use of a generative AI model that produces short jingles for elevators\u2014admittedly, an absurd use case\u2014is hardly creating any significant impact or risk of harm, beyond the potential for mild annoyance. However, at the other end of the spectrum, a black box AI system that classifies citizen's based on widespread data surveillance and is used to make decisions about their access to public services, could have a significant impact on the lives of millions or billions of people. The former barely creates any need for explainability, whereas the latter would require a significant investment in explainability to ensure its responsible design, development, and deployment.</p>"},{"location":"skills-tracks/rri/rri-204-index/","title":"About this Module - Explainability","text":"<p>This module is about the importance of explainability in AI and data-driven systems. It focuses on three relevant aspects usually required for achieving explainability in the context of AI and data-driven systems: project transparency, model interpretability, and being able to provide situated explanations. Although none of them are, strictly speaking, necessary for the crafting of appropriate explanations, in most cases they are extremely useful.</p>"},{"location":"skills-tracks/rri/rri-204-index/#learning-objectives","title":"Learning Objectives","text":"<p>This module has the following learning objectives:</p> <ul> <li>Explore the concept of explainability in the context of data science and AI, and understand the importance of appropriate, context-dependent explanations.</li> <li>Understand what is meant by project transparency, what it looks like, and what some of its limitations are.</li> <li>Learn about model interpretability, the different types of methods available for interpreting models, and how we can classify them.</li> <li>Identify the need for explanations that are tailored to specific situated contexts and its corresponding audience.</li> <li>Explore the difference between outcome-based and process-based explanations and when it might be best to employ one or the other.</li> </ul> <p>Important This module is not a technical introduction to Interepretable ML methods, nor does it attempt to provide an up-to-date overview of the current methods in Interpretable ML or Explainable AI. New methods are currently being developed at a rapid pace, and many of these methods are designed to solve problems with specific techniques (e.g. integrated gradients and feature relevance for classificatory models). It is not possible, nor desirable, to keep these resources up-to-date with these sorts of developments. Rather, we aim to provide clarity on the practical and ethical consequences of explainability in data-driven technologies. As such, we discuss those methods (or classes of methods) that are well established and have wide applicability across domains and use cases.</p>"},{"location":"skills-tracks/rri/rri-204-index/#table-of-contents","title":"Table of Contents","text":"<ul> <li> <p> What is Explainability?</p> <p>This section introduces the module's principle of explainability. It looks at the definition of explainability, its scope, and the main difference with the related concept of interpretability.</p> <p> Go to module</p> </li> <li> <p> Project Transparency</p> <p>This section highlights the importance of transparent and accountable processes of project governance. It looks at questions such as what is project transparency, why we might need it, and what does responsible project transparency look like. It also touches upon the limits of project transparency and the need to balance the different requirements of a project (of which transparency is only one).</p> <p> Go to module</p> </li> <li> <p> Model Interpretability</p> <p>This section dives into the more technical notion of model interpretability, that is, the degree to which a human can understand the cause of a decision (in this case made by an AI system).  It focuses on the different kinds of methods that exist to interpret models or increase their interpretability.  It also discusses different ways of classifying said methods.</p> <p> Go to module</p> </li> <li> <p> Situated Explanations</p> <p>This final section is about the actual building of explanations. As such, it focuses on the importance of the socio-cultural context in which an explanation is given, as well as the stakeholders to which it is given.  Explanations should be tailored to the audience which require them. It also looks at the difference between outcome-based and process-based explanations.</p> <p> Go to module</p> </li> </ul>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2022/","title":"2022","text":""},{"location":"blog/category/news/","title":"news","text":""}]}