# About this Module - Explainability

This module is about the importance of explainability in AI and data-driven systems.
It focuses on three relevant aspects usually required for achieving explainability in the context of AI and data-driven systems: project transparency, model interpretability, and being able to provide *situated* explanations.
Although none of them are, strictly speaking, *necessary* for the crafting of appropriate explanations, in most cases they are extremely useful.

## Learning Objectives

This module has the following learning objectives:

- Explore the concept of explainability in the context of data science and AI, and understand the importance of appropriate, context-dependent explanations.
- Understand what is meant by project transparency, what it looks like, and what some of its limitations are.
- Learn about model interpretability, the different types of methods available for interpreting models, and how we can classify them.
- Identify the need for explanations that are tailored to specific situated contexts and its corresponding audience.
- Explore the difference between outcome-based and process-based explanations and when it might be best to employ one or the other.


> **Important**
> This module is not a technical introduction to Interepretable ML methods, nor does it attempt to provide an up-to-date overview of the current methods in Interpretable ML or Explainable AI.
> New methods are currently being developed at a rapid pace, and many of these methods are designed to solve problems with specific techniques (e.g. integrated gradients and feature relevance for classificatory models).
> It is not possible, nor desirable, to keep these resources up-to-date with these sorts of developments.
> Rather, we aim to provide clarity on the practical and ethical consequences of explainability in data-driven technologies.
> As such, we discuss those methods (or classes of methods) that are well established and have wide applicability across domains and use cases.

## Table of Contents

<div class="grid cards" markdown>

-   :octicons-beaker-16:{ .lg .middle } __What is Explainability?__

    ---

    This section introduces the module's principle of explainability.
    It looks at the definition of explainability, its scope, and the main difference with the related concept of interpretability.


    [:octicons-arrow-right-24: Go to module](rri-204-1.md)

-   :fontawesome-solid-arrows-spin:{ .lg .middle } __Project Transparency__

    ---

    This section highlights the importance of transparent and accountable processes of project governance.
    It looks at questions such as what is project transparency, why we might need it, and what does responsible project transparency look like.
    It also touches upon the limits of project transparency and the need to balance the different requirements of a project (of which transparency is only one).

    [:octicons-arrow-right-24: Go to module](rri-204-2.md)

-   :material-thought-bubble:{ .lg .middle } __Model Interpretability__

    ---

    This section dives into the more technical notion of model interpretability, that is, the degree to which a human can understand the cause of a decision (in this case made by an AI system). 
    It focuses on the different kinds of methods that exist to interpret models or increase their interpretability. 
    It also discusses different ways of classifying said methods.

    [:octicons-arrow-right-24: Go to module](rri-204-3.md)

-   :material-chat-processing:{ .lg .middle } __Situated Explanations__

    ---

    This final section is about the actual building of explanations.
    As such, it focuses on the importance of the socio-cultural context in which an explanation is given, as well as the stakeholders to which it is given. 
    Explanations should be tailored to the audience which require them.
    It also looks at the difference between outcome-based and process-based explanations.

    [:octicons-arrow-right-24: Go to module](rri-204-4.md)

</div>

