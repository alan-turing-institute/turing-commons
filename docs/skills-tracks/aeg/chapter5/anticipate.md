# Anticipate impacts
<figure markdown>
  ![deliberation](https://raw.githubusercontent.com/alan-turing-institute/turing-commons/main/docs/assets/images/illustrations/deliberation.png){ align="center" }
</figure>

Anticipating impacts of an AI system involves reflecting on and assessing the potential short-term and long-term effects the system may have on impacted individuals and on affected communities and social groups, more broadly.

Why is this kind of anticipatory reflection important? Its purpose is to safeguard the sustainability of AI projects across the entire project lifecycle instead of taking an approach of dealing with issues as they appear. There is no guarantee that a team will be able to anticipate *all* potential impacts, but dealing with the most relevant ones before they become a problem ensures more sustainable systems overall (it is also a much more efficient use of resources over time).

How does one  ensure that the activities and outputs of the AI system are socially and environmentally sustainable? Project team members must proceed with a continuous responsiveness to the real-world impacts that their system could have.

The way to translate into practice as we have seen, is through concerted and stakeholder-involving exploration of the possible adverse and beneficial effects that could otherwise remain hidden from view if deliberate and structured processes for anticipating downstream impacts were not in place. 

Attending to sustainability, along these lines, also entails the iterative re-visitation and re-evaluation of impact assessments. To be sure, in its general usage, the word “sustainability” refers to the maintenance of and care for an object or endeavour over time. In the context of AI, this implies that building sustainability into a project is not a “one-off” affair.

Rather, carrying out an initial impact assessment at the inception of a project is only a first, albeit critical, step in a much longer, end-to-end process of responsive re-evaluation and re-assessment. Such an iterative approach ensures that continuous attention is payed both to the dynamic and changing character of the project lifecycle and to the shifting conditions of the real-world environments in which studies are embedded.

Methodical impact evaluation should involve an initial adoption of normative criteria that function as metrics for scoping and assessing the possible harms and benefits of the research and its outputs. Taking GPAI’s “12 Principles and Priorities of Responsible Data Innovation” as an example, relevant impact assessment questions could include:

- How, if at all, could our research and its outputs impact each of the following twelve principles and priorities as they relate to all affected stakeholders, especially those who are vulnerable, marginalised, or historically discriminated against? (Affected stakeholders include research subjects and participants, subjects of data collected for or used in the study, researchers, and all other impacted people and social groups.) 

!!! abstract "12 Principles and Priorities of Responsible Data Innovation"

        - Respect for and protection of human dignity
        - Interconnectivity, solidarity, and intergenerational reciprocity
        - Environmental flourishing, sustainability, and the rights of the biosphere
        - Protection of human freedom and autonomy
        - Prevention of harm and protection of the right to life and physical, psychological, and moral integrity
        - Non-discrimination, fairness, and equality
        - Rights of Indigenous peoples and Indigenous data sovereignty 
        - Data protection and the right to respect of private and family life
        - Economic and social rights
        - Accountability and effective remedy
        - Democracy
        - Rule of law

- How could our research and its outputs advance each of these twelve principles and priorities or hinder their realisation?

- Are there particular stakeholder groups who could disproportionately enjoy the benefits of the research and its outputs, or suffer from the potential harms they generate, as these harms and benefits relate to each of the twelve principles and priorities?

- If things go wrong in our research or if its outputs (especially tools produced or capacities enabled) are used out-of-the-scope of their intended purpose and function, what harms could be done to stakeholders in relation to each of the twelve principles and priorities?

It is important to note here that stakeholder involvement in the impact assessment process can be a critical safeguard against evaluative blind spots and omissions. 

Methodical impact evaluation should also involve an assessment of the severity of potential adverse impacts. This brings clarity to the prioritisation of impact mitigation actions by allowing the severity levels of potential negative effects to be differentiated, elucidated, and refined. As explained in the United Nations Guiding Principles on Business and Human Rights (UNGP), assessing the severity of potential negative impacts on fundamental rights and freedoms involves consideration of their scale, scope, and remediability, where scale is defined as “the gravity or seriousness of the impact,” scope as “how widespread the impact is, or the numbers of people impacted,” and remediability, as the “ability to restore those affected to a situation at least the same as, or equivalent to, their situation before the impact” (UNGP, 2011, Principle 14).  

One notable challenge faced by researchers who are assessing the severity of potential adverse impacts is identifying cumulative or aggregate downstream impacts, which can be much more difficult than identifying harms directly or proximately caused by a project.

Discerning these impacts may require additional research and consultation with domain experts and other relevant stakeholders. This difficulty results from the fact that cumulative impacts are often incremental and more difficult to perceive, and they frequently involve complex contexts of multiple actors or projects operating in the same area or sector or affecting the same populations.[@gotzmann2020] Some “big picture” questions to reflect on when assessing cumulative or aggregate impacts include:
<!-- (Götzmann et al., 2020) -->

- Could the project contribute to wider scale adverse impacts when its deployment is coordinated with (or occurs in tandem with) other projects or innovation activity that serve similar functions or purposes? For example, if the impacts of a  project that aims to discover an effective method of behavioural nudging at scale are considered in combination with the proliferation of many other similar projects or computational systems in a given sector, concerns about wider cumulative effects like mass manipulation, objectification, and infringement on autonomy and human dignity become relevant.

- Could the project replicate, reinforce, or augment socio-historically entrenched legacy harms that create knock-on effects in impacted individuals and groups? For example, if a project analyses sensitive personal information contained in databases scraped from social media websites without gaining the proper consent of research subject in accordance with their reasonable expectations, it could add to the legacy harms of companies that have used data recklessly and eroded public trust regarding the respect of privacy and data protection rights in the digital sphere. This can create wider chilling effects on elements of open communication, information sharing, and interpersonal connection that are essential components for the sustainability of democratic forms of life.

- Could the production and use of the system be understood to contribute to wider aggregate adverse impacts on the biosphere and on planetary health when its deployment is considered in combination with other systems that may have similar environmental impacts? For example, a project that involves moderate levels of energy consumption in model training or data storage may be seen to contribute to significant environmental impact when considered alongside the energy consumption of similar projects across research ecosystems.

Once impacts have been evaluated and the severity of any potential harms assessed, impact prevention and mitigation planning should commence. Diligent impact mitigation planning begins with a scoping and prioritization stage. Team members (and engaged stakeholders, where appropriate) should go through all the identified potential adverse impacts and map out the interrelations and interdependencies between them as well as surrounding social factors (such as contextually specific stakeholder vulnerabilities and precariousness) that could make impact mitigation more challenging. Where prioritization of prevention and mitigation actions is necessary (for instance, where delays in addressing a potential harm could reduce its remediability), decision-making should be steered by the relative severity of the impacts under consideration. As a general rule, while impact prevention and mitigation planning may involve prioritization of actions, all potential adverse impacts must be addressed. When potential adverse impacts have been mapped out and organised, and mitigation actions have been considered, the research team (and engaged stakeholders, where appropriate) should begin co-designing an impact mitigation plan (IMP). The IMP will become the part of your transparent reporting methodology that specifies the actions and processes needed to address the adverse impacts which have been identified and that assigns responsibility for the completions of these tasks and processes. As such, the IMP will serve a crucial documenting function. 

**Establishment of protocols for re-visitation and re-evaluation of the research impact assessment:**

Impact assessments must pay continuous attention both to the dynamic and changing character of the project lifecycle and to the shifting conditions of the real-world environments in which research practices, results, and outputs are embedded. There are two sets of factors that should inform when and how often initial impact assessments are re-visited to ensure that they remain adequately responsive to factors that could present new potential harms or significantly influence impacts that have been previously identified: 

1.	Lifecycle and production factors: Choices made at any point along the workflow may affect the veracity of prior impact assessments—leading to a need for re-assessment, reconsideration, and amendment. For instance, design choices could be made that were not anticipated in the initial impact assessment (such choices might include adjusting the variables that are included in the model, choosing more complex algorithms, or grouping variables in ways that may impact specific groups). These changes may influence how a computational model performs, how it is explained, or how it impacts affected individuals and groups. Processes are also iterative and frequently bi-directional, and this often results in the need for revision and update. For these reasons, impact assessments must remain agile, attentive to change, and at-the-ready to evaluatively move back and forth across the decision-making pipeline as downstream actions affect upstream choices and evaluations.

2.	Environmental factors: Changes in project-relevant social, regulatory, policy or legal environments (occurring during the time in which the research is taking place) may have a bearing on how well the resulting computational model works and on how the research outputs impact affected individuals and groups. Likewise, domain-level reforms, policy changes, or changes in data recording methods may take place in the population of concern in ways that affect whether the data used to train the model accurately portrays phenomena, populations, or related factors in an accurate manner. In the same vein, cultural or behavioral shifts may occur within affected populations that alter the underlying data distribution and hamper the predictive and explanatory efficacy of a model, which has been trained on data collected prior to such shifts. All of these alterations of environmental conditions can have a significant effect on how research practices, outputs, and results impact affected individual and communities.


<!-- Although the demand to anticipate project impacts is not new for some areas of impact, there has not been a consistent standard for applying impact assessment to AI systems (CITE).  -->
<!-- There is a growing awareness of the importance of anticipatory reflection in terms of the wider social impacts an AI system might have. In academia for example, the NeuroIPS conference introduced a new ethics review protocol that required paper submissions to include an impact statement “discussing the broader impact of their work, including possible societal consequences—both positive and negative” (NeurIPS, 2020) (CITE). -->
<!-- ADD some more info from  -->

<!-- This demand to anticipate research impacts is not new in the modern academy—especially in the biomedical and social sciences, where IRB processes for research involving human subjects have been in place for decades (Abbott and Grady, 2011; Grady 2015). However, the novel human scale, breadth, and reach of CSS research, as well as the new (and often subtler) range of potential harms it poses to impacted individuals, communities, and the biosphere, call into question the adequacy of conventional IRB processes (Metcalf and Crawford, 2016). While the latter have been praised a necessary step forward in protecting the physical, mental, and moral integrity of human research subjects, building public trust in science, and institutionalising needed mechanisms for ethical oversight (Resnik, 2018), critics have also highlighted their unreliability, superficiality, narrowness, and inapplicability to the new set of information hazards posed by the processing of aggregated big data (Prunkl et al., 2021; Raymond, 2019). -->

<!-- A growing awareness of these deficiencies has generated an expanding interest in CSS-adjacent computational disciplines (like machine learning, artificial intelligence, and computational linguistics) to come up with more robust impact assessment regimes and ethics review processes (Hecht et al., 2020; Leins et al., 2020; Nanayakkara, 2021). For instance, in 2020, the NeurIPS conference introduced a new ethics review protocol that required paper submissions to include an impact statement “discussing the broader impact of their work, including possible societal consequences—both positive and negative” (NeurIPS, 2020). Informatively, this protocol was converted into a responsible research practices checklist in 2021 (NeurIPS, 2021) after technically oriented researchers protested that they lacked the training and guidance needed to carry out impact assessments effectively (Ashurst et al., 2021; Johnson, 2020; Prunkl et al., 2021). Though there has been recent progress made, in both AI and CSS research communities, to integrate some form of ethics training into professional development (Ashurst et al., 2020; Salganik and SICSS, nd.) and to articulate guidelines for anticipating ethical impacts (NeurIPS, 2022), there remains a lack of institutionalised instruction, codified guidance, and professional stewardship for research impact assessment processes. As an example, conferences such as ICWSM (2022), ICML (2022), NAACL (2022), and NAAC (2022) each require some form of research impact evaluation and ethical consideration, but aside from directing researchers to relevant professional guidelines and codes of conduct (e.g., from the ACL, ACM, and AAAI), there is scant direction on how to operationalise impact assessment processes (Prunkl et al., 2021). -->

<!-- What is missing from this patchwork of ethics review requirements and guidance is a set of widely accepted procedural mechanisms that would enable and standardise conscientious research impact assessment practices. To fill this gap, recent research into the governance practices needed to create responsible data research environments has called for a coherent, integrated, and holistic approach to impact assessment that includes several interrelated elements (Leslie 2019, 2020; Leslie et al., 2021, 2022b, 2022c, 2022d):  -->
<!-- is this the same in AI projects in general. -->

<!-- As we saw in chapter 3, engaging with stakeholders is a crucial step for anticipating the potential impacts of an AI project. Diligent impact assessment practices should include processes that allows team members to identify and evaluate the salience and contextual characteristics of individuals or groups who may be affected by, or may affect, the research project under consideration (Mitchell et al., 2017; Reed et al., 2005; Schmeer, 1999; Varvasovszky and Brugha, 2000). Stakeholder analysis aims to help researchers understand the relevance of each identified stakeholder to their project and to its use contexts.  It does this by providing a structured way to assess the relative interests, rights, vulnerabilities, and advantages of identified stakeholders as these characteristics may be impacted by, or may impact, the research.  -->

<!-- Three steps are involved in thorough stakeholder analysis. First, researchers should draw on desk-based research, domain expertise, local knowledge, and the lived experience of relevant community members to get a sense of the social environment and human factors that may be affected by, or may affect, the research. This initial exploration should also include positionality reflection to help determine whether the backgrounds of researchers could introduce biases or blind spots into the analysis (elaborated on in the next section). Second, building on this contextual understanding, researchers should identify those individuals and groups who may be significantly impacted by, or may impact, the project, paying close attention to vulnerable and protected groups. Finally, researchers should carry out a stakeholder salience analysis to determine the individuals and groups who are most relevant when considering potential project impacts. This involves assessing the relative interests, rights, vulnerabilities, and advantages of identified stakeholders as these characteristics may be impacted by the project.  -->

<!-- Stakeholder analyses may be carried out in a variety of ways that involve more-or-less stakeholder involvement. This spectrum of options ranges from analyses carried out exclusively by a research team without active community engagement to analyses built around the inclusion of community-led participation and co-design from the earliest stages of stakeholder identification. The degree of stakeholder involvement should vary from project to project based upon a preliminary assessment of the potential risks and hazards of the research, with stakeholder engagement being proportionate to the severity and scale of the potential dangers posed by the project.  -->

<!-- Establishment of clear normative criteria for impact assessment: Effective research impact assessment practices should start from a clear set of ethical values or human rights criteria against which the potential impacts of a project on affected individuals and communities can be evaluated. Such criteria should provide common but non-exclusive point of departure for collective deliberation about the ethical permissibility of the research project under consideration. Adopting common normative criteria from the outset enables reciprocally respectful, sincere, and open discussion about the ethical challenges a research project may face by helping to create a shared vocabulary for informed dialogue and impact assessment. Such a common starting point also facilitates deliberation about how to balance ethical values when they come into tension. -->

<!-- There is, however, a crucial hurdle that must be cleared when establishing which normative criteria to adopt. Amid the undeniable ethical plurality of modern social life, it has become essential to acknowledge the historically relative and contextually situated character of normative criteria per se (Ess, 2006; Lassman, 2011; Madsen and Strong, 2009). This implies that no fixed or universally accepted list of ethical values or fundamental rights and freedoms could pre-reflectively provide such a common starting point. Over the past several decades, research ethicists have, for this reason, taken a more pragmatic and empirically driven position, in proposing basic values, that begins by considering the set of real-world dangers posed by practices of scientific research and by the use of the innovations they yield. Indeed, the principles that have emerged from the two main sources of modern Western research ethics, namely, bioethics and human rights, have found their origins in moral claims that have responded directly to tangible, technologically inflicted harms and atrocities. Whereas the human rights perspective (and its expressions in the founding documents of research ethics, the 1947 Nuremberg Code and the 1964 Helsinki Declaration) has its roots in efforts to redress the well-known technological barbarisms and genocides of the mid-twentieth century, in the case of bioethics, its emergence tracked the public exposure in the 1960s and 1970s of several atrocities of human experimentation—such as the infamous Tuskegee syphilis experiment in the US.   -->

<!-- The responsiveness of the principles of human rights and bioethics to technological harms goes some way to explaining their prominence in contemporary digital ethics. Across all the iterations of the Internet Research Ethics guidelines (IRE 1.0, IRE 2.0, and IRE 3.0), the “Primary Ethical Norms”, which are taken as basic normative criteria (respect for persons, beneficence, and justice), are drawn directly from bioethics (Beauchamp, 2008; franzke et al., 2020). Likewise, in the applied ethics of artificial intelligence and data science, ethics researchers have broadly converged around human rights and bioethical principles that are seen as effectually responding to the real-world problems posed by the use of the AI and data-driven technologies themselves. These hazards include the potential loss of human agency, privacy, and social connection in the wake of expanding automation and datafication, harmful outcomes that may result from the use of poor-quality data or poorly designed systems, and the possibility that entrenched societal dynamics of bias and discrimination will be perpetuated or even augmented by data-driven technologies that tend to reinforce existing social and historical patterns. Accordingly, principles like protecting human dignity, respecting the integrity of private and family life, ensuring solidarity and social connection, supporting human and biospheric wellbeing, and safeguarding equal status, social justice, and the common good have emerged as widely accepted normative criteria (Council of Europe, 2020; High Level Expert Group on AI, 2019; Institute Of Electrical And Electronics Engineers, 2018; Leslie, 2019; Toronto Declaration, 2018; University of Montreal, 2017). -->

<!-- While these ethical values provide a solid basis for research impact assessment in CSS, there is another valence of ethical plurality that must be confronted. From a more interculturally oriented perspective, researchers must acknowledge that the exclusion of non-Western ethical frameworks from the dominant discourses that have shaped the ethics and governance of digital technologies and computational research up to the present reflects deeper legacies of coloniality and Western cultural hegemony that are in need of redress (de Sousa Santos, 2018 Medina, 2012; Quijano, 2007).  On this view, given the planetary stretch of CSS research, its research ethics must confront the way that such legacies have created an unsustainable homogeneity of ethical values in digital ethics. Resistance to the prevailing the monoculture of Western-centric morality will allow CSS research ethics to become sufficiently responsive to the condition of cultural and ethical pluralism that typifies the modern, interconnected global society both between nation-states and regions and within them (Aggarwal, 2020; franzke et al., 2020; Leslie et al., 2022a).   -->

<!-- Any normative criteria that form the basis for research impact assessment must thus be inclusive of the diverse cultural self-understandings and lived experience of all those who may be affected whether or not their value standpoints lie within predominant Western sociocultural sensibilities (Birhane, 2021; Mhlambi, 2020). To meet such a need for an ethically pluralistic and normatively inclusive approach to CSS research ethics, the establishment of clear normative criteria for impact assessment must reflect and foster non-Western visions of ethical life—visions that often depart from the predominant individualistic ethos of Anglo-European framings and instead embrace a more relational, biocentric, and community-based view of moral action and interaction (such as seen, for instance, in the Ubuntu affirmation of moral personhood through social relationality or in the Abya Yala Indigenous prioritization of living well, sumac kawsay, and the care for Mother Earth, Pachamama, in South America) (Eze, 2008; Gyekye, 1992; Huanacuni 2010; Kalumba, 2020; Mbiti, 1970; Menkiti, 1984; Walsh 2015, 2018). Attempts to actualise this broadened scope of normative criteria for assessment of the impacts of computational research and innovation have been recently made in UNESCO’s “Recommendation on the ethics of artificial intelligence”, which has been adopted by its 193 member states, and in the “12 Principles and Priorities of Responsible Data Innovation” proposed as part of the Global Partnership on AI’s (GPAI’s)  2021-2022 Advancing Data Justice Research and Practice project (include here as Annex 1). -->

<!-- Methodical evaluation of potential impacts and impact mitigation planning: The actual research impact assessment process provides an opportunity for research teams (and engaged stakeholders, where deemed appropriate) to produce detailed evaluations of the potential and actual impacts that the project may have, to contextualize and corroborate potential harms and benefits, to make possible the collaborative assessment of the severity of potential adverse impacts identified, and to facilitate the co-design of an impact mitigation plan. -->
<!--  -->
<!-- Methodical impact evaluation should involve an initial adoption of normative criteria that function as metrics for scoping and assessing the possible harms and benefits of the research and its outputs.  -->

<!-- Let't take GPAI's "12 Principles and Priorities of Responsible Data Innovation” as an example. -->


<!-- Relevant impact assessment questions could include: -->

<!-- - How, if at all, could our research and its outputs impact each of the following twelve principles and priorities as they relate to all affected stakeholders, especially those who are vulnerable, marginalised, or historically discriminated against? (Affected stakeholders include research subjects and participants, subjects of data collected for or used in the study, researchers, and all other impacted people and social groups.)  -->


<!-- - How could the project advance each of these twelve principles and priorities or hinder their realisation? -->

<!-- - Are there particular stakeholder groups who could disproportionately enjoy the benefits of the project, or suffer from the potential harms it generates, as these harms and benefits relate to each of the twelve principles and priorities? -->

<!-- - If things go wrong in the project (especially tools produced or capacities enabled) are used out-of-the-scope of their intended purpose and function, what harms could be done to stakeholders in relation to each of the twelve principles and priorities? -->

<!-- It is important to note here that stakeholder involvement in the research impact assessment process can be a critical safeguard against evaluative blind spots and omissions. Impacted individuals and social groups are often in a better position to identify salient impacts, and the inclusion of affected people in impact evaluation processes enables research teams to appropriately contextualize and corroborate the potential harms and benefits they discern in dialogue with people whose positionality and lived experience well situates them to reflectively anticipate possible hazards and advantages. -->

<!-- Methodical impact evaluation should also involve an assessment of the severity of potential adverse impacts. This brings clarity to the prioritisation of impact mitigation actions by allowing the severity levels of potential negative effects to be differentiated, elucidated, and refined. As explained in the United Nations Guiding Principles on Business and Human Rights (UNGP), assessing the severity of potential negative impacts on fundamental rights and freedoms involves consideration of their scale, scope, and remediability, where scale is defined as “the gravity or seriousness of the impact,” scope as “how widespread the impact is, or the numbers of people impacted,” and remediability, as the “ability to restore those affected to a situation at least the same as, or equivalent to, their situation before the impact” (UNGP, 2011, Principle 14).   -->

<!-- One notable challenge faced by researchers who are assessing the severity of potential adverse impacts is identifying cumulative or aggregate impacts of the research and its outputs on stakeholders (and their progeny) that could expand their effects beyond the scope of impact identified for those individuals and communities who are directed affected. Identifying cumulative or aggregate downstream impacts can be much more difficult than identifying harms directly or proximately caused by a research project and its outputs, and discerning these impacts may require additional research and consultation with domain experts and other relevant stakeholders. This difficulty results from the fact that cumulative impacts are often incremental and more difficult to perceive, and they frequently involve complex contexts of multiple actors or research projects operating in the same area or sector or affecting the same populations (Götzmann et al., 2020). Some “big picture” questions to reflect on when assessing cumulative or aggregate impacts include: -->

<!-- Could the research and its outputs contribute to wider scale adverse impacts when its deployment is coordinated with (or occurs in tandem with) other research projects or innovation activity that serve similar functions or purposes? For example, if the impacts of a CSS project that aims to discover an effective method of behavioural nudging at scale are considered in combination with the proliferation of many other similar projects or computational systems in a given sector, concerns about wider cumulative effects like mass manipulation, objectification, and infringement on autonomy and human dignity become relevant. -->

<!-- Could the research and its outputs replicate, reinforce, or augment socio-historically entrenched legacy harms that create knock-on effects in impacted individuals and groups? For example, if a CSS project analyses sensitive personal information contained in databases scraped from social media websites without gaining the proper consent of research subject in accordance with their reasonable expectations, it could add to the legacy harms of companies that have used data recklessly and eroded public trust regarding the respect of privacy and data protection rights in the digital sphere. This can create wider chilling effects on elements of open communication, information sharing, and interpersonal connection that are essential components for the sustainability of democratic forms of life. -->

<!-- Could the production and use of the system be understood to contribute to wider aggregate adverse impacts on the biosphere and on planetary health when its deployment is considered in combination with other systems that may have similar environmental impacts? For example, a CSS project that involves moderate levels of energy consumption in model training or data storage may be seen to contribute to significant environmental impact when considered alongside the energy consumption of similar projects across research ecosystems. -->

<!-- Once impacts have been evaluated and the severity of any potential harms assessed, impact prevention and mitigation planning should commence. Diligent impact mitigation planning begins with a scoping and prioritization stage. Research team members (and engaged stakeholders, where appropriate) should go through all the identified potential adverse impacts and map out the interrelations and interdependencies between them as well as surrounding social factors (such as contextually specific stakeholder vulnerabilities and precariousness) that could make impact mitigation more challenging. Where prioritization of prevention and mitigation actions is necessary (for instance, where delays in addressing a potential harm could reduce its remediability), decision-making should be steered by the relative severity of the impacts under consideration. As a general rule, while impact prevention and mitigation planning may involve prioritization of actions, all potential adverse impacts must be addressed. When potential adverse impacts have been mapped out and organised, and mitigation actions have been considered, the research team (and engaged stakeholders, where appropriate) should begin co-designing an impact mitigation plan (IMP). The IMP will become the part of your transparent reporting methodology that specifies the actions and processes needed to address the adverse impacts which have been identified and that assigns responsibility for the completions of these tasks and processes. As such, the IMP will serve a crucial documenting function.  -->

<!-- Establishment of protocols for re-visitation and re-evaluation of the research impact assessment: Research impact assessments must pay continuous attention both to the dynamic and changing character of the research lifecycles and to the shifting conditions of the real-world environments in which research practices, results, and outputs are embedded. There are two sets of factors that should inform when and how often initial research impact assessments are re-visited to ensure that they remain adequately responsive to factors that could present new potential harms or significantly influence impacts that have been previously identified:  -->

<!-- 1.	Research workflow and production factors: Choices made at any point along the research workflow may affect the veracity of prior impact assessments—leading to a need for re-assessment, reconsideration, and amendment. For instance, research design choices could be made that were not anticipated in the initial impact assessment (such choices might include adjusting the variables that are included in the model, choosing more complex algorithms, or grouping variables in ways that may impact specific groups). These changes may influence how a computational model performs, how it is explained, or how it impacts affected individuals and groups. Research processes are also iterative and frequently bi-directional, and this often results in the need for revision and update. For these reasons, research impact assessments must remain agile, attentive to change, and at-the-ready to evaluatively move back and forth across the decision-making pipeline as downstream actions affect upstream choices and evaluations. -->

<!-- 2.	Environmental factors: Changes in project-relevant social, regulatory, policy or legal environments (occurring during the time in which the research is taking place) may have a bearing on how well the resulting computational model works and on how the research outputs impact affected individuals and groups. Likewise, domain-level reforms, policy changes, or changes in data recording methods may take place in the population of concern in ways that affect whether the data used to train the model accurately portrays phenomena, populations, or related factors in an accurate manner. In the same vein, cultural or behavioral shifts may occur within affected populations that alter the underlying data distribution and hamper the predictive and explanatory efficacy of a model, which has been trained on data collected prior to such shifts. All of these alterations of environmental conditions can have a significant effect on how research practices, outputs, and results impact affected individual and communities. -->
